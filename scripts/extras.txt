        # pick random diagonal metric
        self.Phi = nn.Parameter(
            torch.normal(1, 1 / np.sqrt(self.hidden_size), (self.hidden_size,))
        )
        # / np.sqrt(self.hidden_size)

        # to parameterize weight matrix
        self.B = nn.Parameter(
            torch.normal(
                1, 1 / np.sqrt(self.hidden_size), (self.hidden_size, self.hidden_size)
            )
        )



class VanillaRNNDiagonalMetric(nn.Module):
    def __init__(self, param_dict: dict):
        super().__init__()

        # timestep for Euler integration
        self.alpha = param_dict["alpha"]

        # number of input, hidden, and output
        self.input_size = param_dict["input_size"]
        self.hidden_size = param_dict["hidden_size"]
        self.output_size = param_dict["output_size"]

        # leak rate of neurons
        self.gamma = param_dict["gamma"]

        # set nonlinearity of the vanilla RNN
        self.nonlinearity = param_dict["nonlinearity"]
        if self.nonlinearity == "tanh":
            self.phi = torch.tanh

        if self.nonlinearity == "relu":
            self.phi = F.relu

        if self.nonlinearity == "none":
            print("Nl = none")
            self.phi = torch.nn.Identity()

        # diagonal metric to be learned
        self.Phi = nn.Parameter(
            torch.normal(1, 1 / np.sqrt(self.hidden_size), (self.hidden_size,))
        )

        # to parameterize weight matrix
        self.B = nn.Parameter(
            torch.normal(
                1, 1 / np.sqrt(self.hidden_size), (self.hidden_size, self.hidden_size)
            )
        )

        # initialize the input-to-hidden weights
        self.weight_ih = nn.Parameter(
            torch.normal(
                0, 1 / np.sqrt(self.hidden_size), (self.hidden_size, self.input_size)
            )
        )

        # initialize the hidden-to-output weights
        self.weight_ho = nn.Parameter(
            torch.normal(
                0, 1 / np.sqrt(self.hidden_size), (self.output_size, self.hidden_size)
            )
        )

        # initialize the output bias weights
        self.bias_oh = nn.Parameter(
            torch.normal(0, 1 / np.sqrt(self.hidden_size), (1, self.output_size))
        )

        # initialize the hidden bias weights
        self.bias_hh = nn.Parameter(
            torch.normal(0, 1 / np.sqrt(self.hidden_size), (1, self.hidden_size))
        )

        # output weights
        self.W_hidden_to_out = nn.Linear(
            in_features=self.hidden_size, out_features=self.output_size
        )

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        state = torch.zeros(input.shape[0], self.hidden_size)

        # for storing RNN outputs and hidden states
        outputs = []

        # loop over input
        for i in range(input.shape[1]):
            # compute output
            hy = state @ self.weight_ho.T + self.bias_oh

            # save output and hidden states
            outputs += [hy]

            # compute the RNN update
            fx = -state + self.phi(
                state @ (torch.diag(self.Phi**-1) @ self.B @ torch.diag(self.Phi)).T
                + input[:, i, :] @ self.weight_ih.T
                + self.bias_hh
            )

            # step hidden state foward using Euler discretization
            state = state + self.alpha * (fx)

        # organize states and outputs and return
        return torch.stack(outputs).permute(1, 0, 2)


import torch.jit as jit


class vRNNLayer(nn.Module):
    """Vanilla RNN layer in continuous time."""

    def __init__(self, param_dict: dict):
        super(vRNNLayer, self).__init__()

        self.device = param_dict["device"]

        # timestep for Euler integration
        self.alpha = param_dict["alpha"]

        # number of input, hidden, and output
        self.input_size = param_dict["input_size"]
        self.hidden_size = param_dict["hidden_size"]
        self.output_size = param_dict["output_size"]

        # leak rate of neurons
        self.gamma = param_dict["gamma"]

        # set nonlinearity of the vanilla RNN
        self.nonlinearity = param_dict["nonlinearity"]

        # set nonlinearity of the vRNN
        if self.nonlinearity == "tanh":
            self.phi = torch.tanh
        if self.nonlinearity == "relu":
            self.phi = F.relu
        if self.nonlinearity == "none":
            print("Nl = none")
            self.phi = torch.nn.Identity()

        # initialize weights
        self.W_in = nn.Linear(
            in_features=self.input_size, out_features=self.hidden_size
        )
        self.W_hh = nn.Linear(
            in_features=self.hidden_size, out_features=self.hidden_size
        )
        self.W_out = nn.Linear(
            in_features=self.hidden_size, out_features=self.output_size
        )

    def forward(self, input: Tensor) -> Tensor:
        # initialize state at the origin. randn is there just in case we want to play with this later.
        hx = torch.zeros(
            size=(input.shape[1], self.hidden_size),
            device=self.device,
            requires_grad=False,
        )

        inputs = input.unbind(0)

        # for storing RNN outputs and hidden states
        outputs = []  # torch.jit.annotate(List[torch.Tensor], [])

        # loop over input
        for i in range(len(inputs)):
            # save output and hidden states
            outputs += [self.W_out(hx)]

            # compute the RNN update
            fx = -hx + self.phi(self.W_hh(hx) + self.W_in(inputs[i]))

            # step hidden state foward using Euler discretization
            hx = hx + self.alpha * (fx)

        # organize states and outputs and return
        return torch.stack(outputs)  # .permute(1, 0, 2)


'''
class VSRNN(jit.ScriptModule):
    # Vanilla Symmetric RNN (VSRNN)
    def __init__(self, param_dict: dict):
        super().__init__()

        self.device = param_dict["device"]

        # timestep for Euler integration
        self.alpha = param_dict["alpha"]

        # number of input, hidden, and output
        self.input_size = param_dict["input_size"]
        self.hidden_size = param_dict["hidden_size"]
        self.output_size = param_dict["output_size"]

        # leak rate of neurons
        self.gamma = param_dict["gamma"]

        # set nonlinearity of the vanilla RNN
        self.nonlinearity = param_dict["nonlinearity"]

        # recurrence
        self.rnn = nn.RNNCell(
            self.input_size, self.hidden_size, nonlinearity=self.nonlinearity
        )
        # self.rnn.weight_hh = nn.Parameter(torch.eye(self.hidden_size))

        # self.rnn.to(self.device)

        """
        # parameterize W = I - A.T @ A - epsilon*I
        parametrize.register_parametrization(
            self.rnn,
            "weight_hh",
            SymmetricStable(n=self.hidden_size, epsilon=1e-4, device=self.device),
        )
        """

        # output weights
        self.W_hidden_to_out = nn.Linear(
            in_features=self.hidden_size, out_features=self.output_size
        )

    @jit.script_method
    def forward(self, input: torch.Tensor) -> torch.Tensor:
        inputs = input.unbind(1)

        outputs = torch.jit.annotate(List[torch.Tensor], [])

        # Initialize hidden state
        hx = (1 / self.hidden_size) * torch.randn(
            input.shape[0], self.hidden_size, device=self.device
        )

        # loop over input
        for i in range(input.shape[1]):
            fx = -hx + self.rnn(inputs[i], hx)
            hx = hx + self.alpha * (fx)
            outputs += [self.W_hidden_to_out(hx)]

        # organize outputs and return
        return torch.stack(outputs).permute(1, 0, 2)

    def _check_stability(self, stability_type="symmetric"):
        with torch.no_grad():
            # check if svd condition is met
            if stability_type == "svd":
                s = torch.linalg.svdvals(
                    torch.diag(self.Phi)
                    @ self.rnn.weight_hh
                    @ torch.diag(self.Phi**-1)
                )
                if torch.any(s > 1):
                    print("RNN is not provably stable.")
                else:
                    print("RNN is provably stable in given metric.")
                return s

            # check if symmetric eigenvalue condition is met
            if stability_type == "symmetric":
                e, _ = torch.linalg.eigh(self.rnn.weight_hh)
                if torch.any(e > 1):
                    print("RNN is not provably stable with symmetric condition. ")
                else:
                    print("RNN is provably stable with symmetric condition.")
                return e
'''



eeg_train = np.load('/home/leo/rnns-of-rnns/data/EEG_train.npy')

def run_adaptation_EEG(net = None, hidden_size = 16, p = 4, dt = 0.005, T = 150,trials = 1 ,eta = 1):

    n = int(hidden_size*p)
    #ts = np.arange(0, T, dt)
    ts = np.linspace(0,1,len(eeg_train))


    es = np.zeros((trials,len(ts)))



    for trial in range(trials):
        x = np.random.normal(0, 1, n)

        #_, sine_waves = generate_sine_waves(n, T, time_step= dt, frequency = 5)
        
        #_, sine_waves_desired = generate_sine_waves(n, T, time_step = dt, frequency = 1)
        
        #P = np.random.normal(0,1/np.sqrt(n),size = (n,1))
        #sine_waves_desired = P @ sine_wave_desired
        
        states = []

        for k,t in enumerate(ts):
            
            states.append(x)

            e = x - eeg_train[k,:]
            es[trial,k] = (1/np.sqrt(n))*np.linalg.norm(e)            

            
            net.A = net.A + dt*(-eta*np.outer(e,np.tanh(x))*net.B)
            f = net(x, -1*e)
            
            

            x = x +  dt*f

        states = np.stack(states)
        #outputs = states @ P
        
        #output_errors = outputs - sine_wave_desired
        #es_output = np.linalg.norm(output_errors,axis = 1)
        
    
    return es, states#, outputs, es_output
    