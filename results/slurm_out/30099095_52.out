Gradient step: 0 	 Performance: 0.06 	 Training loss: 3.16
Gradient step: 24 	 Performance: 0.085 	 Training loss: 62.53
Gradient step: 49 	 Performance: 0.05 	 Training loss: 36.21
Gradient step: 74 	 Performance: 0.06 	 Training loss: 31.17
Gradient step: 99 	 Performance: 0.195 	 Training loss: 25.93
Gradient step: 124 	 Performance: 0.225 	 Training loss: 23.70
Gradient step: 149 	 Performance: 0.25 	 Training loss: 22.22
Gradient step: 174 	 Performance: 0.26 	 Training loss: 20.79
Gradient step: 199 	 Performance: 0.335 	 Training loss: 19.58
Gradient step: 224 	 Performance: 0.365 	 Training loss: 18.74
Gradient step: 249 	 Performance: 0.405 	 Training loss: 17.69
Gradient step: 274 	 Performance: 0.46 	 Training loss: 16.54
Gradient step: 299 	 Performance: 0.505 	 Training loss: 15.26
Gradient step: 324 	 Performance: 0.49 	 Training loss: 14.79
Gradient step: 349 	 Performance: 0.47 	 Training loss: 13.74
Gradient step: 374 	 Performance: 0.53 	 Training loss: 12.96
Gradient step: 399 	 Performance: 0.655 	 Training loss: 12.51
Gradient step: 424 	 Performance: 0.705 	 Training loss: 11.88
Gradient step: 449 	 Performance: 0.68 	 Training loss: 11.46
Gradient step: 474 	 Performance: 0.76 	 Training loss: 10.97
Gradient step: 499 	 Performance: 0.625 	 Training loss: 10.92
Gradient step: 524 	 Performance: 0.7 	 Training loss: 11.18
Gradient step: 549 	 Performance: 0.655 	 Training loss: 10.73
Gradient step: 574 	 Performance: 0.71 	 Training loss: 10.55
Gradient step: 599 	 Performance: 0.735 	 Training loss: 10.09
Gradient step: 624 	 Performance: 0.725 	 Training loss: 9.86
Gradient step: 649 	 Performance: 0.595 	 Training loss: 9.82
Gradient step: 674 	 Performance: 0.75 	 Training loss: 9.36
Gradient step: 699 	 Performance: 0.72 	 Training loss: 9.48
Gradient step: 724 	 Performance: 0.735 	 Training loss: 9.83
Gradient step: 749 	 Performance: 0.745 	 Training loss: 9.37
Gradient step: 774 	 Performance: 0.735 	 Training loss: 8.40
Gradient step: 799 	 Performance: 0.805 	 Training loss: 8.36
Gradient step: 824 	 Performance: 0.725 	 Training loss: 8.09
Gradient step: 849 	 Performance: 0.76 	 Training loss: 8.19
Gradient step: 874 	 Performance: 0.715 	 Training loss: 7.99
Gradient step: 899 	 Performance: 0.805 	 Training loss: 8.11
Gradient step: 924 	 Performance: 0.765 	 Training loss: 8.09
Gradient step: 949 	 Performance: 0.785 	 Training loss: 8.01
Gradient step: 974 	 Performance: 0.82 	 Training loss: 8.07
Gradient step: 999 	 Performance: 0.84 	 Training loss: 7.76
Gradient step: 1024 	 Performance: 0.745 	 Training loss: 7.77
Gradient step: 1049 	 Performance: 0.825 	 Training loss: 7.81
Gradient step: 1074 	 Performance: 0.8 	 Training loss: 8.05
Gradient step: 1099 	 Performance: 0.815 	 Training loss: 8.11
Gradient step: 1124 	 Performance: 0.81 	 Training loss: 7.93
Gradient step: 1149 	 Performance: 0.82 	 Training loss: 7.63
Gradient step: 1174 	 Performance: 0.81 	 Training loss: 7.85
Gradient step: 1199 	 Performance: 0.8 	 Training loss: 7.52
Gradient step: 1224 	 Performance: 0.79 	 Training loss: 7.71
Gradient step: 1249 	 Performance: 0.8 	 Training loss: 7.85
Gradient step: 1274 	 Performance: 0.81 	 Training loss: 7.42
Gradient step: 1299 	 Performance: 0.805 	 Training loss: 7.65
Gradient step: 1324 	 Performance: 0.87 	 Training loss: 7.65
Gradient step: 1349 	 Performance: 0.755 	 Training loss: 7.90
Gradient step: 1374 	 Performance: 0.85 	 Training loss: 7.69
Gradient step: 1399 	 Performance: 0.81 	 Training loss: 7.65
Gradient step: 1424 	 Performance: 0.845 	 Training loss: 7.59
Gradient step: 1449 	 Performance: 0.835 	 Training loss: 7.84
Gradient step: 1474 	 Performance: 0.86 	 Training loss: 7.67
Gradient step: 1499 	 Performance: 0.81 	 Training loss: 7.68
Gradient step: 1524 	 Performance: 0.8 	 Training loss: 7.59
Gradient step: 1549 	 Performance: 0.755 	 Training loss: 7.37
Gradient step: 1574 	 Performance: 0.78 	 Training loss: 7.61
Gradient step: 1599 	 Performance: 0.815 	 Training loss: 7.56
Gradient step: 1624 	 Performance: 0.795 	 Training loss: 7.73
Gradient step: 1649 	 Performance: 0.82 	 Training loss: 7.58
Gradient step: 1674 	 Performance: 0.81 	 Training loss: 7.51
Gradient step: 1699 	 Performance: 0.78 	 Training loss: 7.65
Gradient step: 1724 	 Performance: 0.825 	 Training loss: 7.47
Gradient step: 1749 	 Performance: 0.82 	 Training loss: 7.59
Gradient step: 1774 	 Performance: 0.785 	 Training loss: 7.46
Gradient step: 1799 	 Performance: 0.785 	 Training loss: 7.53
Gradient step: 1824 	 Performance: 0.85 	 Training loss: 7.58
Gradient step: 1849 	 Performance: 0.79 	 Training loss: 7.55
Gradient step: 1874 	 Performance: 0.82 	 Training loss: 7.40
Gradient step: 1899 	 Performance: 0.815 	 Training loss: 7.75
Gradient step: 1924 	 Performance: 0.77 	 Training loss: 7.65
Gradient step: 1949 	 Performance: 0.845 	 Training loss: 7.47
Gradient step: 1974 	 Performance: 0.8 	 Training loss: 7.58
Gradient step: 1999 	 Performance: 0.775 	 Training loss: 7.53
Gradient step: 2024 	 Performance: 0.825 	 Training loss: 7.58
Gradient step: 2049 	 Performance: 0.82 	 Training loss: 7.44
Gradient step: 2074 	 Performance: 0.825 	 Training loss: 7.64
Gradient step: 2099 	 Performance: 0.76 	 Training loss: 7.52
Gradient step: 2124 	 Performance: 0.785 	 Training loss: 7.72
Gradient step: 2149 	 Performance: 0.815 	 Training loss: 7.68
Gradient step: 2174 	 Performance: 0.825 	 Training loss: 7.58
Gradient step: 2199 	 Performance: 0.79 	 Training loss: 7.52
Gradient step: 2224 	 Performance: 0.835 	 Training loss: 7.61
Gradient step: 2249 	 Performance: 0.805 	 Training loss: 7.52
Gradient step: 2274 	 Performance: 0.825 	 Training loss: 7.49
Gradient step: 2299 	 Performance: 0.8 	 Training loss: 7.49
Gradient step: 2324 	 Performance: 0.88 	 Training loss: 7.47
Gradient step: 2349 	 Performance: 0.83 	 Training loss: 7.83
Gradient step: 2374 	 Performance: 0.815 	 Training loss: 7.54
Gradient step: 2399 	 Performance: 0.8 	 Training loss: 7.33
Gradient step: 2424 	 Performance: 0.765 	 Training loss: 7.92
Gradient step: 2449 	 Performance: 0.805 	 Training loss: 7.46
Gradient step: 2474 	 Performance: 0.78 	 Training loss: 7.56
Gradient step: 2499 	 Performance: 0.785 	 Training loss: 7.60
Gradient step: 2524 	 Performance: 0.76 	 Training loss: 7.57
Gradient step: 2549 	 Performance: 0.815 	 Training loss: 7.60
Gradient step: 2574 	 Performance: 0.81 	 Training loss: 7.77
Gradient step: 2599 	 Performance: 0.79 	 Training loss: 7.43
Gradient step: 2624 	 Performance: 0.81 	 Training loss: 7.67
Gradient step: 2649 	 Performance: 0.79 	 Training loss: 7.64
Gradient step: 2674 	 Performance: 0.815 	 Training loss: 7.40
Gradient step: 2699 	 Performance: 0.725 	 Training loss: 7.41
Gradient step: 2724 	 Performance: 0.81 	 Training loss: 7.51
Gradient step: 2749 	 Performance: 0.795 	 Training loss: 7.73
Gradient step: 2774 	 Performance: 0.815 	 Training loss: 7.77
Gradient step: 2799 	 Performance: 0.795 	 Training loss: 7.57
Gradient step: 2824 	 Performance: 0.81 	 Training loss: 7.46
Gradient step: 2849 	 Performance: 0.81 	 Training loss: 7.46
Gradient step: 2874 	 Performance: 0.775 	 Training loss: 7.71
Gradient step: 2899 	 Performance: 0.835 	 Training loss: 7.49
Gradient step: 2924 	 Performance: 0.805 	 Training loss: 7.54
Gradient step: 2949 	 Performance: 0.83 	 Training loss: 7.49
Gradient step: 2974 	 Performance: 0.855 	 Training loss: 7.71
Gradient step: 2999 	 Performance: 0.815 	 Training loss: 7.49
Gradient step: 3024 	 Performance: 0.835 	 Training loss: 7.72
Gradient step: 3049 	 Performance: 0.82 	 Training loss: 7.50
Gradient step: 3074 	 Performance: 0.82 	 Training loss: 7.62
Gradient step: 3099 	 Performance: 0.825 	 Training loss: 7.49
Gradient step: 3124 	 Performance: 0.785 	 Training loss: 7.61
Gradient step: 3149 	 Performance: 0.805 	 Training loss: 7.67
Gradient step: 3174 	 Performance: 0.74 	 Training loss: 7.47
Gradient step: 3199 	 Performance: 0.815 	 Training loss: 7.56
Gradient step: 3224 	 Performance: 0.81 	 Training loss: 7.59
Gradient step: 3249 	 Performance: 0.795 	 Training loss: 7.48
Gradient step: 3274 	 Performance: 0.745 	 Training loss: 7.54
Gradient step: 3299 	 Performance: 0.785 	 Training loss: 7.70
Gradient step: 3324 	 Performance: 0.795 	 Training loss: 7.66
Gradient step: 3349 	 Performance: 0.86 	 Training loss: 7.68
Gradient step: 3374 	 Performance: 0.8 	 Training loss: 7.58
Gradient step: 3399 	 Performance: 0.845 	 Training loss: 7.44
Gradient step: 3424 	 Performance: 0.785 	 Training loss: 7.51
Gradient step: 3449 	 Performance: 0.81 	 Training loss: 7.64
Gradient step: 3474 	 Performance: 0.815 	 Training loss: 7.73
Gradient step: 3499 	 Performance: 0.84 	 Training loss: 7.55
Gradient step: 3524 	 Performance: 0.845 	 Training loss: 7.70
Gradient step: 3549 	 Performance: 0.75 	 Training loss: 7.55
Gradient step: 3574 	 Performance: 0.815 	 Training loss: 7.65
Gradient step: 3599 	 Performance: 0.815 	 Training loss: 7.62
Gradient step: 3624 	 Performance: 0.845 	 Training loss: 7.79
Gradient step: 3649 	 Performance: 0.84 	 Training loss: 7.75
Gradient step: 3674 	 Performance: 0.79 	 Training loss: 7.75
Gradient step: 3699 	 Performance: 0.855 	 Training loss: 7.64
Gradient step: 3724 	 Performance: 0.785 	 Training loss: 7.59
Gradient step: 3749 	 Performance: 0.825 	 Training loss: 7.47
Gradient step: 3774 	 Performance: 0.825 	 Training loss: 7.40
Gradient step: 3799 	 Performance: 0.83 	 Training loss: 7.58
Gradient step: 3824 	 Performance: 0.795 	 Training loss: 7.59
Gradient step: 3849 	 Performance: 0.83 	 Training loss: 7.63
Gradient step: 3874 	 Performance: 0.83 	 Training loss: 7.53
Gradient step: 3899 	 Performance: 0.79 	 Training loss: 7.54
Gradient step: 3924 	 Performance: 0.745 	 Training loss: 7.56
Gradient step: 3949 	 Performance: 0.8 	 Training loss: 7.65
Gradient step: 3974 	 Performance: 0.77 	 Training loss: 7.56
Gradient step: 3999 	 Performance: 0.81 	 Training loss: 7.64
Gradient step: 4024 	 Performance: 0.795 	 Training loss: 7.55
Gradient step: 4049 	 Performance: 0.73 	 Training loss: 7.90
Gradient step: 4074 	 Performance: 0.82 	 Training loss: 7.67
Gradient step: 4099 	 Performance: 0.775 	 Training loss: 7.70
Gradient step: 4124 	 Performance: 0.865 	 Training loss: 7.41
Gradient step: 4149 	 Performance: 0.775 	 Training loss: 7.45
Gradient step: 4174 	 Performance: 0.805 	 Training loss: 7.43
Gradient step: 4199 	 Performance: 0.83 	 Training loss: 7.54
Gradient step: 4224 	 Performance: 0.86 	 Training loss: 7.64
Gradient step: 4249 	 Performance: 0.77 	 Training loss: 7.65
Gradient step: 4274 	 Performance: 0.785 	 Training loss: 7.39
Gradient step: 4299 	 Performance: 0.835 	 Training loss: 7.73
Gradient step: 4324 	 Performance: 0.825 	 Training loss: 7.59
Gradient step: 4349 	 Performance: 0.865 	 Training loss: 7.76
Gradient step: 4374 	 Performance: 0.81 	 Training loss: 7.58
Gradient step: 4399 	 Performance: 0.75 	 Training loss: 7.72
Gradient step: 4424 	 Performance: 0.855 	 Training loss: 7.42
Gradient step: 4449 	 Performance: 0.805 	 Training loss: 7.49
Gradient step: 4474 	 Performance: 0.805 	 Training loss: 7.64
Gradient step: 4499 	 Performance: 0.78 	 Training loss: 7.56
Gradient step: 4524 	 Performance: 0.825 	 Training loss: 7.87
Gradient step: 4549 	 Performance: 0.825 	 Training loss: 7.64
Gradient step: 4574 	 Performance: 0.795 	 Training loss: 7.54
Gradient step: 4599 	 Performance: 0.78 	 Training loss: 7.61
Gradient step: 4624 	 Performance: 0.85 	 Training loss: 7.60
Gradient step: 4649 	 Performance: 0.78 	 Training loss: 7.49
Gradient step: 4674 	 Performance: 0.775 	 Training loss: 7.55
Gradient step: 4699 	 Performance: 0.77 	 Training loss: 7.71
Gradient step: 4724 	 Performance: 0.805 	 Training loss: 7.70
Gradient step: 4749 	 Performance: 0.81 	 Training loss: 7.60
Gradient step: 4774 	 Performance: 0.785 	 Training loss: 7.63
Gradient step: 4799 	 Performance: 0.76 	 Training loss: 7.54
Gradient step: 4824 	 Performance: 0.765 	 Training loss: 7.53
Gradient step: 4849 	 Performance: 0.835 	 Training loss: 7.41
Gradient step: 4874 	 Performance: 0.795 	 Training loss: 7.59
Gradient step: 4899 	 Performance: 0.855 	 Training loss: 7.47
Gradient step: 4924 	 Performance: 0.78 	 Training loss: 7.86
Gradient step: 4949 	 Performance: 0.815 	 Training loss: 7.71
Gradient step: 4974 	 Performance: 0.755 	 Training loss: 7.60
Gradient step: 4999 	 Performance: 0.78 	 Training loss: 7.38
Gradient step: 5024 	 Performance: 0.805 	 Training loss: 7.46
Gradient step: 5049 	 Performance: 0.825 	 Training loss: 7.31
Gradient step: 5074 	 Performance: 0.8 	 Training loss: 7.41
Gradient step: 5099 	 Performance: 0.79 	 Training loss: 7.43
Gradient step: 5124 	 Performance: 0.785 	 Training loss: 7.77
Gradient step: 5149 	 Performance: 0.79 	 Training loss: 7.65
Gradient step: 5174 	 Performance: 0.765 	 Training loss: 7.54
Gradient step: 5199 	 Performance: 0.82 	 Training loss: 7.81
Gradient step: 5224 	 Performance: 0.835 	 Training loss: 7.36
Gradient step: 5249 	 Performance: 0.775 	 Training loss: 7.75
Gradient step: 5274 	 Performance: 0.81 	 Training loss: 7.60
Gradient step: 5299 	 Performance: 0.835 	 Training loss: 7.54
Gradient step: 5324 	 Performance: 0.81 	 Training loss: 7.50
Gradient step: 5349 	 Performance: 0.79 	 Training loss: 7.44
Gradient step: 5374 	 Performance: 0.77 	 Training loss: 7.49
Gradient step: 5399 	 Performance: 0.81 	 Training loss: 7.48
Gradient step: 5424 	 Performance: 0.815 	 Training loss: 7.55
Gradient step: 5449 	 Performance: 0.825 	 Training loss: 7.64
Gradient step: 5474 	 Performance: 0.8 	 Training loss: 7.52
Gradient step: 5499 	 Performance: 0.81 	 Training loss: 7.44
Gradient step: 5524 	 Performance: 0.81 	 Training loss: 7.63
Gradient step: 5549 	 Performance: 0.78 	 Training loss: 7.78
Gradient step: 5574 	 Performance: 0.83 	 Training loss: 7.40
Gradient step: 5599 	 Performance: 0.75 	 Training loss: 7.69
Gradient step: 5624 	 Performance: 0.815 	 Training loss: 7.48
Gradient step: 5649 	 Performance: 0.815 	 Training loss: 7.57
Gradient step: 5674 	 Performance: 0.835 	 Training loss: 7.54
Gradient step: 5699 	 Performance: 0.805 	 Training loss: 7.52
Gradient step: 5724 	 Performance: 0.835 	 Training loss: 7.47
Gradient step: 5749 	 Performance: 0.81 	 Training loss: 7.72
Gradient step: 5774 	 Performance: 0.85 	 Training loss: 7.74
Gradient step: 5799 	 Performance: 0.75 	 Training loss: 7.60
Gradient step: 5824 	 Performance: 0.82 	 Training loss: 7.69
Gradient step: 5849 	 Performance: 0.82 	 Training loss: 7.70
Gradient step: 5874 	 Performance: 0.85 	 Training loss: 7.56
Gradient step: 5899 	 Performance: 0.84 	 Training loss: 7.64
Gradient step: 5924 	 Performance: 0.76 	 Training loss: 7.51
Gradient step: 5949 	 Performance: 0.825 	 Training loss: 7.65
Gradient step: 5974 	 Performance: 0.82 	 Training loss: 7.41
Gradient step: 5999 	 Performance: 0.81 	 Training loss: 7.56
Gradient step: 6024 	 Performance: 0.815 	 Training loss: 7.61
Gradient step: 6049 	 Performance: 0.805 	 Training loss: 7.48
Gradient step: 6074 	 Performance: 0.795 	 Training loss: 7.69
Gradient step: 6099 	 Performance: 0.8 	 Training loss: 7.53
Gradient step: 6124 	 Performance: 0.82 	 Training loss: 7.53
Gradient step: 6149 	 Performance: 0.81 	 Training loss: 7.56
Gradient step: 6174 	 Performance: 0.805 	 Training loss: 7.64
Gradient step: 6199 	 Performance: 0.82 	 Training loss: 7.65
Gradient step: 6224 	 Performance: 0.815 	 Training loss: 7.77
Gradient step: 6249 	 Performance: 0.785 	 Training loss: 7.50
Gradient step: 6274 	 Performance: 0.76 	 Training loss: 7.67
Gradient step: 6299 	 Performance: 0.8 	 Training loss: 7.46
Gradient step: 6324 	 Performance: 0.805 	 Training loss: 7.54
Gradient step: 6349 	 Performance: 0.755 	 Training loss: 7.68
Gradient step: 6374 	 Performance: 0.795 	 Training loss: 7.61
Gradient step: 6399 	 Performance: 0.75 	 Training loss: 7.60
Gradient step: 6424 	 Performance: 0.795 	 Training loss: 7.59
Gradient step: 6449 	 Performance: 0.805 	 Training loss: 7.36
Gradient step: 6474 	 Performance: 0.765 	 Training loss: 7.52
Gradient step: 6499 	 Performance: 0.85 	 Training loss: 7.39
Gradient step: 6524 	 Performance: 0.79 	 Training loss: 7.46
Gradient step: 6549 	 Performance: 0.79 	 Training loss: 7.58
Gradient step: 6574 	 Performance: 0.815 	 Training loss: 7.59
Gradient step: 6599 	 Performance: 0.81 	 Training loss: 7.46
Gradient step: 6624 	 Performance: 0.815 	 Training loss: 7.48
Gradient step: 6649 	 Performance: 0.82 	 Training loss: 7.71
Gradient step: 6674 	 Performance: 0.805 	 Training loss: 7.69
Gradient step: 6699 	 Performance: 0.81 	 Training loss: 7.45
Gradient step: 6724 	 Performance: 0.805 	 Training loss: 7.88
Gradient step: 6749 	 Performance: 0.78 	 Training loss: 7.61
Gradient step: 6774 	 Performance: 0.825 	 Training loss: 7.54
Gradient step: 6799 	 Performance: 0.825 	 Training loss: 7.70
Gradient step: 6824 	 Performance: 0.83 	 Training loss: 7.42
Gradient step: 6849 	 Performance: 0.765 	 Training loss: 7.62
Gradient step: 6874 	 Performance: 0.825 	 Training loss: 7.66
Gradient step: 6899 	 Performance: 0.785 	 Training loss: 7.55
Gradient step: 6924 	 Performance: 0.83 	 Training loss: 7.56
Gradient step: 6949 	 Performance: 0.815 	 Training loss: 7.57
Gradient step: 6974 	 Performance: 0.805 	 Training loss: 7.41
Gradient step: 6999 	 Performance: 0.785 	 Training loss: 7.62
Gradient step: 7024 	 Performance: 0.8 	 Training loss: 7.62
Gradient step: 7049 	 Performance: 0.79 	 Training loss: 7.50
Gradient step: 7074 	 Performance: 0.785 	 Training loss: 7.65
Gradient step: 7099 	 Performance: 0.845 	 Training loss: 7.62
Gradient step: 7124 	 Performance: 0.835 	 Training loss: 7.57
Gradient step: 7149 	 Performance: 0.815 	 Training loss: 7.59
Gradient step: 7174 	 Performance: 0.785 	 Training loss: 7.65
Gradient step: 7199 	 Performance: 0.79 	 Training loss: 7.40
Gradient step: 7224 	 Performance: 0.83 	 Training loss: 7.63
Gradient step: 7249 	 Performance: 0.735 	 Training loss: 7.57
Gradient step: 7274 	 Performance: 0.835 	 Training loss: 7.65
Gradient step: 7299 	 Performance: 0.805 	 Training loss: 7.75
Gradient step: 7324 	 Performance: 0.78 	 Training loss: 7.51
Gradient step: 7349 	 Performance: 0.855 	 Training loss: 7.52
Gradient step: 7374 	 Performance: 0.8 	 Training loss: 7.47
Gradient step: 7399 	 Performance: 0.835 	 Training loss: 7.41
Gradient step: 7424 	 Performance: 0.81 	 Training loss: 7.57
Gradient step: 7449 	 Performance: 0.85 	 Training loss: 7.60
Gradient step: 7474 	 Performance: 0.815 	 Training loss: 7.40
Gradient step: 7499 	 Performance: 0.76 	 Training loss: 7.64
Gradient step: 7524 	 Performance: 0.835 	 Training loss: 7.36
Gradient step: 7549 	 Performance: 0.8 	 Training loss: 7.57
Gradient step: 7574 	 Performance: 0.82 	 Training loss: 7.56
Gradient step: 7599 	 Performance: 0.81 	 Training loss: 7.64
Gradient step: 7624 	 Performance: 0.815 	 Training loss: 7.62
Gradient step: 7649 	 Performance: 0.785 	 Training loss: 7.51
Gradient step: 7674 	 Performance: 0.825 	 Training loss: 7.65
Gradient step: 7699 	 Performance: 0.775 	 Training loss: 7.84
Gradient step: 7724 	 Performance: 0.825 	 Training loss: 7.57
Gradient step: 7749 	 Performance: 0.76 	 Training loss: 7.59
Gradient step: 7774 	 Performance: 0.805 	 Training loss: 7.66
Gradient step: 7799 	 Performance: 0.865 	 Training loss: 7.51
Gradient step: 7824 	 Performance: 0.795 	 Training loss: 7.62
Gradient step: 7849 	 Performance: 0.805 	 Training loss: 7.49
Gradient step: 7874 	 Performance: 0.8 	 Training loss: 7.52
Gradient step: 7899 	 Performance: 0.785 	 Training loss: 7.53
Gradient step: 7924 	 Performance: 0.83 	 Training loss: 7.44
Gradient step: 7949 	 Performance: 0.815 	 Training loss: 7.53
Gradient step: 7974 	 Performance: 0.83 	 Training loss: 7.62
Gradient step: 7999 	 Performance: 0.755 	 Training loss: 7.55
Gradient step: 8024 	 Performance: 0.795 	 Training loss: 7.58
Gradient step: 8049 	 Performance: 0.835 	 Training loss: 7.67
Gradient step: 8074 	 Performance: 0.79 	 Training loss: 7.84
Gradient step: 8099 	 Performance: 0.785 	 Training loss: 7.52
Gradient step: 8124 	 Performance: 0.82 	 Training loss: 7.52
Gradient step: 8149 	 Performance: 0.825 	 Training loss: 7.58
Gradient step: 8174 	 Performance: 0.805 	 Training loss: 7.29
Gradient step: 8199 	 Performance: 0.815 	 Training loss: 7.51
Gradient step: 8224 	 Performance: 0.77 	 Training loss: 7.60
Gradient step: 8249 	 Performance: 0.805 	 Training loss: 7.58
Gradient step: 8274 	 Performance: 0.82 	 Training loss: 7.48
Gradient step: 8299 	 Performance: 0.76 	 Training loss: 7.59
Gradient step: 8324 	 Performance: 0.81 	 Training loss: 7.52
Gradient step: 8349 	 Performance: 0.79 	 Training loss: 7.40
Gradient step: 8374 	 Performance: 0.775 	 Training loss: 7.32
Gradient step: 8399 	 Performance: 0.79 	 Training loss: 7.46
Gradient step: 8424 	 Performance: 0.79 	 Training loss: 7.61
Gradient step: 8449 	 Performance: 0.78 	 Training loss: 7.65
Gradient step: 8474 	 Performance: 0.84 	 Training loss: 7.71
Gradient step: 8499 	 Performance: 0.765 	 Training loss: 7.47
Gradient step: 8524 	 Performance: 0.815 	 Training loss: 7.52
Gradient step: 8549 	 Performance: 0.835 	 Training loss: 7.61
Gradient step: 8574 	 Performance: 0.795 	 Training loss: 7.48
Gradient step: 8599 	 Performance: 0.85 	 Training loss: 7.55
Gradient step: 8624 	 Performance: 0.805 	 Training loss: 7.57
Gradient step: 8649 	 Performance: 0.805 	 Training loss: 7.58
Gradient step: 8674 	 Performance: 0.805 	 Training loss: 7.57
Gradient step: 8699 	 Performance: 0.765 	 Training loss: 7.75
Gradient step: 8724 	 Performance: 0.755 	 Training loss: 7.66
Gradient step: 8749 	 Performance: 0.79 	 Training loss: 7.77
Gradient step: 8774 	 Performance: 0.855 	 Training loss: 7.64
Gradient step: 8799 	 Performance: 0.825 	 Training loss: 7.58
Gradient step: 8824 	 Performance: 0.785 	 Training loss: 7.57
Gradient step: 8849 	 Performance: 0.8 	 Training loss: 7.60
Gradient step: 8874 	 Performance: 0.775 	 Training loss: 7.52
Gradient step: 8899 	 Performance: 0.825 	 Training loss: 7.54
Gradient step: 8924 	 Performance: 0.815 	 Training loss: 7.57
Gradient step: 8949 	 Performance: 0.815 	 Training loss: 7.50
Gradient step: 8974 	 Performance: 0.74 	 Training loss: 7.46
Gradient step: 8999 	 Performance: 0.815 	 Training loss: 7.52
Gradient step: 9024 	 Performance: 0.825 	 Training loss: 7.52
Gradient step: 9049 	 Performance: 0.835 	 Training loss: 7.44
Gradient step: 9074 	 Performance: 0.775 	 Training loss: 7.52
Gradient step: 9099 	 Performance: 0.785 	 Training loss: 7.54
Gradient step: 9124 	 Performance: 0.795 	 Training loss: 7.57
Gradient step: 9149 	 Performance: 0.76 	 Training loss: 7.81
Gradient step: 9174 	 Performance: 0.795 	 Training loss: 7.62
Gradient step: 9199 	 Performance: 0.775 	 Training loss: 7.58
Gradient step: 9224 	 Performance: 0.76 	 Training loss: 7.73
Gradient step: 9249 	 Performance: 0.765 	 Training loss: 7.60
Gradient step: 9274 	 Performance: 0.825 	 Training loss: 7.71
Gradient step: 9299 	 Performance: 0.805 	 Training loss: 7.57
Gradient step: 9324 	 Performance: 0.815 	 Training loss: 7.62
Gradient step: 9349 	 Performance: 0.84 	 Training loss: 7.72
Gradient step: 9374 	 Performance: 0.8 	 Training loss: 7.67
Gradient step: 9399 	 Performance: 0.8 	 Training loss: 7.60
Gradient step: 9424 	 Performance: 0.82 	 Training loss: 7.56
Gradient step: 9449 	 Performance: 0.83 	 Training loss: 7.40
Gradient step: 9474 	 Performance: 0.83 	 Training loss: 7.63
Gradient step: 9499 	 Performance: 0.755 	 Training loss: 7.65
Gradient step: 9524 	 Performance: 0.84 	 Training loss: 7.60
Gradient step: 9549 	 Performance: 0.83 	 Training loss: 7.52
Gradient step: 9574 	 Performance: 0.785 	 Training loss: 7.72
Gradient step: 9599 	 Performance: 0.8 	 Training loss: 7.75
Gradient step: 9624 	 Performance: 0.785 	 Training loss: 7.54
Gradient step: 9649 	 Performance: 0.795 	 Training loss: 7.85
Gradient step: 9674 	 Performance: 0.815 	 Training loss: 7.60
Gradient step: 9699 	 Performance: 0.815 	 Training loss: 7.60
Gradient step: 9724 	 Performance: 0.755 	 Training loss: 7.40
Gradient step: 9749 	 Performance: 0.8 	 Training loss: 7.69
Gradient step: 9774 	 Performance: 0.765 	 Training loss: 7.71
Gradient step: 9799 	 Performance: 0.82 	 Training loss: 7.66
Gradient step: 9824 	 Performance: 0.81 	 Training loss: 7.69
Gradient step: 9849 	 Performance: 0.78 	 Training loss: 7.74
Gradient step: 9874 	 Performance: 0.79 	 Training loss: 7.51
Gradient step: 9899 	 Performance: 0.805 	 Training loss: 7.57
Gradient step: 9924 	 Performance: 0.815 	 Training loss: 7.57
Gradient step: 9949 	 Performance: 0.825 	 Training loss: 7.53
Gradient step: 9974 	 Performance: 0.825 	 Training loss: 7.56
Gradient step: 9999 	 Performance: 0.775 	 Training loss: 7.73
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/None/multidmseqr/None' already exists!
Gradient step: 0 	 Performance: 0.05 	 Training loss: 2.97
Gradient step: 24 	 Performance: 0.05 	 Training loss: 54.68
Gradient step: 49 	 Performance: 0.05 	 Training loss: 37.55
Gradient step: 74 	 Performance: 0.105 	 Training loss: 31.21
Gradient step: 99 	 Performance: 0.17 	 Training loss: 25.61
Gradient step: 124 	 Performance: 0.225 	 Training loss: 22.66
Gradient step: 149 	 Performance: 0.325 	 Training loss: 20.01
Gradient step: 174 	 Performance: 0.34 	 Training loss: 17.67
Gradient step: 199 	 Performance: 0.4 	 Training loss: 16.36
Gradient step: 224 	 Performance: 0.395 	 Training loss: 15.44
Gradient step: 249 	 Performance: 0.42 	 Training loss: 14.96
Gradient step: 274 	 Performance: 0.49 	 Training loss: 14.60
Gradient step: 299 	 Performance: 0.495 	 Training loss: 14.04
Gradient step: 324 	 Performance: 0.52 	 Training loss: 13.54
Gradient step: 349 	 Performance: 0.59 	 Training loss: 13.08
Gradient step: 374 	 Performance: 0.595 	 Training loss: 12.83
Gradient step: 399 	 Performance: 0.58 	 Training loss: 12.51
Gradient step: 424 	 Performance: 0.605 	 Training loss: 12.11
Gradient step: 449 	 Performance: 0.585 	 Training loss: 12.10
Gradient step: 474 	 Performance: 0.625 	 Training loss: 12.05
Gradient step: 499 	 Performance: 0.62 	 Training loss: 11.93
Gradient step: 524 	 Performance: 0.63 	 Training loss: 11.65
Gradient step: 549 	 Performance: 0.665 	 Training loss: 11.23
Gradient step: 574 	 Performance: 0.685 	 Training loss: 11.09
Gradient step: 599 	 Performance: 0.625 	 Training loss: 11.18
Gradient step: 624 	 Performance: 0.535 	 Training loss: 11.06
Gradient step: 649 	 Performance: 0.655 	 Training loss: 10.90
Gradient step: 674 	 Performance: 0.655 	 Training loss: 10.44
Gradient step: 699 	 Performance: 0.685 	 Training loss: 10.69
Gradient step: 724 	 Performance: 0.71 	 Training loss: 10.70
Gradient step: 749 	 Performance: 0.64 	 Training loss: 10.42
Gradient step: 774 	 Performance: 0.77 	 Training loss: 10.16
Gradient step: 799 	 Performance: 0.71 	 Training loss: 10.14
Gradient step: 824 	 Performance: 0.75 	 Training loss: 10.50
Gradient step: 849 	 Performance: 0.7 	 Training loss: 9.67
Gradient step: 874 	 Performance: 0.73 	 Training loss: 10.09
Gradient step: 899 	 Performance: 0.73 	 Training loss: 10.21
Gradient step: 924 	 Performance: 0.675 	 Training loss: 9.87
Gradient step: 949 	 Performance: 0.7 	 Training loss: 10.39
Gradient step: 974 	 Performance: 0.755 	 Training loss: 9.73
Gradient step: 999 	 Performance: 0.735 	 Training loss: 9.78
Gradient step: 1024 	 Performance: 0.76 	 Training loss: 9.67
Gradient step: 1049 	 Performance: 0.725 	 Training loss: 9.41
Gradient step: 1074 	 Performance: 0.79 	 Training loss: 8.85
Gradient step: 1099 	 Performance: 0.745 	 Training loss: 8.51
Gradient step: 1124 	 Performance: 0.79 	 Training loss: 8.69
Gradient step: 1149 	 Performance: 0.805 	 Training loss: 8.61
Gradient step: 1174 	 Performance: 0.765 	 Training loss: 8.55
Gradient step: 1199 	 Performance: 0.76 	 Training loss: 8.51
Gradient step: 1224 	 Performance: 0.76 	 Training loss: 8.40
Gradient step: 1249 	 Performance: 0.77 	 Training loss: 8.34
Gradient step: 1274 	 Performance: 0.76 	 Training loss: 8.36
Gradient step: 1299 	 Performance: 0.745 	 Training loss: 8.49
Gradient step: 1324 	 Performance: 0.725 	 Training loss: 8.62
Gradient step: 1349 	 Performance: 0.735 	 Training loss: 8.46
Gradient step: 1374 	 Performance: 0.71 	 Training loss: 8.44
Gradient step: 1399 	 Performance: 0.755 	 Training loss: 8.37
Gradient step: 1424 	 Performance: 0.77 	 Training loss: 8.43
Gradient step: 1449 	 Performance: 0.735 	 Training loss: 8.17
Gradient step: 1474 	 Performance: 0.725 	 Training loss: 8.37
Gradient step: 1499 	 Performance: 0.8 	 Training loss: 8.23
Gradient step: 1524 	 Performance: 0.745 	 Training loss: 8.30
Gradient step: 1549 	 Performance: 0.765 	 Training loss: 8.24
Gradient step: 1574 	 Performance: 0.76 	 Training loss: 8.23
Gradient step: 1599 	 Performance: 0.785 	 Training loss: 8.23
Gradient step: 1624 	 Performance: 0.805 	 Training loss: 8.45
Gradient step: 1649 	 Performance: 0.81 	 Training loss: 8.04
Gradient step: 1674 	 Performance: 0.77 	 Training loss: 8.20
Gradient step: 1699 	 Performance: 0.7 	 Training loss: 8.13
Gradient step: 1724 	 Performance: 0.755 	 Training loss: 8.14
Gradient step: 1749 	 Performance: 0.82 	 Training loss: 8.20
Gradient step: 1774 	 Performance: 0.755 	 Training loss: 8.06
Gradient step: 1799 	 Performance: 0.8 	 Training loss: 8.24
Gradient step: 1824 	 Performance: 0.825 	 Training loss: 8.12
Gradient step: 1849 	 Performance: 0.82 	 Training loss: 8.04
Gradient step: 1874 	 Performance: 0.775 	 Training loss: 8.24
Gradient step: 1899 	 Performance: 0.825 	 Training loss: 8.11
Gradient step: 1924 	 Performance: 0.795 	 Training loss: 8.27
Gradient step: 1949 	 Performance: 0.73 	 Training loss: 8.05
Gradient step: 1974 	 Performance: 0.805 	 Training loss: 8.57
Gradient step: 1999 	 Performance: 0.8 	 Training loss: 8.02
Gradient step: 2024 	 Performance: 0.79 	 Training loss: 8.24
Gradient step: 2049 	 Performance: 0.79 	 Training loss: 8.21
Gradient step: 2074 	 Performance: 0.74 	 Training loss: 8.27
Gradient step: 2099 	 Performance: 0.76 	 Training loss: 8.22
Gradient step: 2124 	 Performance: 0.74 	 Training loss: 8.45
Gradient step: 2149 	 Performance: 0.74 	 Training loss: 8.17
Gradient step: 2174 	 Performance: 0.795 	 Training loss: 8.04
Gradient step: 2199 	 Performance: 0.805 	 Training loss: 8.00
Gradient step: 2224 	 Performance: 0.78 	 Training loss: 8.20
Gradient step: 2249 	 Performance: 0.795 	 Training loss: 8.31
Gradient step: 2274 	 Performance: 0.845 	 Training loss: 8.24
Gradient step: 2299 	 Performance: 0.775 	 Training loss: 8.08
Gradient step: 2324 	 Performance: 0.7 	 Training loss: 8.07
Gradient step: 2349 	 Performance: 0.805 	 Training loss: 8.28
Gradient step: 2374 	 Performance: 0.79 	 Training loss: 8.20
Gradient step: 2399 	 Performance: 0.795 	 Training loss: 8.23
Gradient step: 2424 	 Performance: 0.77 	 Training loss: 8.07
Gradient step: 2449 	 Performance: 0.745 	 Training loss: 8.30
Gradient step: 2474 	 Performance: 0.745 	 Training loss: 8.18
Gradient step: 2499 	 Performance: 0.8 	 Training loss: 8.28
Gradient step: 2524 	 Performance: 0.775 	 Training loss: 8.22
Gradient step: 2549 	 Performance: 0.785 	 Training loss: 8.17
Gradient step: 2574 	 Performance: 0.79 	 Training loss: 8.25
Gradient step: 2599 	 Performance: 0.74 	 Training loss: 8.11
Gradient step: 2624 	 Performance: 0.69 	 Training loss: 8.35
Gradient step: 2649 	 Performance: 0.73 	 Training loss: 8.20
Gradient step: 2674 	 Performance: 0.815 	 Training loss: 8.22
Gradient step: 2699 	 Performance: 0.73 	 Training loss: 8.13
Gradient step: 2724 	 Performance: 0.76 	 Training loss: 8.10
Gradient step: 2749 	 Performance: 0.73 	 Training loss: 8.26
Gradient step: 2774 	 Performance: 0.735 	 Training loss: 8.16
Gradient step: 2799 	 Performance: 0.77 	 Training loss: 8.34
Gradient step: 2824 	 Performance: 0.74 	 Training loss: 8.15
Gradient step: 2849 	 Performance: 0.815 	 Training loss: 8.07
Gradient step: 2874 	 Performance: 0.78 	 Training loss: 8.18
Gradient step: 2899 	 Performance: 0.775 	 Training loss: 8.14
Gradient step: 2924 	 Performance: 0.77 	 Training loss: 8.24
Gradient step: 2949 	 Performance: 0.735 	 Training loss: 8.19
Gradient step: 2974 	 Performance: 0.775 	 Training loss: 8.23
Gradient step: 2999 	 Performance: 0.77 	 Training loss: 8.03
Gradient step: 3024 	 Performance: 0.755 	 Training loss: 8.40
Gradient step: 3049 	 Performance: 0.75 	 Training loss: 8.18
Gradient step: 3074 	 Performance: 0.78 	 Training loss: 8.26
Gradient step: 3099 	 Performance: 0.715 	 Training loss: 8.19
Gradient step: 3124 	 Performance: 0.77 	 Training loss: 8.32
Gradient step: 3149 	 Performance: 0.73 	 Training loss: 8.15
Gradient step: 3174 	 Performance: 0.805 	 Training loss: 8.13
Gradient step: 3199 	 Performance: 0.78 	 Training loss: 8.10
Gradient step: 3224 	 Performance: 0.75 	 Training loss: 8.13
Gradient step: 3249 	 Performance: 0.755 	 Training loss: 8.35
Gradient step: 3274 	 Performance: 0.725 	 Training loss: 7.97
Gradient step: 3299 	 Performance: 0.795 	 Training loss: 8.21
Gradient step: 3324 	 Performance: 0.81 	 Training loss: 8.25
Gradient step: 3349 	 Performance: 0.755 	 Training loss: 8.21
Gradient step: 3374 	 Performance: 0.805 	 Training loss: 8.25
Gradient step: 3399 	 Performance: 0.805 	 Training loss: 8.28
Gradient step: 3424 	 Performance: 0.805 	 Training loss: 8.22
Gradient step: 3449 	 Performance: 0.81 	 Training loss: 8.30
Gradient step: 3474 	 Performance: 0.75 	 Training loss: 8.36
Gradient step: 3499 	 Performance: 0.765 	 Training loss: 8.07
Gradient step: 3524 	 Performance: 0.695 	 Training loss: 8.23
Gradient step: 3549 	 Performance: 0.755 	 Training loss: 8.42
Gradient step: 3574 	 Performance: 0.75 	 Training loss: 8.09
Gradient step: 3599 	 Performance: 0.725 	 Training loss: 8.12
Gradient step: 3624 	 Performance: 0.775 	 Training loss: 8.14
Gradient step: 3649 	 Performance: 0.83 	 Training loss: 8.21
Gradient step: 3674 	 Performance: 0.745 	 Training loss: 8.31
Gradient step: 3699 	 Performance: 0.805 	 Training loss: 8.00
Gradient step: 3724 	 Performance: 0.745 	 Training loss: 8.12
Gradient step: 3749 	 Performance: 0.795 	 Training loss: 8.24
Gradient step: 3774 	 Performance: 0.8 	 Training loss: 8.21
Gradient step: 3799 	 Performance: 0.79 	 Training loss: 8.23
Gradient step: 3824 	 Performance: 0.77 	 Training loss: 8.23
Gradient step: 3849 	 Performance: 0.8 	 Training loss: 8.06
Gradient step: 3874 	 Performance: 0.78 	 Training loss: 8.23
Gradient step: 3899 	 Performance: 0.765 	 Training loss: 8.24
Gradient step: 3924 	 Performance: 0.81 	 Training loss: 8.10
Gradient step: 3949 	 Performance: 0.785 	 Training loss: 8.17
Gradient step: 3974 	 Performance: 0.8 	 Training loss: 8.05
Gradient step: 3999 	 Performance: 0.725 	 Training loss: 8.21
Gradient step: 4024 	 Performance: 0.75 	 Training loss: 7.94
Gradient step: 4049 	 Performance: 0.775 	 Training loss: 8.15
Gradient step: 4074 	 Performance: 0.81 	 Training loss: 8.31
Gradient step: 4099 	 Performance: 0.785 	 Training loss: 8.12
Gradient step: 4124 	 Performance: 0.775 	 Training loss: 8.16
Gradient step: 4149 	 Performance: 0.78 	 Training loss: 8.10
Gradient step: 4174 	 Performance: 0.79 	 Training loss: 8.25
Gradient step: 4199 	 Performance: 0.795 	 Training loss: 8.15
Gradient step: 4224 	 Performance: 0.76 	 Training loss: 8.13
Gradient step: 4249 	 Performance: 0.795 	 Training loss: 8.10
Gradient step: 4274 	 Performance: 0.8 	 Training loss: 8.12
Gradient step: 4299 	 Performance: 0.83 	 Training loss: 8.07
Gradient step: 4324 	 Performance: 0.74 	 Training loss: 8.00
Gradient step: 4349 	 Performance: 0.82 	 Training loss: 8.36
Gradient step: 4374 	 Performance: 0.81 	 Training loss: 8.37
Gradient step: 4399 	 Performance: 0.79 	 Training loss: 8.23
Gradient step: 4424 	 Performance: 0.715 	 Training loss: 8.22
Gradient step: 4449 	 Performance: 0.76 	 Training loss: 8.33
Gradient step: 4474 	 Performance: 0.8 	 Training loss: 8.25
Gradient step: 4499 	 Performance: 0.81 	 Training loss: 8.07
Gradient step: 4524 	 Performance: 0.85 	 Training loss: 8.10
Gradient step: 4549 	 Performance: 0.77 	 Training loss: 8.13
Gradient step: 4574 	 Performance: 0.78 	 Training loss: 8.24
Gradient step: 4599 	 Performance: 0.805 	 Training loss: 8.21
Gradient step: 4624 	 Performance: 0.705 	 Training loss: 8.19
Gradient step: 4649 	 Performance: 0.725 	 Training loss: 8.19
Gradient step: 4674 	 Performance: 0.775 	 Training loss: 8.27
Gradient step: 4699 	 Performance: 0.73 	 Training loss: 8.16
Gradient step: 4724 	 Performance: 0.825 	 Training loss: 8.14
Gradient step: 4749 	 Performance: 0.765 	 Training loss: 8.03
Gradient step: 4774 	 Performance: 0.8 	 Training loss: 8.24
Gradient step: 4799 	 Performance: 0.78 	 Training loss: 8.22
Gradient step: 4824 	 Performance: 0.745 	 Training loss: 8.11
Gradient step: 4849 	 Performance: 0.75 	 Training loss: 8.29
Gradient step: 4874 	 Performance: 0.805 	 Training loss: 8.30
Gradient step: 4899 	 Performance: 0.805 	 Training loss: 8.09
Gradient step: 4924 	 Performance: 0.8 	 Training loss: 8.22
Gradient step: 4949 	 Performance: 0.77 	 Training loss: 8.16
Gradient step: 4974 	 Performance: 0.77 	 Training loss: 8.17
Gradient step: 4999 	 Performance: 0.765 	 Training loss: 8.21
Gradient step: 5024 	 Performance: 0.77 	 Training loss: 8.33
Gradient step: 5049 	 Performance: 0.755 	 Training loss: 8.31
Gradient step: 5074 	 Performance: 0.75 	 Training loss: 8.16
Gradient step: 5099 	 Performance: 0.79 	 Training loss: 8.32
Gradient step: 5124 	 Performance: 0.78 	 Training loss: 8.07
Gradient step: 5149 	 Performance: 0.76 	 Training loss: 8.14
Gradient step: 5174 	 Performance: 0.78 	 Training loss: 7.95
Gradient step: 5199 	 Performance: 0.79 	 Training loss: 8.19
Gradient step: 5224 	 Performance: 0.765 	 Training loss: 8.10
Gradient step: 5249 	 Performance: 0.75 	 Training loss: 8.11
Gradient step: 5274 	 Performance: 0.775 	 Training loss: 8.25
Gradient step: 5299 	 Performance: 0.74 	 Training loss: 8.22
Gradient step: 5324 	 Performance: 0.755 	 Training loss: 8.11
Gradient step: 5349 	 Performance: 0.755 	 Training loss: 8.12
Gradient step: 5374 	 Performance: 0.795 	 Training loss: 8.12
Gradient step: 5399 	 Performance: 0.805 	 Training loss: 8.39
Gradient step: 5424 	 Performance: 0.685 	 Training loss: 8.23
Gradient step: 5449 	 Performance: 0.755 	 Training loss: 8.10
Gradient step: 5474 	 Performance: 0.715 	 Training loss: 8.10
Gradient step: 5499 	 Performance: 0.765 	 Training loss: 8.05
Gradient step: 5524 	 Performance: 0.76 	 Training loss: 7.98
Gradient step: 5549 	 Performance: 0.735 	 Training loss: 8.30
Gradient step: 5574 	 Performance: 0.73 	 Training loss: 8.16
Gradient step: 5599 	 Performance: 0.73 	 Training loss: 8.16
Gradient step: 5624 	 Performance: 0.81 	 Training loss: 8.11
Gradient step: 5649 	 Performance: 0.78 	 Training loss: 8.14
Gradient step: 5674 	 Performance: 0.735 	 Training loss: 8.24
Gradient step: 5699 	 Performance: 0.77 	 Training loss: 8.11
Gradient step: 5724 	 Performance: 0.75 	 Training loss: 8.29
Gradient step: 5749 	 Performance: 0.705 	 Training loss: 8.21
Gradient step: 5774 	 Performance: 0.745 	 Training loss: 8.01
Gradient step: 5799 	 Performance: 0.785 	 Training loss: 8.12
Gradient step: 5824 	 Performance: 0.79 	 Training loss: 8.04
Gradient step: 5849 	 Performance: 0.81 	 Training loss: 8.15
Gradient step: 5874 	 Performance: 0.735 	 Training loss: 8.20
Gradient step: 5899 	 Performance: 0.765 	 Training loss: 8.18
Gradient step: 5924 	 Performance: 0.775 	 Training loss: 8.22
Gradient step: 5949 	 Performance: 0.75 	 Training loss: 8.34
Gradient step: 5974 	 Performance: 0.75 	 Training loss: 8.43
Gradient step: 5999 	 Performance: 0.765 	 Training loss: 8.32
Gradient step: 6024 	 Performance: 0.79 	 Training loss: 8.15
Gradient step: 6049 	 Performance: 0.835 	 Training loss: 8.27
Gradient step: 6074 	 Performance: 0.695 	 Training loss: 8.20
Gradient step: 6099 	 Performance: 0.795 	 Training loss: 8.08
Gradient step: 6124 	 Performance: 0.77 	 Training loss: 8.12
Gradient step: 6149 	 Performance: 0.78 	 Training loss: 8.09
Gradient step: 6174 	 Performance: 0.78 	 Training loss: 8.19
Gradient step: 6199 	 Performance: 0.74 	 Training loss: 8.30
Gradient step: 6224 	 Performance: 0.815 	 Training loss: 8.13
Gradient step: 6249 	 Performance: 0.79 	 Training loss: 8.22
Gradient step: 6274 	 Performance: 0.78 	 Training loss: 8.23
Gradient step: 6299 	 Performance: 0.8 	 Training loss: 8.06
Gradient step: 6324 	 Performance: 0.81 	 Training loss: 8.20
Gradient step: 6349 	 Performance: 0.78 	 Training loss: 8.13
Gradient step: 6374 	 Performance: 0.795 	 Training loss: 8.09
Gradient step: 6399 	 Performance: 0.715 	 Training loss: 8.06
Gradient step: 6424 	 Performance: 0.795 	 Training loss: 8.16
Gradient step: 6449 	 Performance: 0.72 	 Training loss: 8.17
Gradient step: 6474 	 Performance: 0.77 	 Training loss: 8.12
Gradient step: 6499 	 Performance: 0.77 	 Training loss: 7.99
Gradient step: 6524 	 Performance: 0.755 	 Training loss: 8.08
Gradient step: 6549 	 Performance: 0.74 	 Training loss: 8.25
Gradient step: 6574 	 Performance: 0.795 	 Training loss: 8.30
Gradient step: 6599 	 Performance: 0.78 	 Training loss: 8.39
Gradient step: 6624 	 Performance: 0.79 	 Training loss: 8.32
Gradient step: 6649 	 Performance: 0.84 	 Training loss: 8.21
Gradient step: 6674 	 Performance: 0.76 	 Training loss: 8.13
Gradient step: 6699 	 Performance: 0.775 	 Training loss: 8.17
Gradient step: 6724 	 Performance: 0.795 	 Training loss: 8.22
Gradient step: 6749 	 Performance: 0.74 	 Training loss: 8.09
Gradient step: 6774 	 Performance: 0.795 	 Training loss: 8.19
Gradient step: 6799 	 Performance: 0.715 	 Training loss: 8.29
Gradient step: 6824 	 Performance: 0.795 	 Training loss: 8.08
Gradient step: 6849 	 Performance: 0.815 	 Training loss: 8.09
Gradient step: 6874 	 Performance: 0.795 	 Training loss: 8.12
Gradient step: 6899 	 Performance: 0.835 	 Training loss: 8.28
Gradient step: 6924 	 Performance: 0.815 	 Training loss: 8.22
Gradient step: 6949 	 Performance: 0.76 	 Training loss: 8.30
Gradient step: 6974 	 Performance: 0.74 	 Training loss: 8.12
Gradient step: 6999 	 Performance: 0.78 	 Training loss: 8.11
Gradient step: 7024 	 Performance: 0.785 	 Training loss: 8.17
Gradient step: 7049 	 Performance: 0.725 	 Training loss: 8.52
Gradient step: 7074 	 Performance: 0.785 	 Training loss: 8.06
Gradient step: 7099 	 Performance: 0.795 	 Training loss: 8.15
Gradient step: 7124 	 Performance: 0.84 	 Training loss: 8.17
Gradient step: 7149 	 Performance: 0.785 	 Training loss: 8.12
Gradient step: 7174 	 Performance: 0.755 	 Training loss: 8.16
Gradient step: 7199 	 Performance: 0.775 	 Training loss: 8.18
Gradient step: 7224 	 Performance: 0.765 	 Training loss: 8.06
Gradient step: 7249 	 Performance: 0.8 	 Training loss: 8.13
Gradient step: 7274 	 Performance: 0.74 	 Training loss: 8.22
Gradient step: 7299 	 Performance: 0.78 	 Training loss: 8.23
Gradient step: 7324 	 Performance: 0.785 	 Training loss: 8.20
Gradient step: 7349 	 Performance: 0.76 	 Training loss: 8.20
Gradient step: 7374 	 Performance: 0.765 	 Training loss: 8.21
Gradient step: 7399 	 Performance: 0.805 	 Training loss: 8.14
Gradient step: 7424 	 Performance: 0.745 	 Training loss: 8.11
Gradient step: 7449 	 Performance: 0.755 	 Training loss: 8.25
Gradient step: 7474 	 Performance: 0.77 	 Training loss: 8.00
Gradient step: 7499 	 Performance: 0.805 	 Training loss: 8.24
Gradient step: 7524 	 Performance: 0.81 	 Training loss: 8.27
Gradient step: 7549 	 Performance: 0.775 	 Training loss: 8.22
Gradient step: 7574 	 Performance: 0.795 	 Training loss: 8.16
Gradient step: 7599 	 Performance: 0.785 	 Training loss: 8.14
Gradient step: 7624 	 Performance: 0.775 	 Training loss: 8.20
Gradient step: 7649 	 Performance: 0.775 	 Training loss: 8.24
Gradient step: 7674 	 Performance: 0.795 	 Training loss: 8.02
Gradient step: 7699 	 Performance: 0.79 	 Training loss: 8.30
Gradient step: 7724 	 Performance: 0.745 	 Training loss: 8.35
Gradient step: 7749 	 Performance: 0.805 	 Training loss: 8.17
Gradient step: 7774 	 Performance: 0.78 	 Training loss: 8.28
Gradient step: 7799 	 Performance: 0.8 	 Training loss: 8.24
Gradient step: 7824 	 Performance: 0.815 	 Training loss: 8.17
Gradient step: 7849 	 Performance: 0.795 	 Training loss: 8.36
Gradient step: 7874 	 Performance: 0.74 	 Training loss: 8.20
Gradient step: 7899 	 Performance: 0.755 	 Training loss: 8.15
Gradient step: 7924 	 Performance: 0.745 	 Training loss: 8.03
Gradient step: 7949 	 Performance: 0.735 	 Training loss: 8.13
Gradient step: 7974 	 Performance: 0.805 	 Training loss: 8.29
Gradient step: 7999 	 Performance: 0.77 	 Training loss: 8.20
Gradient step: 8024 	 Performance: 0.765 	 Training loss: 8.30
Gradient step: 8049 	 Performance: 0.7 	 Training loss: 8.21
Gradient step: 8074 	 Performance: 0.815 	 Training loss: 8.23
Gradient step: 8099 	 Performance: 0.795 	 Training loss: 8.14
Gradient step: 8124 	 Performance: 0.79 	 Training loss: 8.15
Gradient step: 8149 	 Performance: 0.735 	 Training loss: 8.34
Gradient step: 8174 	 Performance: 0.825 	 Training loss: 8.20
Gradient step: 8199 	 Performance: 0.77 	 Training loss: 8.22
Gradient step: 8224 	 Performance: 0.795 	 Training loss: 7.94
Gradient step: 8249 	 Performance: 0.745 	 Training loss: 8.13
Gradient step: 8274 	 Performance: 0.775 	 Training loss: 8.12
Gradient step: 8299 	 Performance: 0.78 	 Training loss: 8.37
Gradient step: 8324 	 Performance: 0.795 	 Training loss: 8.26
Gradient step: 8349 	 Performance: 0.765 	 Training loss: 8.17
Gradient step: 8374 	 Performance: 0.82 	 Training loss: 8.06
Gradient step: 8399 	 Performance: 0.775 	 Training loss: 8.10
Gradient step: 8424 	 Performance: 0.8 	 Training loss: 8.31
Gradient step: 8449 	 Performance: 0.775 	 Training loss: 8.29
Gradient step: 8474 	 Performance: 0.8 	 Training loss: 8.01
Gradient step: 8499 	 Performance: 0.775 	 Training loss: 8.18
Gradient step: 8524 	 Performance: 0.775 	 Training loss: 8.49
Gradient step: 8549 	 Performance: 0.8 	 Training loss: 8.20
Gradient step: 8574 	 Performance: 0.8 	 Training loss: 8.22
Gradient step: 8599 	 Performance: 0.81 	 Training loss: 8.34
Gradient step: 8624 	 Performance: 0.745 	 Training loss: 8.22
Gradient step: 8649 	 Performance: 0.72 	 Training loss: 8.27
Gradient step: 8674 	 Performance: 0.71 	 Training loss: 7.96
Gradient step: 8699 	 Performance: 0.74 	 Training loss: 8.21
Gradient step: 8724 	 Performance: 0.77 	 Training loss: 8.14
Gradient step: 8749 	 Performance: 0.795 	 Training loss: 8.16
Gradient step: 8774 	 Performance: 0.795 	 Training loss: 8.26
Gradient step: 8799 	 Performance: 0.765 	 Training loss: 8.19
Gradient step: 8824 	 Performance: 0.79 	 Training loss: 8.32
Gradient step: 8849 	 Performance: 0.75 	 Training loss: 8.22
Gradient step: 8874 	 Performance: 0.78 	 Training loss: 7.99
Gradient step: 8899 	 Performance: 0.755 	 Training loss: 8.13
Gradient step: 8924 	 Performance: 0.815 	 Training loss: 8.28
Gradient step: 8949 	 Performance: 0.73 	 Training loss: 8.20
Gradient step: 8974 	 Performance: 0.785 	 Training loss: 8.29
Gradient step: 8999 	 Performance: 0.765 	 Training loss: 8.09
Gradient step: 9024 	 Performance: 0.78 	 Training loss: 8.33
Gradient step: 9049 	 Performance: 0.81 	 Training loss: 8.18
Gradient step: 9074 	 Performance: 0.805 	 Training loss: 8.08
Gradient step: 9099 	 Performance: 0.795 	 Training loss: 8.14
Gradient step: 9124 	 Performance: 0.805 	 Training loss: 8.25
Gradient step: 9149 	 Performance: 0.755 	 Training loss: 7.97
Gradient step: 9174 	 Performance: 0.765 	 Training loss: 8.09
Gradient step: 9199 	 Performance: 0.74 	 Training loss: 8.18
Gradient step: 9224 	 Performance: 0.79 	 Training loss: 8.27
Gradient step: 9249 	 Performance: 0.775 	 Training loss: 8.13
Gradient step: 9274 	 Performance: 0.815 	 Training loss: 8.15
Gradient step: 9299 	 Performance: 0.75 	 Training loss: 8.09
Gradient step: 9324 	 Performance: 0.84 	 Training loss: 8.07
Gradient step: 9349 	 Performance: 0.745 	 Training loss: 8.24
Gradient step: 9374 	 Performance: 0.775 	 Training loss: 8.38
Gradient step: 9399 	 Performance: 0.76 	 Training loss: 8.08
Gradient step: 9424 	 Performance: 0.73 	 Training loss: 8.17
Gradient step: 9449 	 Performance: 0.785 	 Training loss: 8.14
Gradient step: 9474 	 Performance: 0.78 	 Training loss: 8.37
Gradient step: 9499 	 Performance: 0.755 	 Training loss: 8.15
Gradient step: 9524 	 Performance: 0.785 	 Training loss: 8.10
Gradient step: 9549 	 Performance: 0.77 	 Training loss: 8.10
Gradient step: 9574 	 Performance: 0.755 	 Training loss: 8.25
Gradient step: 9599 	 Performance: 0.765 	 Training loss: 8.21
Gradient step: 9624 	 Performance: 0.76 	 Training loss: 8.17
Gradient step: 9649 	 Performance: 0.775 	 Training loss: 8.10
Gradient step: 9674 	 Performance: 0.8 	 Training loss: 8.12
Gradient step: 9699 	 Performance: 0.755 	 Training loss: 8.11
Gradient step: 9724 	 Performance: 0.795 	 Training loss: 8.23
Gradient step: 9749 	 Performance: 0.81 	 Training loss: 7.93
Gradient step: 9774 	 Performance: 0.76 	 Training loss: 8.41
Gradient step: 9799 	 Performance: 0.755 	 Training loss: 8.21
Gradient step: 9824 	 Performance: 0.775 	 Training loss: 8.15
Gradient step: 9849 	 Performance: 0.745 	 Training loss: 8.22
Gradient step: 9874 	 Performance: 0.765 	 Training loss: 8.20
Gradient step: 9899 	 Performance: 0.755 	 Training loss: 8.20
Gradient step: 9924 	 Performance: 0.755 	 Training loss: 8.18
Gradient step: 9949 	 Performance: 0.675 	 Training loss: 8.34
Gradient step: 9974 	 Performance: 0.795 	 Training loss: 8.10
Gradient step: 9999 	 Performance: 0.775 	 Training loss: 8.19
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/multidmseqr/conformal' created successfully!
Gradient step: 0 	 Performance: 0.045 	 Training loss: 3.17
Gradient step: 24 	 Performance: 0.07 	 Training loss: 63.33
Gradient step: 49 	 Performance: 0.05 	 Training loss: 36.69
Gradient step: 74 	 Performance: 0.065 	 Training loss: 31.41
Gradient step: 99 	 Performance: 0.225 	 Training loss: 25.96
Gradient step: 124 	 Performance: 0.215 	 Training loss: 23.79
Gradient step: 149 	 Performance: 0.205 	 Training loss: 22.33
Gradient step: 174 	 Performance: 0.305 	 Training loss: 21.02
Gradient step: 199 	 Performance: 0.385 	 Training loss: 19.99
Gradient step: 224 	 Performance: 0.425 	 Training loss: 18.74
Gradient step: 249 	 Performance: 0.385 	 Training loss: 18.32
Gradient step: 274 	 Performance: 0.4 	 Training loss: 17.30
Gradient step: 299 	 Performance: 0.405 	 Training loss: 16.21
Gradient step: 324 	 Performance: 0.56 	 Training loss: 15.47
Gradient step: 349 	 Performance: 0.615 	 Training loss: 14.41
Gradient step: 374 	 Performance: 0.62 	 Training loss: 13.61
Gradient step: 399 	 Performance: 0.71 	 Training loss: 12.71
Gradient step: 424 	 Performance: 0.51 	 Training loss: 12.46
Gradient step: 449 	 Performance: 0.72 	 Training loss: 11.94
Gradient step: 474 	 Performance: 0.765 	 Training loss: 11.38
Gradient step: 499 	 Performance: 0.635 	 Training loss: 11.60
Gradient step: 524 	 Performance: 0.765 	 Training loss: 10.90
Gradient step: 549 	 Performance: 0.67 	 Training loss: 10.59
Gradient step: 574 	 Performance: 0.695 	 Training loss: 10.75
Gradient step: 599 	 Performance: 0.715 	 Training loss: 10.00
Gradient step: 624 	 Performance: 0.7 	 Training loss: 10.18
Gradient step: 649 	 Performance: 0.74 	 Training loss: 9.83
Gradient step: 674 	 Performance: 0.71 	 Training loss: 10.01
Gradient step: 699 	 Performance: 0.77 	 Training loss: 9.61
Gradient step: 724 	 Performance: 0.72 	 Training loss: 9.56
Gradient step: 749 	 Performance: 0.765 	 Training loss: 9.47
Gradient step: 774 	 Performance: 0.76 	 Training loss: 9.10
Gradient step: 799 	 Performance: 0.805 	 Training loss: 9.15
Gradient step: 824 	 Performance: 0.78 	 Training loss: 8.81
Gradient step: 849 	 Performance: 0.825 	 Training loss: 8.98
Gradient step: 874 	 Performance: 0.8 	 Training loss: 8.73
Gradient step: 899 	 Performance: 0.825 	 Training loss: 8.92
Gradient step: 924 	 Performance: 0.79 	 Training loss: 8.71
Gradient step: 949 	 Performance: 0.78 	 Training loss: 8.32
Gradient step: 974 	 Performance: 0.765 	 Training loss: 8.45
Gradient step: 999 	 Performance: 0.77 	 Training loss: 8.57
Gradient step: 1024 	 Performance: 0.79 	 Training loss: 8.39
Gradient step: 1049 	 Performance: 0.77 	 Training loss: 8.48
Gradient step: 1074 	 Performance: 0.8 	 Training loss: 7.90
Gradient step: 1099 	 Performance: 0.765 	 Training loss: 8.36
Gradient step: 1124 	 Performance: 0.795 	 Training loss: 8.74
Gradient step: 1149 	 Performance: 0.79 	 Training loss: 7.52
Gradient step: 1174 	 Performance: 0.895 	 Training loss: 7.40
Gradient step: 1199 	 Performance: 0.855 	 Training loss: 7.23
Gradient step: 1224 	 Performance: 0.815 	 Training loss: 6.98
Gradient step: 1249 	 Performance: 0.8 	 Training loss: 7.14
Gradient step: 1274 	 Performance: 0.8 	 Training loss: 6.89
Gradient step: 1299 	 Performance: 0.845 	 Training loss: 7.09
Gradient step: 1324 	 Performance: 0.795 	 Training loss: 7.11
Gradient step: 1349 	 Performance: 0.795 	 Training loss: 6.93
Gradient step: 1374 	 Performance: 0.79 	 Training loss: 7.08
Gradient step: 1399 	 Performance: 0.855 	 Training loss: 6.95
Gradient step: 1424 	 Performance: 0.835 	 Training loss: 6.84
Gradient step: 1449 	 Performance: 0.845 	 Training loss: 6.88
Gradient step: 1474 	 Performance: 0.835 	 Training loss: 6.86
Gradient step: 1499 	 Performance: 0.755 	 Training loss: 6.92
Gradient step: 1524 	 Performance: 0.795 	 Training loss: 6.75
Gradient step: 1549 	 Performance: 0.84 	 Training loss: 6.53
Gradient step: 1574 	 Performance: 0.84 	 Training loss: 6.97
Gradient step: 1599 	 Performance: 0.79 	 Training loss: 6.81
Gradient step: 1624 	 Performance: 0.84 	 Training loss: 6.77
Gradient step: 1649 	 Performance: 0.82 	 Training loss: 6.80
Gradient step: 1674 	 Performance: 0.865 	 Training loss: 6.91
Gradient step: 1699 	 Performance: 0.895 	 Training loss: 6.99
Gradient step: 1724 	 Performance: 0.855 	 Training loss: 6.84
Gradient step: 1749 	 Performance: 0.86 	 Training loss: 6.98
Gradient step: 1774 	 Performance: 0.895 	 Training loss: 6.69
Gradient step: 1799 	 Performance: 0.82 	 Training loss: 6.84
Gradient step: 1824 	 Performance: 0.85 	 Training loss: 6.85
Gradient step: 1849 	 Performance: 0.82 	 Training loss: 6.79
Gradient step: 1874 	 Performance: 0.855 	 Training loss: 6.96
Gradient step: 1899 	 Performance: 0.805 	 Training loss: 6.76
Gradient step: 1924 	 Performance: 0.78 	 Training loss: 6.81
Gradient step: 1949 	 Performance: 0.855 	 Training loss: 6.68
Gradient step: 1974 	 Performance: 0.795 	 Training loss: 6.73
Gradient step: 1999 	 Performance: 0.83 	 Training loss: 7.04
Gradient step: 2024 	 Performance: 0.825 	 Training loss: 6.80
Gradient step: 2049 	 Performance: 0.87 	 Training loss: 7.07
Gradient step: 2074 	 Performance: 0.85 	 Training loss: 6.87
Gradient step: 2099 	 Performance: 0.845 	 Training loss: 6.96
Gradient step: 2124 	 Performance: 0.825 	 Training loss: 6.92
Gradient step: 2149 	 Performance: 0.815 	 Training loss: 6.64
Gradient step: 2174 	 Performance: 0.87 	 Training loss: 6.53
Gradient step: 2199 	 Performance: 0.835 	 Training loss: 6.98
Gradient step: 2224 	 Performance: 0.89 	 Training loss: 7.01
Gradient step: 2249 	 Performance: 0.84 	 Training loss: 6.79
Gradient step: 2274 	 Performance: 0.835 	 Training loss: 6.64
Gradient step: 2299 	 Performance: 0.81 	 Training loss: 6.91
Gradient step: 2324 	 Performance: 0.85 	 Training loss: 6.89
Gradient step: 2349 	 Performance: 0.835 	 Training loss: 6.72
Gradient step: 2374 	 Performance: 0.86 	 Training loss: 6.81
Gradient step: 2399 	 Performance: 0.84 	 Training loss: 6.80
Gradient step: 2424 	 Performance: 0.845 	 Training loss: 6.71
Gradient step: 2449 	 Performance: 0.85 	 Training loss: 6.89
Gradient step: 2474 	 Performance: 0.845 	 Training loss: 6.72
Gradient step: 2499 	 Performance: 0.845 	 Training loss: 6.79
Gradient step: 2524 	 Performance: 0.85 	 Training loss: 6.95
Gradient step: 2549 	 Performance: 0.875 	 Training loss: 6.78
Gradient step: 2574 	 Performance: 0.83 	 Training loss: 6.76
Gradient step: 2599 	 Performance: 0.855 	 Training loss: 6.76
Gradient step: 2624 	 Performance: 0.875 	 Training loss: 6.96
Gradient step: 2649 	 Performance: 0.84 	 Training loss: 6.75
Gradient step: 2674 	 Performance: 0.795 	 Training loss: 6.89
Gradient step: 2699 	 Performance: 0.83 	 Training loss: 7.06
Gradient step: 2724 	 Performance: 0.86 	 Training loss: 6.83
Gradient step: 2749 	 Performance: 0.795 	 Training loss: 6.75
Gradient step: 2774 	 Performance: 0.83 	 Training loss: 6.71
Gradient step: 2799 	 Performance: 0.81 	 Training loss: 6.82
Gradient step: 2824 	 Performance: 0.795 	 Training loss: 6.84
Gradient step: 2849 	 Performance: 0.82 	 Training loss: 7.02
Gradient step: 2874 	 Performance: 0.835 	 Training loss: 6.84
Gradient step: 2899 	 Performance: 0.86 	 Training loss: 7.03
Gradient step: 2924 	 Performance: 0.765 	 Training loss: 6.92
Gradient step: 2949 	 Performance: 0.84 	 Training loss: 6.88
Gradient step: 2974 	 Performance: 0.81 	 Training loss: 6.78
Gradient step: 2999 	 Performance: 0.8 	 Training loss: 6.71
Gradient step: 3024 	 Performance: 0.805 	 Training loss: 6.87
Gradient step: 3049 	 Performance: 0.845 	 Training loss: 6.99
Gradient step: 3074 	 Performance: 0.825 	 Training loss: 6.89
Gradient step: 3099 	 Performance: 0.855 	 Training loss: 6.86
Gradient step: 3124 	 Performance: 0.865 	 Training loss: 6.95
Gradient step: 3149 	 Performance: 0.82 	 Training loss: 6.84
Gradient step: 3174 	 Performance: 0.825 	 Training loss: 6.69
Gradient step: 3199 	 Performance: 0.795 	 Training loss: 6.82
Gradient step: 3224 	 Performance: 0.84 	 Training loss: 6.88
Gradient step: 3249 	 Performance: 0.795 	 Training loss: 6.80
Gradient step: 3274 	 Performance: 0.82 	 Training loss: 6.88
Gradient step: 3299 	 Performance: 0.77 	 Training loss: 6.56
Gradient step: 3324 	 Performance: 0.855 	 Training loss: 6.99
Gradient step: 3349 	 Performance: 0.825 	 Training loss: 6.89
Gradient step: 3374 	 Performance: 0.815 	 Training loss: 6.75
Gradient step: 3399 	 Performance: 0.84 	 Training loss: 6.66
Gradient step: 3424 	 Performance: 0.815 	 Training loss: 6.83
Gradient step: 3449 	 Performance: 0.82 	 Training loss: 6.78
Gradient step: 3474 	 Performance: 0.825 	 Training loss: 6.85
Gradient step: 3499 	 Performance: 0.855 	 Training loss: 6.88
Gradient step: 3524 	 Performance: 0.81 	 Training loss: 6.70
Gradient step: 3549 	 Performance: 0.82 	 Training loss: 6.83
Gradient step: 3574 	 Performance: 0.885 	 Training loss: 6.95
Gradient step: 3599 	 Performance: 0.815 	 Training loss: 6.76
Gradient step: 3624 	 Performance: 0.785 	 Training loss: 6.84
Gradient step: 3649 	 Performance: 0.85 	 Training loss: 6.89
Gradient step: 3674 	 Performance: 0.75 	 Training loss: 6.89
Gradient step: 3699 	 Performance: 0.82 	 Training loss: 6.92
Gradient step: 3724 	 Performance: 0.855 	 Training loss: 6.76
Gradient step: 3749 	 Performance: 0.795 	 Training loss: 6.66
Gradient step: 3774 	 Performance: 0.85 	 Training loss: 6.90
Gradient step: 3799 	 Performance: 0.85 	 Training loss: 6.90
Gradient step: 3824 	 Performance: 0.855 	 Training loss: 6.65
Gradient step: 3849 	 Performance: 0.83 	 Training loss: 6.59
Gradient step: 3874 	 Performance: 0.86 	 Training loss: 6.67
Gradient step: 3899 	 Performance: 0.82 	 Training loss: 6.82
Gradient step: 3924 	 Performance: 0.82 	 Training loss: 6.85
Gradient step: 3949 	 Performance: 0.84 	 Training loss: 6.74
Gradient step: 3974 	 Performance: 0.835 	 Training loss: 6.59
Gradient step: 3999 	 Performance: 0.835 	 Training loss: 6.69
Gradient step: 4024 	 Performance: 0.85 	 Training loss: 6.77
Gradient step: 4049 	 Performance: 0.805 	 Training loss: 6.69
Gradient step: 4074 	 Performance: 0.88 	 Training loss: 6.85
Gradient step: 4099 	 Performance: 0.85 	 Training loss: 6.57
Gradient step: 4124 	 Performance: 0.845 	 Training loss: 6.77
Gradient step: 4149 	 Performance: 0.845 	 Training loss: 6.79
Gradient step: 4174 	 Performance: 0.84 	 Training loss: 6.54
Gradient step: 4199 	 Performance: 0.855 	 Training loss: 6.72
Gradient step: 4224 	 Performance: 0.885 	 Training loss: 6.80
Gradient step: 4249 	 Performance: 0.88 	 Training loss: 6.89
Gradient step: 4274 	 Performance: 0.825 	 Training loss: 6.81
Gradient step: 4299 	 Performance: 0.82 	 Training loss: 6.95
Gradient step: 4324 	 Performance: 0.88 	 Training loss: 6.80
Gradient step: 4349 	 Performance: 0.825 	 Training loss: 6.50
Gradient step: 4374 	 Performance: 0.85 	 Training loss: 6.88
Gradient step: 4399 	 Performance: 0.9 	 Training loss: 6.62
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/multidmseqr/None' created successfully!
Gradient step: 0 	 Performance: 0.045 	 Training loss: 2.97
Gradient step: 24 	 Performance: 0.085 	 Training loss: 54.01
Gradient step: 49 	 Performance: 0.045 	 Training loss: 37.74
Gradient step: 74 	 Performance: 0.075 	 Training loss: 32.16
Gradient step: 99 	 Performance: 0.21 	 Training loss: 26.82
Gradient step: 124 	 Performance: 0.27 	 Training loss: 22.72
Gradient step: 149 	 Performance: 0.355 	 Training loss: 19.34
Gradient step: 174 	 Performance: 0.445 	 Training loss: 17.24
Gradient step: 199 	 Performance: 0.425 	 Training loss: 16.06
Gradient step: 224 	 Performance: 0.48 	 Training loss: 15.08
Gradient step: 249 	 Performance: 0.605 	 Training loss: 14.71
Gradient step: 274 	 Performance: 0.555 	 Training loss: 14.03
Gradient step: 299 	 Performance: 0.605 	 Training loss: 13.58
Gradient step: 324 	 Performance: 0.585 	 Training loss: 13.07
Gradient step: 349 	 Performance: 0.505 	 Training loss: 12.78
Gradient step: 374 	 Performance: 0.59 	 Training loss: 12.32
Gradient step: 399 	 Performance: 0.56 	 Training loss: 12.08
Gradient step: 424 	 Performance: 0.635 	 Training loss: 12.05
Gradient step: 449 	 Performance: 0.645 	 Training loss: 11.59
Gradient step: 474 	 Performance: 0.63 	 Training loss: 11.58
Gradient step: 499 	 Performance: 0.64 	 Training loss: 11.19
Gradient step: 524 	 Performance: 0.67 	 Training loss: 11.21
Gradient step: 549 	 Performance: 0.7 	 Training loss: 10.90
Gradient step: 574 	 Performance: 0.73 	 Training loss: 10.93
Gradient step: 599 	 Performance: 0.665 	 Training loss: 10.80
Gradient step: 624 	 Performance: 0.705 	 Training loss: 10.94
Gradient step: 649 	 Performance: 0.73 	 Training loss: 10.84
Gradient step: 674 	 Performance: 0.705 	 Training loss: 10.63
Gradient step: 699 	 Performance: 0.79 	 Training loss: 10.26
Gradient step: 724 	 Performance: 0.74 	 Training loss: 10.35
Gradient step: 749 	 Performance: 0.765 	 Training loss: 10.18
Gradient step: 774 	 Performance: 0.625 	 Training loss: 10.10
Gradient step: 799 	 Performance: 0.715 	 Training loss: 10.51
Gradient step: 824 	 Performance: 0.715 	 Training loss: 9.84
Gradient step: 849 	 Performance: 0.74 	 Training loss: 9.90
Gradient step: 874 	 Performance: 0.735 	 Training loss: 9.92
Gradient step: 899 	 Performance: 0.78 	 Training loss: 9.78
Gradient step: 924 	 Performance: 0.71 	 Training loss: 9.94
Gradient step: 949 	 Performance: 0.705 	 Training loss: 9.76
Gradient step: 974 	 Performance: 0.725 	 Training loss: 10.02
Gradient step: 999 	 Performance: 0.765 	 Training loss: 9.29
Gradient step: 1024 	 Performance: 0.8 	 Training loss: 8.55
Gradient step: 1049 	 Performance: 0.755 	 Training loss: 8.73
Gradient step: 1074 	 Performance: 0.745 	 Training loss: 8.75
Gradient step: 1099 	 Performance: 0.76 	 Training loss: 8.33
Gradient step: 1124 	 Performance: 0.8 	 Training loss: 8.44
Gradient step: 1149 	 Performance: 0.78 	 Training loss: 8.49
Gradient step: 1174 	 Performance: 0.805 	 Training loss: 8.53
Gradient step: 1199 	 Performance: 0.765 	 Training loss: 8.46
Gradient step: 1224 	 Performance: 0.775 	 Training loss: 8.58
Gradient step: 1249 	 Performance: 0.77 	 Training loss: 8.39
Gradient step: 1274 	 Performance: 0.82 	 Training loss: 8.51
Gradient step: 1299 	 Performance: 0.83 	 Training loss: 8.38
Gradient step: 1324 	 Performance: 0.79 	 Training loss: 8.28
Gradient step: 1349 	 Performance: 0.765 	 Training loss: 8.56
Gradient step: 1374 	 Performance: 0.745 	 Training loss: 8.25
Gradient step: 1399 	 Performance: 0.81 	 Training loss: 8.63
Gradient step: 1424 	 Performance: 0.795 	 Training loss: 8.23
Gradient step: 1449 	 Performance: 0.79 	 Training loss: 8.39
Gradient step: 1474 	 Performance: 0.78 	 Training loss: 8.19
Gradient step: 1499 	 Performance: 0.835 	 Training loss: 8.36
Gradient step: 1524 	 Performance: 0.745 	 Training loss: 8.26
Gradient step: 1549 	 Performance: 0.77 	 Training loss: 8.28
Gradient step: 1574 	 Performance: 0.84 	 Training loss: 8.36
Gradient step: 1599 	 Performance: 0.825 	 Training loss: 8.23
Gradient step: 1624 	 Performance: 0.75 	 Training loss: 8.30
Gradient step: 1649 	 Performance: 0.785 	 Training loss: 8.24
Gradient step: 1674 	 Performance: 0.795 	 Training loss: 8.20
Gradient step: 1699 	 Performance: 0.83 	 Training loss: 8.18
Gradient step: 1724 	 Performance: 0.83 	 Training loss: 8.24
Gradient step: 1749 	 Performance: 0.78 	 Training loss: 8.18
Gradient step: 1774 	 Performance: 0.77 	 Training loss: 8.22
Gradient step: 1799 	 Performance: 0.835 	 Training loss: 8.25
Gradient step: 1824 	 Performance: 0.765 	 Training loss: 8.21
Gradient step: 1849 	 Performance: 0.87 	 Training loss: 8.39
Gradient step: 1874 	 Performance: 0.755 	 Training loss: 8.09
Gradient step: 1899 	 Performance: 0.78 	 Training loss: 8.47
Gradient step: 1924 	 Performance: 0.78 	 Training loss: 8.19
Gradient step: 1949 	 Performance: 0.825 	 Training loss: 7.98
Gradient step: 1974 	 Performance: 0.775 	 Training loss: 8.18
Gradient step: 1999 	 Performance: 0.805 	 Training loss: 8.05
Gradient step: 2024 	 Performance: 0.75 	 Training loss: 8.03
Gradient step: 2049 	 Performance: 0.72 	 Training loss: 8.01
Gradient step: 2074 	 Performance: 0.815 	 Training loss: 7.99
Gradient step: 2099 	 Performance: 0.795 	 Training loss: 8.05
Gradient step: 2124 	 Performance: 0.795 	 Training loss: 8.39
Gradient step: 2149 	 Performance: 0.795 	 Training loss: 8.09
Gradient step: 2174 	 Performance: 0.805 	 Training loss: 7.86
Gradient step: 2199 	 Performance: 0.805 	 Training loss: 7.87
Gradient step: 2224 	 Performance: 0.83 	 Training loss: 7.95
Gradient step: 2249 	 Performance: 0.81 	 Training loss: 7.97
Gradient step: 2274 	 Performance: 0.85 	 Training loss: 8.00
Gradient step: 2299 	 Performance: 0.775 	 Training loss: 7.88
Gradient step: 2324 	 Performance: 0.78 	 Training loss: 7.81
Gradient step: 2349 	 Performance: 0.775 	 Training loss: 7.91
Gradient step: 2374 	 Performance: 0.78 	 Training loss: 7.80
Gradient step: 2399 	 Performance: 0.825 	 Training loss: 7.77
Gradient step: 2424 	 Performance: 0.785 	 Training loss: 7.96
Gradient step: 2449 	 Performance: 0.81 	 Training loss: 7.92
Gradient step: 2474 	 Performance: 0.815 	 Training loss: 7.88
Gradient step: 2499 	 Performance: 0.78 	 Training loss: 8.16
Gradient step: 2524 	 Performance: 0.825 	 Training loss: 7.80
Gradient step: 2549 	 Performance: 0.8 	 Training loss: 7.83
Gradient step: 2574 	 Performance: 0.845 	 Training loss: 7.92
Gradient step: 2599 	 Performance: 0.81 	 Training loss: 7.89
Gradient step: 2624 	 Performance: 0.795 	 Training loss: 7.90
Gradient step: 2649 	 Performance: 0.76 	 Training loss: 7.93
Gradient step: 2674 	 Performance: 0.81 	 Training loss: 7.79
Gradient step: 2699 	 Performance: 0.745 	 Training loss: 7.89
Gradient step: 2724 	 Performance: 0.845 	 Training loss: 7.95
Gradient step: 2749 	 Performance: 0.795 	 Training loss: 7.94
Gradient step: 2774 	 Performance: 0.79 	 Training loss: 7.68
Gradient step: 2799 	 Performance: 0.775 	 Training loss: 7.76
Gradient step: 2824 	 Performance: 0.765 	 Training loss: 7.90
Gradient step: 2849 	 Performance: 0.84 	 Training loss: 7.78
Gradient step: 2874 	 Performance: 0.79 	 Training loss: 7.84
Gradient step: 2899 	 Performance: 0.815 	 Training loss: 7.89
Gradient step: 2924 	 Performance: 0.81 	 Training loss: 7.85
Gradient step: 2949 	 Performance: 0.825 	 Training loss: 8.06
Gradient step: 2974 	 Performance: 0.835 	 Training loss: 7.69
Gradient step: 2999 	 Performance: 0.82 	 Training loss: 7.85
Gradient step: 3024 	 Performance: 0.84 	 Training loss: 7.76
Gradient step: 3049 	 Performance: 0.81 	 Training loss: 8.13
Gradient step: 3074 	 Performance: 0.79 	 Training loss: 8.05
Gradient step: 3099 	 Performance: 0.8 	 Training loss: 7.91
Gradient step: 3124 	 Performance: 0.775 	 Training loss: 7.97
Gradient step: 3149 	 Performance: 0.8 	 Training loss: 7.72
Gradient step: 3174 	 Performance: 0.8 	 Training loss: 7.66
Gradient step: 3199 	 Performance: 0.865 	 Training loss: 7.94
Gradient step: 3224 	 Performance: 0.79 	 Training loss: 7.89
Gradient step: 3249 	 Performance: 0.855 	 Training loss: 7.96
Gradient step: 3274 	 Performance: 0.775 	 Training loss: 7.95
Gradient step: 3299 	 Performance: 0.83 	 Training loss: 8.11
Gradient step: 3324 	 Performance: 0.77 	 Training loss: 7.84
Gradient step: 3349 	 Performance: 0.8 	 Training loss: 7.86
Gradient step: 3374 	 Performance: 0.795 	 Training loss: 7.86
Gradient step: 3399 	 Performance: 0.795 	 Training loss: 7.98
Gradient step: 3424 	 Performance: 0.79 	 Training loss: 7.87
Gradient step: 3449 	 Performance: 0.85 	 Training loss: 7.94
Gradient step: 3474 	 Performance: 0.775 	 Training loss: 8.06
Gradient step: 3499 	 Performance: 0.875 	 Training loss: 7.79
Gradient step: 3524 	 Performance: 0.795 	 Training loss: 7.89
Gradient step: 3549 	 Performance: 0.865 	 Training loss: 7.64
Gradient step: 3574 	 Performance: 0.78 	 Training loss: 8.13
Gradient step: 3599 	 Performance: 0.83 	 Training loss: 7.83
Gradient step: 3624 	 Performance: 0.825 	 Training loss: 7.71
Gradient step: 3649 	 Performance: 0.81 	 Training loss: 7.98
Gradient step: 3674 	 Performance: 0.785 	 Training loss: 7.66
Gradient step: 3699 	 Performance: 0.79 	 Training loss: 7.87
Gradient step: 3724 	 Performance: 0.815 	 Training loss: 7.99
Gradient step: 3749 	 Performance: 0.83 	 Training loss: 8.20
Gradient step: 3774 	 Performance: 0.835 	 Training loss: 7.79
Gradient step: 3799 	 Performance: 0.81 	 Training loss: 7.95
Gradient step: 3824 	 Performance: 0.735 	 Training loss: 7.83
Gradient step: 3849 	 Performance: 0.805 	 Training loss: 7.70
Gradient step: 3874 	 Performance: 0.775 	 Training loss: 7.96
Gradient step: 3899 	 Performance: 0.835 	 Training loss: 7.78
Gradient step: 3924 	 Performance: 0.795 	 Training loss: 8.01
Gradient step: 3949 	 Performance: 0.82 	 Training loss: 7.97
Gradient step: 3974 	 Performance: 0.835 	 Training loss: 8.02
Gradient step: 3999 	 Performance: 0.815 	 Training loss: 7.95
Gradient step: 4024 	 Performance: 0.8 	 Training loss: 7.70
Gradient step: 4049 	 Performance: 0.815 	 Training loss: 7.97
Gradient step: 4074 	 Performance: 0.825 	 Training loss: 7.92
Gradient step: 4099 	 Performance: 0.75 	 Training loss: 8.00
Gradient step: 4124 	 Performance: 0.75 	 Training loss: 8.09
Gradient step: 4149 	 Performance: 0.785 	 Training loss: 7.88
Gradient step: 4174 	 Performance: 0.845 	 Training loss: 7.92
Gradient step: 4199 	 Performance: 0.76 	 Training loss: 7.82
Gradient step: 4224 	 Performance: 0.81 	 Training loss: 8.02
Gradient step: 4249 	 Performance: 0.84 	 Training loss: 7.89
Gradient step: 4274 	 Performance: 0.8 	 Training loss: 7.91
Gradient step: 4299 	 Performance: 0.83 	 Training loss: 7.76
Gradient step: 4324 	 Performance: 0.8 	 Training loss: 7.71
Gradient step: 4349 	 Performance: 0.765 	 Training loss: 7.78
Gradient step: 4374 	 Performance: 0.815 	 Training loss: 7.95
Gradient step: 4399 	 Performance: 0.82 	 Training loss: 7.89
Gradient step: 4424 	 Performance: 0.815 	 Training loss: 7.99
Gradient step: 4449 	 Performance: 0.815 	 Training loss: 7.84
Gradient step: 4474 	 Performance: 0.78 	 Training loss: 7.96
Gradient step: 4499 	 Performance: 0.78 	 Training loss: 7.85
Gradient step: 4524 	 Performance: 0.795 	 Training loss: 7.69
Gradient step: 4549 	 Performance: 0.825 	 Training loss: 8.08
Gradient step: 4574 	 Performance: 0.755 	 Training loss: 7.81
Gradient step: 4599 	 Performance: 0.78 	 Training loss: 7.98
Gradient step: 4624 	 Performance: 0.835 	 Training loss: 7.69
Gradient step: 4649 	 Performance: 0.855 	 Training loss: 7.77
Gradient step: 4674 	 Performance: 0.84 	 Training loss: 7.92
Gradient step: 4699 	 Performance: 0.84 	 Training loss: 7.84
Gradient step: 4724 	 Performance: 0.815 	 Training loss: 7.93
Gradient step: 4749 	 Performance: 0.785 	 Training loss: 8.02
Gradient step: 4774 	 Performance: 0.76 	 Training loss: 7.74
Gradient step: 4799 	 Performance: 0.815 	 Training loss: 7.93
Gradient step: 4824 	 Performance: 0.775 	 Training loss: 7.80
Gradient step: 4849 	 Performance: 0.805 	 Training loss: 7.90
Gradient step: 4874 	 Performance: 0.79 	 Training loss: 7.80
Gradient step: 4899 	 Performance: 0.775 	 Training loss: 7.92
Gradient step: 4924 	 Performance: 0.775 	 Training loss: 7.76
Gradient step: 4949 	 Performance: 0.795 	 Training loss: 8.06
Gradient step: 4974 	 Performance: 0.78 	 Training loss: 8.00
Gradient step: 4999 	 Performance: 0.735 	 Training loss: 8.01
Gradient step: 5024 	 Performance: 0.81 	 Training loss: 7.85
Gradient step: 5049 	 Performance: 0.845 	 Training loss: 7.99
Gradient step: 5074 	 Performance: 0.84 	 Training loss: 7.85
Gradient step: 5099 	 Performance: 0.81 	 Training loss: 7.90
Gradient step: 5124 	 Performance: 0.755 	 Training loss: 7.88
Gradient step: 5149 	 Performance: 0.845 	 Training loss: 7.82
Gradient step: 5174 	 Performance: 0.82 	 Training loss: 7.89
Gradient step: 5199 	 Performance: 0.8 	 Training loss: 7.79
Gradient step: 5224 	 Performance: 0.785 	 Training loss: 7.83
Gradient step: 5249 	 Performance: 0.805 	 Training loss: 7.81
Gradient step: 5274 	 Performance: 0.83 	 Training loss: 7.71
Gradient step: 5299 	 Performance: 0.77 	 Training loss: 7.97
Gradient step: 5324 	 Performance: 0.81 	 Training loss: 7.79
Gradient step: 5349 	 Performance: 0.81 	 Training loss: 7.87
Gradient step: 5374 	 Performance: 0.82 	 Training loss: 7.98
Gradient step: 5399 	 Performance: 0.77 	 Training loss: 7.82
Gradient step: 5424 	 Performance: 0.8 	 Training loss: 8.05
Gradient step: 5449 	 Performance: 0.81 	 Training loss: 8.04
Gradient step: 5474 	 Performance: 0.78 	 Training loss: 8.10
Gradient step: 5499 	 Performance: 0.82 	 Training loss: 7.85
Gradient step: 5524 	 Performance: 0.765 	 Training loss: 7.81
Gradient step: 5549 	 Performance: 0.78 	 Training loss: 7.86
Gradient step: 5574 	 Performance: 0.76 	 Training loss: 7.88
Gradient step: 5599 	 Performance: 0.85 	 Training loss: 7.89
Gradient step: 5624 	 Performance: 0.77 	 Training loss: 8.12
Gradient step: 5649 	 Performance: 0.835 	 Training loss: 7.74
Gradient step: 5674 	 Performance: 0.815 	 Training loss: 7.88
Gradient step: 5699 	 Performance: 0.795 	 Training loss: 7.83
Gradient step: 5724 	 Performance: 0.795 	 Training loss: 7.84
Gradient step: 5749 	 Performance: 0.8 	 Training loss: 7.86
Gradient step: 5774 	 Performance: 0.78 	 Training loss: 7.76
Gradient step: 5799 	 Performance: 0.84 	 Training loss: 7.88
Gradient step: 5824 	 Performance: 0.795 	 Training loss: 7.75
Gradient step: 5849 	 Performance: 0.805 	 Training loss: 7.90
Gradient step: 5874 	 Performance: 0.81 	 Training loss: 7.73
Gradient step: 5899 	 Performance: 0.765 	 Training loss: 7.81
Gradient step: 5924 	 Performance: 0.785 	 Training loss: 7.72
Gradient step: 5949 	 Performance: 0.825 	 Training loss: 7.83
Gradient step: 5974 	 Performance: 0.75 	 Training loss: 7.97
Gradient step: 5999 	 Performance: 0.75 	 Training loss: 7.89
Gradient step: 6024 	 Performance: 0.785 	 Training loss: 7.92
Gradient step: 6049 	 Performance: 0.81 	 Training loss: 7.87
Gradient step: 6074 	 Performance: 0.82 	 Training loss: 7.81
Gradient step: 6099 	 Performance: 0.765 	 Training loss: 7.94
Gradient step: 6124 	 Performance: 0.815 	 Training loss: 7.89
Gradient step: 6149 	 Performance: 0.815 	 Training loss: 8.02
Gradient step: 6174 	 Performance: 0.845 	 Training loss: 7.83
Gradient step: 6199 	 Performance: 0.815 	 Training loss: 7.80
Gradient step: 6224 	 Performance: 0.85 	 Training loss: 7.94
Gradient step: 6249 	 Performance: 0.775 	 Training loss: 8.05
Gradient step: 6274 	 Performance: 0.78 	 Training loss: 8.10
Gradient step: 6299 	 Performance: 0.735 	 Training loss: 7.91
Gradient step: 6324 	 Performance: 0.765 	 Training loss: 7.82
Gradient step: 6349 	 Performance: 0.81 	 Training loss: 7.66
Gradient step: 6374 	 Performance: 0.81 	 Training loss: 7.87
Gradient step: 6399 	 Performance: 0.785 	 Training loss: 8.07
Gradient step: 6424 	 Performance: 0.76 	 Training loss: 7.84
Gradient step: 6449 	 Performance: 0.81 	 Training loss: 7.86
Gradient step: 6474 	 Performance: 0.8 	 Training loss: 7.99
Gradient step: 6499 	 Performance: 0.835 	 Training loss: 7.79
Gradient step: 6524 	 Performance: 0.785 	 Training loss: 7.96
Gradient step: 6549 	 Performance: 0.79 	 Training loss: 7.94
Gradient step: 6574 	 Performance: 0.78 	 Training loss: 7.97
Gradient step: 6599 	 Performance: 0.82 	 Training loss: 8.04
Gradient step: 6624 	 Performance: 0.845 	 Training loss: 8.13
Gradient step: 6649 	 Performance: 0.87 	 Training loss: 7.72
Gradient step: 6674 	 Performance: 0.8 	 Training loss: 7.87
Gradient step: 6699 	 Performance: 0.795 	 Training loss: 7.76
Gradient step: 6724 	 Performance: 0.815 	 Training loss: 8.09
Gradient step: 6749 	 Performance: 0.81 	 Training loss: 7.99
Gradient step: 6774 	 Performance: 0.83 	 Training loss: 7.80
Gradient step: 6799 	 Performance: 0.77 	 Training loss: 7.75
Gradient step: 6824 	 Performance: 0.825 	 Training loss: 7.93
Gradient step: 6849 	 Performance: 0.825 	 Training loss: 7.81
Gradient step: 6874 	 Performance: 0.77 	 Training loss: 7.88
Gradient step: 6899 	 Performance: 0.75 	 Training loss: 8.07
Gradient step: 6924 	 Performance: 0.82 	 Training loss: 8.03
Gradient step: 6949 	 Performance: 0.79 	 Training loss: 8.10
Gradient step: 6974 	 Performance: 0.83 	 Training loss: 7.86
Gradient step: 6999 	 Performance: 0.79 	 Training loss: 7.93
Gradient step: 7024 	 Performance: 0.73 	 Training loss: 7.86
Gradient step: 7049 	 Performance: 0.81 	 Training loss: 7.88
Gradient step: 7074 	 Performance: 0.805 	 Training loss: 7.96
Gradient step: 7099 	 Performance: 0.815 	 Training loss: 7.66
Gradient step: 7124 	 Performance: 0.815 	 Training loss: 7.97
Gradient step: 7149 	 Performance: 0.8 	 Training loss: 7.90
Gradient step: 7174 	 Performance: 0.815 	 Training loss: 7.84
Gradient step: 7199 	 Performance: 0.775 	 Training loss: 7.96
Gradient step: 7224 	 Performance: 0.825 	 Training loss: 7.77
Gradient step: 7249 	 Performance: 0.785 	 Training loss: 7.84
Gradient step: 7274 	 Performance: 0.84 	 Training loss: 7.80
Gradient step: 7299 	 Performance: 0.81 	 Training loss: 7.97
Gradient step: 7324 	 Performance: 0.795 	 Training loss: 8.00
Gradient step: 7349 	 Performance: 0.795 	 Training loss: 8.18
Gradient step: 7374 	 Performance: 0.87 	 Training loss: 7.98
Gradient step: 7399 	 Performance: 0.82 	 Training loss: 7.96
Gradient step: 7424 	 Performance: 0.805 	 Training loss: 7.77
Gradient step: 7449 	 Performance: 0.775 	 Training loss: 7.80
Gradient step: 7474 	 Performance: 0.735 	 Training loss: 7.61
Gradient step: 7499 	 Performance: 0.81 	 Training loss: 7.69
Gradient step: 7524 	 Performance: 0.785 	 Training loss: 7.94
Gradient step: 7549 	 Performance: 0.815 	 Training loss: 7.98
Gradient step: 7574 	 Performance: 0.75 	 Training loss: 7.92
Gradient step: 7599 	 Performance: 0.805 	 Training loss: 7.94
Gradient step: 7624 	 Performance: 0.735 	 Training loss: 7.96
Gradient step: 7649 	 Performance: 0.86 	 Training loss: 8.06
Gradient step: 7674 	 Performance: 0.815 	 Training loss: 7.90
Gradient step: 7699 	 Performance: 0.735 	 Training loss: 7.79
Gradient step: 7724 	 Performance: 0.825 	 Training loss: 7.89
Gradient step: 7749 	 Performance: 0.83 	 Training loss: 7.98
Gradient step: 7774 	 Performance: 0.805 	 Training loss: 8.01
Gradient step: 7799 	 Performance: 0.8 	 Training loss: 7.81
Gradient step: 7824 	 Performance: 0.795 	 Training loss: 7.85
Gradient step: 7849 	 Performance: 0.77 	 Training loss: 8.17
Gradient step: 7874 	 Performance: 0.81 	 Training loss: 7.89
Gradient step: 7899 	 Performance: 0.815 	 Training loss: 7.79
Gradient step: 7924 	 Performance: 0.83 	 Training loss: 7.93
Gradient step: 7949 	 Performance: 0.845 	 Training loss: 7.86
Gradient step: 7974 	 Performance: 0.755 	 Training loss: 7.85
Gradient step: 7999 	 Performance: 0.77 	 Training loss: 7.91
Gradient step: 8024 	 Performance: 0.77 	 Training loss: 7.93
Gradient step: 8049 	 Performance: 0.805 	 Training loss: 7.91
Gradient step: 8074 	 Performance: 0.845 	 Training loss: 7.91
Gradient step: 8099 	 Performance: 0.83 	 Training loss: 7.91
Gradient step: 8124 	 Performance: 0.8 	 Training loss: 7.88
Gradient step: 8149 	 Performance: 0.85 	 Training loss: 7.90
Gradient step: 8174 	 Performance: 0.795 	 Training loss: 7.84
Gradient step: 8199 	 Performance: 0.8 	 Training loss: 7.71
Gradient step: 8224 	 Performance: 0.79 	 Training loss: 7.88
Gradient step: 8249 	 Performance: 0.795 	 Training loss: 8.15
Gradient step: 8274 	 Performance: 0.765 	 Training loss: 7.84
Gradient step: 8299 	 Performance: 0.82 	 Training loss: 7.94
Gradient step: 8324 	 Performance: 0.8 	 Training loss: 8.07
Gradient step: 8349 	 Performance: 0.82 	 Training loss: 7.82
Gradient step: 8374 	 Performance: 0.8 	 Training loss: 7.79
Gradient step: 8399 	 Performance: 0.795 	 Training loss: 8.00
Gradient step: 8424 	 Performance: 0.8 	 Training loss: 7.77
Gradient step: 8449 	 Performance: 0.8 	 Training loss: 7.86
Gradient step: 8474 	 Performance: 0.79 	 Training loss: 7.73
Gradient step: 8499 	 Performance: 0.79 	 Training loss: 8.05
Gradient step: 8524 	 Performance: 0.765 	 Training loss: 7.74
Gradient step: 8549 	 Performance: 0.79 	 Training loss: 7.78
Gradient step: 8574 	 Performance: 0.85 	 Training loss: 7.91
Gradient step: 8599 	 Performance: 0.795 	 Training loss: 7.82
Gradient step: 8624 	 Performance: 0.825 	 Training loss: 7.74
Gradient step: 8649 	 Performance: 0.86 	 Training loss: 7.92
Gradient step: 8674 	 Performance: 0.815 	 Training loss: 7.68
Gradient step: 8699 	 Performance: 0.755 	 Training loss: 7.82
Gradient step: 8724 	 Performance: 0.78 	 Training loss: 7.71
Gradient step: 8749 	 Performance: 0.785 	 Training loss: 7.87
Gradient step: 8774 	 Performance: 0.78 	 Training loss: 8.00
Gradient step: 8799 	 Performance: 0.76 	 Training loss: 7.79
Gradient step: 8824 	 Performance: 0.825 	 Training loss: 7.84
Gradient step: 8849 	 Performance: 0.77 	 Training loss: 7.94
Gradient step: 8874 	 Performance: 0.815 	 Training loss: 8.01
Gradient step: 8899 	 Performance: 0.81 	 Training loss: 7.79
Gradient step: 8924 	 Performance: 0.83 	 Training loss: 7.92
Gradient step: 8949 	 Performance: 0.78 	 Training loss: 8.04
Gradient step: 8974 	 Performance: 0.87 	 Training loss: 8.00
Gradient step: 8999 	 Performance: 0.81 	 Training loss: 8.08
Gradient step: 9024 	 Performance: 0.755 	 Training loss: 7.96
Gradient step: 9049 	 Performance: 0.805 	 Training loss: 7.91
Gradient step: 9074 	 Performance: 0.855 	 Training loss: 7.84
Gradient step: 9099 	 Performance: 0.765 	 Training loss: 7.89
Gradient step: 9124 	 Performance: 0.76 	 Training loss: 7.91
Gradient step: 9149 	 Performance: 0.805 	 Training loss: 7.81
Gradient step: 9174 	 Performance: 0.795 	 Training loss: 7.80
Gradient step: 9199 	 Performance: 0.78 	 Training loss: 7.83
Gradient step: 9224 	 Performance: 0.8 	 Training loss: 7.89
Gradient step: 9249 	 Performance: 0.78 	 Training loss: 7.84
Gradient step: 9274 	 Performance: 0.8 	 Training loss: 7.87
Gradient step: 9299 	 Performance: 0.835 	 Training loss: 8.00
Gradient step: 9324 	 Performance: 0.75 	 Training loss: 7.88
Gradient step: 9349 	 Performance: 0.8 	 Training loss: 7.66
Gradient step: 9374 	 Performance: 0.795 	 Training loss: 7.90
Gradient step: 9399 	 Performance: 0.83 	 Training loss: 7.92
Gradient step: 9424 	 Performance: 0.78 	 Training loss: 7.93
Gradient step: 9449 	 Performance: 0.82 	 Training loss: 7.96
Gradient step: 9474 	 Performance: 0.81 	 Training loss: 7.91
Gradient step: 9499 	 Performance: 0.83 	 Training loss: 7.64
Gradient step: 9524 	 Performance: 0.845 	 Training loss: 7.78
Gradient step: 9549 	 Performance: 0.825 	 Training loss: 7.93
Gradient step: 9574 	 Performance: 0.835 	 Training loss: 7.88
Gradient step: 9599 	 Performance: 0.82 	 Training loss: 8.02
Gradient step: 9624 	 Performance: 0.77 	 Training loss: 7.91
Gradient step: 9649 	 Performance: 0.785 	 Training loss: 7.85
Gradient step: 9674 	 Performance: 0.77 	 Training loss: 7.89
Gradient step: 9699 	 Performance: 0.82 	 Training loss: 7.98
Gradient step: 9724 	 Performance: 0.78 	 Training loss: 7.99
Gradient step: 9749 	 Performance: 0.785 	 Training loss: 7.95
Gradient step: 9774 	 Performance: 0.775 	 Training loss: 7.94
Gradient step: 9799 	 Performance: 0.8 	 Training loss: 7.97
Gradient step: 9824 	 Performance: 0.8 	 Training loss: 7.64
Gradient step: 9849 	 Performance: 0.81 	 Training loss: 7.90
Gradient step: 9874 	 Performance: 0.82 	 Training loss: 7.85
Gradient step: 9899 	 Performance: 0.765 	 Training loss: 8.01
Gradient step: 9924 	 Performance: 0.765 	 Training loss: 7.88
Gradient step: 9949 	 Performance: 0.825 	 Training loss: 7.90
Gradient step: 9974 	 Performance: 0.815 	 Training loss: 8.01
Gradient step: 9999 	 Performance: 0.77 	 Training loss: 7.87
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/multidmseqr/conformal' created successfully!
Gradient step: 0 	 Performance: 0.09 	 Training loss: 3.17
Gradient step: 24 	 Performance: 0.085 	 Training loss: 62.63
Gradient step: 49 	 Performance: 0.075 	 Training loss: 36.58
Gradient step: 74 	 Performance: 0.055 	 Training loss: 31.38
Gradient step: 99 	 Performance: 0.19 	 Training loss: 25.97
Gradient step: 124 	 Performance: 0.255 	 Training loss: 23.39
Gradient step: 149 	 Performance: 0.235 	 Training loss: 21.81
Gradient step: 174 	 Performance: 0.27 	 Training loss: 20.52
Gradient step: 199 	 Performance: 0.395 	 Training loss: 19.55
Gradient step: 224 	 Performance: 0.45 	 Training loss: 18.09
Gradient step: 249 	 Performance: 0.39 	 Training loss: 17.05
Gradient step: 274 	 Performance: 0.535 	 Training loss: 15.91
Gradient step: 299 	 Performance: 0.495 	 Training loss: 15.14
Gradient step: 324 	 Performance: 0.545 	 Training loss: 14.30
Gradient step: 349 	 Performance: 0.56 	 Training loss: 13.21
Gradient step: 374 	 Performance: 0.68 	 Training loss: 12.86
Gradient step: 399 	 Performance: 0.665 	 Training loss: 12.25
Gradient step: 424 	 Performance: 0.66 	 Training loss: 11.93
Gradient step: 449 	 Performance: 0.685 	 Training loss: 11.65
Gradient step: 474 	 Performance: 0.69 	 Training loss: 10.90
Gradient step: 499 	 Performance: 0.72 	 Training loss: 10.78
Gradient step: 524 	 Performance: 0.71 	 Training loss: 10.88
Gradient step: 549 	 Performance: 0.7 	 Training loss: 10.45
Gradient step: 574 	 Performance: 0.745 	 Training loss: 10.27
Gradient step: 599 	 Performance: 0.8 	 Training loss: 9.55
Gradient step: 624 	 Performance: 0.71 	 Training loss: 9.72
Gradient step: 649 	 Performance: 0.65 	 Training loss: 9.49
Gradient step: 674 	 Performance: 0.755 	 Training loss: 9.42
Gradient step: 699 	 Performance: 0.815 	 Training loss: 9.20
Gradient step: 724 	 Performance: 0.69 	 Training loss: 9.31
Gradient step: 749 	 Performance: 0.765 	 Training loss: 9.28
Gradient step: 774 	 Performance: 0.73 	 Training loss: 9.23
Gradient step: 799 	 Performance: 0.78 	 Training loss: 9.13
Gradient step: 824 	 Performance: 0.75 	 Training loss: 8.89
Gradient step: 849 	 Performance: 0.75 	 Training loss: 9.66
Gradient step: 874 	 Performance: 0.795 	 Training loss: 8.60
Gradient step: 899 	 Performance: 0.83 	 Training loss: 8.42
Gradient step: 924 	 Performance: 0.815 	 Training loss: 8.71
Gradient step: 949 	 Performance: 0.735 	 Training loss: 8.41
Gradient step: 974 	 Performance: 0.83 	 Training loss: 8.47
Gradient step: 999 	 Performance: 0.8 	 Training loss: 8.52
Gradient step: 1024 	 Performance: 0.82 	 Training loss: 8.48
Gradient step: 1049 	 Performance: 0.765 	 Training loss: 8.35
Gradient step: 1074 	 Performance: 0.815 	 Training loss: 8.13
Gradient step: 1099 	 Performance: 0.79 	 Training loss: 8.50
Gradient step: 1124 	 Performance: 0.76 	 Training loss: 7.79
Gradient step: 1149 	 Performance: 0.77 	 Training loss: 8.39
Gradient step: 1174 	 Performance: 0.805 	 Training loss: 8.24
Gradient step: 1199 	 Performance: 0.85 	 Training loss: 7.48
Gradient step: 1224 	 Performance: 0.81 	 Training loss: 7.19
Gradient step: 1249 	 Performance: 0.8 	 Training loss: 7.04
Gradient step: 1274 	 Performance: 0.845 	 Training loss: 7.09
Gradient step: 1299 	 Performance: 0.79 	 Training loss: 6.78
Gradient step: 1324 	 Performance: 0.835 	 Training loss: 6.93
Gradient step: 1349 	 Performance: 0.775 	 Training loss: 7.09
Gradient step: 1374 	 Performance: 0.75 	 Training loss: 6.95
Gradient step: 1399 	 Performance: 0.835 	 Training loss: 7.19
Gradient step: 1424 	 Performance: 0.8 	 Training loss: 6.91
Gradient step: 1449 	 Performance: 0.805 	 Training loss: 7.03
Gradient step: 1474 	 Performance: 0.795 	 Training loss: 6.90
Gradient step: 1499 	 Performance: 0.815 	 Training loss: 6.91
Gradient step: 1524 	 Performance: 0.83 	 Training loss: 6.79
Gradient step: 1549 	 Performance: 0.84 	 Training loss: 6.68
Gradient step: 1574 	 Performance: 0.775 	 Training loss: 6.64
Gradient step: 1599 	 Performance: 0.865 	 Training loss: 6.80
Gradient step: 1624 	 Performance: 0.815 	 Training loss: 6.90
Gradient step: 1649 	 Performance: 0.84 	 Training loss: 6.87
Gradient step: 1674 	 Performance: 0.84 	 Training loss: 6.66
Gradient step: 1699 	 Performance: 0.835 	 Training loss: 6.72
Gradient step: 1724 	 Performance: 0.85 	 Training loss: 6.68
Gradient step: 1749 	 Performance: 0.805 	 Training loss: 6.75
Gradient step: 1774 	 Performance: 0.84 	 Training loss: 6.80
Gradient step: 1799 	 Performance: 0.85 	 Training loss: 6.82
Gradient step: 1824 	 Performance: 0.85 	 Training loss: 6.76
Gradient step: 1849 	 Performance: 0.77 	 Training loss: 6.85
Gradient step: 1874 	 Performance: 0.805 	 Training loss: 6.68
Gradient step: 1899 	 Performance: 0.825 	 Training loss: 6.76
Gradient step: 1924 	 Performance: 0.74 	 Training loss: 6.58
Gradient step: 1949 	 Performance: 0.815 	 Training loss: 6.75
Gradient step: 1974 	 Performance: 0.84 	 Training loss: 6.73
Gradient step: 1999 	 Performance: 0.855 	 Training loss: 6.76
Gradient step: 2024 	 Performance: 0.85 	 Training loss: 6.99
Gradient step: 2049 	 Performance: 0.795 	 Training loss: 6.55
Gradient step: 2074 	 Performance: 0.85 	 Training loss: 6.95
Gradient step: 2099 	 Performance: 0.87 	 Training loss: 6.71
Gradient step: 2124 	 Performance: 0.78 	 Training loss: 6.81
Gradient step: 2149 	 Performance: 0.83 	 Training loss: 6.80
Gradient step: 2174 	 Performance: 0.805 	 Training loss: 6.94
Gradient step: 2199 	 Performance: 0.855 	 Training loss: 6.71
Gradient step: 2224 	 Performance: 0.83 	 Training loss: 6.70
Gradient step: 2249 	 Performance: 0.83 	 Training loss: 6.81
Gradient step: 2274 	 Performance: 0.86 	 Training loss: 6.69
Gradient step: 2299 	 Performance: 0.87 	 Training loss: 6.90
Gradient step: 2324 	 Performance: 0.845 	 Training loss: 6.83
Gradient step: 2349 	 Performance: 0.855 	 Training loss: 6.73
Gradient step: 2374 	 Performance: 0.815 	 Training loss: 6.82
Gradient step: 2399 	 Performance: 0.805 	 Training loss: 6.90
Gradient step: 2424 	 Performance: 0.835 	 Training loss: 6.90
Gradient step: 2449 	 Performance: 0.805 	 Training loss: 6.66
Gradient step: 2474 	 Performance: 0.845 	 Training loss: 6.70
Gradient step: 2499 	 Performance: 0.84 	 Training loss: 6.89
Gradient step: 2524 	 Performance: 0.81 	 Training loss: 6.61
Gradient step: 2549 	 Performance: 0.835 	 Training loss: 6.73
Gradient step: 2574 	 Performance: 0.79 	 Training loss: 6.71
Gradient step: 2599 	 Performance: 0.83 	 Training loss: 6.81
Gradient step: 2624 	 Performance: 0.79 	 Training loss: 6.60
Gradient step: 2649 	 Performance: 0.835 	 Training loss: 6.80
Gradient step: 2674 	 Performance: 0.785 	 Training loss: 6.83
Gradient step: 2699 	 Performance: 0.86 	 Training loss: 6.58
Gradient step: 2724 	 Performance: 0.855 	 Training loss: 6.64
Gradient step: 2749 	 Performance: 0.835 	 Training loss: 6.69
Gradient step: 2774 	 Performance: 0.875 	 Training loss: 6.59
Gradient step: 2799 	 Performance: 0.845 	 Training loss: 6.79
Gradient step: 2824 	 Performance: 0.835 	 Training loss: 6.79
Gradient step: 2849 	 Performance: 0.855 	 Training loss: 6.60
Gradient step: 2874 	 Performance: 0.85 	 Training loss: 6.62
Gradient step: 2899 	 Performance: 0.79 	 Training loss: 6.71
Gradient step: 2924 	 Performance: 0.85 	 Training loss: 6.65
Gradient step: 2949 	 Performance: 0.83 	 Training loss: 6.60
Gradient step: 2974 	 Performance: 0.82 	 Training loss: 6.65
Gradient step: 2999 	 Performance: 0.81 	 Training loss: 6.82
Gradient step: 3024 	 Performance: 0.795 	 Training loss: 6.81
Gradient step: 3049 	 Performance: 0.835 	 Training loss: 6.86
Gradient step: 3074 	 Performance: 0.82 	 Training loss: 6.53
Gradient step: 3099 	 Performance: 0.85 	 Training loss: 6.83
Gradient step: 3124 	 Performance: 0.835 	 Training loss: 6.78
Gradient step: 3149 	 Performance: 0.815 	 Training loss: 6.77
Gradient step: 3174 	 Performance: 0.84 	 Training loss: 6.73
Gradient step: 3199 	 Performance: 0.845 	 Training loss: 6.65
Gradient step: 3224 	 Performance: 0.865 	 Training loss: 6.76
Gradient step: 3249 	 Performance: 0.825 	 Training loss: 6.94
Gradient step: 3274 	 Performance: 0.76 	 Training loss: 6.68
Gradient step: 3299 	 Performance: 0.83 	 Training loss: 6.59
Gradient step: 3324 	 Performance: 0.815 	 Training loss: 6.78
Gradient step: 3349 	 Performance: 0.825 	 Training loss: 6.67
Gradient step: 3374 	 Performance: 0.825 	 Training loss: 6.62
Gradient step: 3399 	 Performance: 0.82 	 Training loss: 6.76
Gradient step: 3424 	 Performance: 0.83 	 Training loss: 6.81
Gradient step: 3449 	 Performance: 0.865 	 Training loss: 6.68
Gradient step: 3474 	 Performance: 0.815 	 Training loss: 6.62
Gradient step: 3499 	 Performance: 0.875 	 Training loss: 6.87
Gradient step: 3524 	 Performance: 0.795 	 Training loss: 6.75
Gradient step: 3549 	 Performance: 0.865 	 Training loss: 6.54
Gradient step: 3574 	 Performance: 0.825 	 Training loss: 6.73
Gradient step: 3599 	 Performance: 0.83 	 Training loss: 6.73
Gradient step: 3624 	 Performance: 0.86 	 Training loss: 6.91
Gradient step: 3649 	 Performance: 0.83 	 Training loss: 6.84
Gradient step: 3674 	 Performance: 0.86 	 Training loss: 6.65
Gradient step: 3699 	 Performance: 0.81 	 Training loss: 6.72
Gradient step: 3724 	 Performance: 0.865 	 Training loss: 6.71
Gradient step: 3749 	 Performance: 0.885 	 Training loss: 6.76
Gradient step: 3774 	 Performance: 0.835 	 Training loss: 6.76
Gradient step: 3799 	 Performance: 0.845 	 Training loss: 6.78
Gradient step: 3824 	 Performance: 0.84 	 Training loss: 6.92
Gradient step: 3849 	 Performance: 0.795 	 Training loss: 6.84
Gradient step: 3874 	 Performance: 0.835 	 Training loss: 6.77
Gradient step: 3899 	 Performance: 0.86 	 Training loss: 6.69
Gradient step: 3924 	 Performance: 0.86 	 Training loss: 7.02
Gradient step: 3949 	 Performance: 0.775 	 Training loss: 6.61
Gradient step: 3974 	 Performance: 0.865 	 Training loss: 6.52
Gradient step: 3999 	 Performance: 0.855 	 Training loss: 6.81
Gradient step: 4024 	 Performance: 0.81 	 Training loss: 6.68
Gradient step: 4049 	 Performance: 0.85 	 Training loss: 6.67
Gradient step: 4074 	 Performance: 0.83 	 Training loss: 6.73
Gradient step: 4099 	 Performance: 0.855 	 Training loss: 6.64
Gradient step: 4124 	 Performance: 0.83 	 Training loss: 6.89
Gradient step: 4149 	 Performance: 0.875 	 Training loss: 6.75
Gradient step: 4174 	 Performance: 0.795 	 Training loss: 6.72
Gradient step: 4199 	 Performance: 0.81 	 Training loss: 6.65
Gradient step: 4224 	 Performance: 0.85 	 Training loss: 6.80
Gradient step: 4249 	 Performance: 0.795 	 Training loss: 6.85
Gradient step: 4274 	 Performance: 0.835 	 Training loss: 7.00
Gradient step: 4299 	 Performance: 0.875 	 Training loss: 6.69
Gradient step: 4324 	 Performance: 0.765 	 Training loss: 6.74
Gradient step: 4349 	 Performance: 0.805 	 Training loss: 6.77
Gradient step: 4374 	 Performance: 0.76 	 Training loss: 6.81
Gradient step: 4399 	 Performance: 0.855 	 Training loss: 6.74
Gradient step: 4424 	 Performance: 0.785 	 Training loss: 6.59
Gradient step: 4449 	 Performance: 0.825 	 Training loss: 6.77
Gradient step: 4474 	 Performance: 0.84 	 Training loss: 6.76
Gradient step: 4499 	 Performance: 0.815 	 Training loss: 6.53
Gradient step: 4524 	 Performance: 0.82 	 Training loss: 6.74
Gradient step: 4549 	 Performance: 0.83 	 Training loss: 6.75
Gradient step: 4574 	 Performance: 0.745 	 Training loss: 6.64
Gradient step: 4599 	 Performance: 0.835 	 Training loss: 6.83
Gradient step: 4624 	 Performance: 0.83 	 Training loss: 6.78
Gradient step: 4649 	 Performance: 0.8 	 Training loss: 6.91
Gradient step: 4674 	 Performance: 0.815 	 Training loss: 6.67
Gradient step: 4699 	 Performance: 0.83 	 Training loss: 6.71
Gradient step: 4724 	 Performance: 0.89 	 Training loss: 6.68
Gradient step: 4749 	 Performance: 0.85 	 Training loss: 6.92
Gradient step: 4774 	 Performance: 0.815 	 Training loss: 6.73
Gradient step: 4799 	 Performance: 0.85 	 Training loss: 6.69
Gradient step: 4824 	 Performance: 0.785 	 Training loss: 6.58
Gradient step: 4849 	 Performance: 0.83 	 Training loss: 6.74
Gradient step: 4874 	 Performance: 0.835 	 Training loss: 6.90
Gradient step: 4899 	 Performance: 0.86 	 Training loss: 6.91
Gradient step: 4924 	 Performance: 0.795 	 Training loss: 6.88
Gradient step: 4949 	 Performance: 0.79 	 Training loss: 6.75
Gradient step: 4974 	 Performance: 0.815 	 Training loss: 6.40
Gradient step: 4999 	 Performance: 0.81 	 Training loss: 6.97
Gradient step: 5024 	 Performance: 0.855 	 Training loss: 6.86
Gradient step: 5049 	 Performance: 0.84 	 Training loss: 6.63
Gradient step: 5074 	 Performance: 0.81 	 Training loss: 6.69
Gradient step: 5099 	 Performance: 0.815 	 Training loss: 6.79
Gradient step: 5124 	 Performance: 0.85 	 Training loss: 6.55
Gradient step: 5149 	 Performance: 0.815 	 Training loss: 6.59
Gradient step: 5174 	 Performance: 0.825 	 Training loss: 6.70
Gradient step: 5199 	 Performance: 0.805 	 Training loss: 6.87
Gradient step: 5224 	 Performance: 0.81 	 Training loss: 6.78
Gradient step: 5249 	 Performance: 0.79 	 Training loss: 6.75
Gradient step: 5274 	 Performance: 0.805 	 Training loss: 6.55
Gradient step: 5299 	 Performance: 0.8 	 Training loss: 6.62
Gradient step: 5324 	 Performance: 0.825 	 Training loss: 6.74
Gradient step: 5349 	 Performance: 0.855 	 Training loss: 6.83
Gradient step: 5374 	 Performance: 0.885 	 Training loss: 6.76
Gradient step: 5399 	 Performance: 0.855 	 Training loss: 6.73
Gradient step: 5424 	 Performance: 0.815 	 Training loss: 6.73
Gradient step: 5449 	 Performance: 0.875 	 Training loss: 6.69
Gradient step: 5474 	 Performance: 0.805 	 Training loss: 6.78
Gradient step: 5499 	 Performance: 0.785 	 Training loss: 6.72
Gradient step: 5524 	 Performance: 0.815 	 Training loss: 6.66
Gradient step: 5549 	 Performance: 0.84 	 Training loss: 6.63
Gradient step: 5574 	 Performance: 0.84 	 Training loss: 6.83
Gradient step: 5599 	 Performance: 0.765 	 Training loss: 6.79
Gradient step: 5624 	 Performance: 0.865 	 Training loss: 6.61
Gradient step: 5649 	 Performance: 0.815 	 Training loss: 6.66
Gradient step: 5674 	 Performance: 0.83 	 Training loss: 6.74
Gradient step: 5699 	 Performance: 0.79 	 Training loss: 6.74
Gradient step: 5724 	 Performance: 0.85 	 Training loss: 6.69
Gradient step: 5749 	 Performance: 0.835 	 Training loss: 6.65
Gradient step: 5774 	 Performance: 0.85 	 Training loss: 6.76
Gradient step: 5799 	 Performance: 0.805 	 Training loss: 6.74
Gradient step: 5824 	 Performance: 0.85 	 Training loss: 6.75
Gradient step: 5849 	 Performance: 0.88 	 Training loss: 7.04
Gradient step: 5874 	 Performance: 0.875 	 Training loss: 6.57
Gradient step: 5899 	 Performance: 0.79 	 Training loss: 6.81
Gradient step: 5924 	 Performance: 0.825 	 Training loss: 6.62
Gradient step: 5949 	 Performance: 0.865 	 Training loss: 6.73
Gradient step: 5974 	 Performance: 0.805 	 Training loss: 6.54
Gradient step: 5999 	 Performance: 0.765 	 Training loss: 6.71
Gradient step: 6024 	 Performance: 0.86 	 Training loss: 6.60
Gradient step: 6049 	 Performance: 0.855 	 Training loss: 6.77
Gradient step: 6074 	 Performance: 0.825 	 Training loss: 6.78
Gradient step: 6099 	 Performance: 0.825 	 Training loss: 6.55
Gradient step: 6124 	 Performance: 0.78 	 Training loss: 6.78
Gradient step: 6149 	 Performance: 0.81 	 Training loss: 6.73
Gradient step: 6174 	 Performance: 0.85 	 Training loss: 6.64
Gradient step: 6199 	 Performance: 0.8 	 Training loss: 6.69
Gradient step: 6224 	 Performance: 0.81 	 Training loss: 6.88
Gradient step: 6249 	 Performance: 0.78 	 Training loss: 6.71
Gradient step: 6274 	 Performance: 0.845 	 Training loss: 6.87
Gradient step: 6299 	 Performance: 0.835 	 Training loss: 6.71
Gradient step: 6324 	 Performance: 0.815 	 Training loss: 6.58
Gradient step: 6349 	 Performance: 0.82 	 Training loss: 6.84
Gradient step: 6374 	 Performance: 0.805 	 Training loss: 7.07
Gradient step: 6399 	 Performance: 0.845 	 Training loss: 6.79
Gradient step: 6424 	 Performance: 0.84 	 Training loss: 6.75
Gradient step: 6449 	 Performance: 0.82 	 Training loss: 6.64
Gradient step: 6474 	 Performance: 0.85 	 Training loss: 6.65
Gradient step: 6499 	 Performance: 0.82 	 Training loss: 6.60
Gradient step: 6524 	 Performance: 0.81 	 Training loss: 6.88
Gradient step: 6549 	 Performance: 0.825 	 Training loss: 6.72
Gradient step: 6574 	 Performance: 0.795 	 Training loss: 6.78
Gradient step: 6599 	 Performance: 0.855 	 Training loss: 6.68
Gradient step: 6624 	 Performance: 0.84 	 Training loss: 6.79
Gradient step: 6649 	 Performance: 0.815 	 Training loss: 6.55
Gradient step: 6674 	 Performance: 0.785 	 Training loss: 6.65
Gradient step: 6699 	 Performance: 0.775 	 Training loss: 6.77
Gradient step: 6724 	 Performance: 0.81 	 Training loss: 6.73
Gradient step: 6749 	 Performance: 0.825 	 Training loss: 6.55
Gradient step: 6774 	 Performance: 0.84 	 Training loss: 6.71
Gradient step: 6799 	 Performance: 0.86 	 Training loss: 6.88
Gradient step: 6824 	 Performance: 0.845 	 Training loss: 6.76
Gradient step: 6849 	 Performance: 0.775 	 Training loss: 6.74
Gradient step: 6874 	 Performance: 0.84 	 Training loss: 6.73
Gradient step: 6899 	 Performance: 0.85 	 Training loss: 6.73
Gradient step: 6924 	 Performance: 0.795 	 Training loss: 6.67
Gradient step: 6949 	 Performance: 0.83 	 Training loss: 6.64
Gradient step: 6974 	 Performance: 0.85 	 Training loss: 6.55
Gradient step: 6999 	 Performance: 0.82 	 Training loss: 6.76
Gradient step: 7024 	 Performance: 0.82 	 Training loss: 7.07
Gradient step: 7049 	 Performance: 0.725 	 Training loss: 6.65
Gradient step: 7074 	 Performance: 0.82 	 Training loss: 6.65
Gradient step: 7099 	 Performance: 0.82 	 Training loss: 6.81
Gradient step: 7124 	 Performance: 0.805 	 Training loss: 6.81
Gradient step: 7149 	 Performance: 0.86 	 Training loss: 6.68
Gradient step: 7174 	 Performance: 0.89 	 Training loss: 6.96
Gradient step: 7199 	 Performance: 0.8 	 Training loss: 6.70
Gradient step: 7224 	 Performance: 0.84 	 Training loss: 7.02
Gradient step: 7249 	 Performance: 0.855 	 Training loss: 6.51
Gradient step: 7274 	 Performance: 0.875 	 Training loss: 6.67
Gradient step: 7299 	 Performance: 0.82 	 Training loss: 6.83
Gradient step: 7324 	 Performance: 0.805 	 Training loss: 6.92
Gradient step: 7349 	 Performance: 0.81 	 Training loss: 6.57
Gradient step: 7374 	 Performance: 0.77 	 Training loss: 6.53
Gradient step: 7399 	 Performance: 0.83 	 Training loss: 6.46
Gradient step: 7424 	 Performance: 0.845 	 Training loss: 6.81
Gradient step: 7449 	 Performance: 0.85 	 Training loss: 6.75
Gradient step: 7474 	 Performance: 0.86 	 Training loss: 6.59
Gradient step: 7499 	 Performance: 0.83 	 Training loss: 6.68
Gradient step: 7524 	 Performance: 0.76 	 Training loss: 6.79
Gradient step: 7549 	 Performance: 0.845 	 Training loss: 6.80
Gradient step: 7574 	 Performance: 0.855 	 Training loss: 6.85
Gradient step: 7599 	 Performance: 0.865 	 Training loss: 6.61
Gradient step: 7624 	 Performance: 0.845 	 Training loss: 6.80
Gradient step: 7649 	 Performance: 0.84 	 Training loss: 6.69
Gradient step: 7674 	 Performance: 0.84 	 Training loss: 6.71
Gradient step: 7699 	 Performance: 0.815 	 Training loss: 6.65
Gradient step: 7724 	 Performance: 0.825 	 Training loss: 6.71
Gradient step: 7749 	 Performance: 0.875 	 Training loss: 6.65
Gradient step: 7774 	 Performance: 0.865 	 Training loss: 6.67
Gradient step: 7799 	 Performance: 0.78 	 Training loss: 6.83
Gradient step: 7824 	 Performance: 0.84 	 Training loss: 6.67
Gradient step: 7849 	 Performance: 0.855 	 Training loss: 6.75
Gradient step: 7874 	 Performance: 0.845 	 Training loss: 6.82
Gradient step: 7899 	 Performance: 0.865 	 Training loss: 6.70
Gradient step: 7924 	 Performance: 0.765 	 Training loss: 6.70
Gradient step: 7949 	 Performance: 0.83 	 Training loss: 6.77
Gradient step: 7974 	 Performance: 0.805 	 Training loss: 6.53
Gradient step: 7999 	 Performance: 0.825 	 Training loss: 6.84
Gradient step: 8024 	 Performance: 0.86 	 Training loss: 6.92
Gradient step: 8049 	 Performance: 0.805 	 Training loss: 6.78
Gradient step: 8074 	 Performance: 0.84 	 Training loss: 6.87
Gradient step: 8099 	 Performance: 0.82 	 Training loss: 6.59
Gradient step: 8124 	 Performance: 0.85 	 Training loss: 6.49
Gradient step: 8149 	 Performance: 0.855 	 Training loss: 6.66
Gradient step: 8174 	 Performance: 0.795 	 Training loss: 6.75
Gradient step: 8199 	 Performance: 0.78 	 Training loss: 6.84
Gradient step: 8224 	 Performance: 0.87 	 Training loss: 6.54
Gradient step: 8249 	 Performance: 0.81 	 Training loss: 6.67
Gradient step: 8274 	 Performance: 0.84 	 Training loss: 6.71
Gradient step: 8299 	 Performance: 0.815 	 Training loss: 6.61
Gradient step: 8324 	 Performance: 0.84 	 Training loss: 6.81
Gradient step: 8349 	 Performance: 0.88 	 Training loss: 6.65
Gradient step: 8374 	 Performance: 0.85 	 Training loss: 6.83
Gradient step: 8399 	 Performance: 0.815 	 Training loss: 6.76
Gradient step: 8424 	 Performance: 0.805 	 Training loss: 6.70
Gradient step: 8449 	 Performance: 0.805 	 Training loss: 6.70
Gradient step: 8474 	 Performance: 0.825 	 Training loss: 6.59
Gradient step: 8499 	 Performance: 0.86 	 Training loss: 6.72
Gradient step: 8524 	 Performance: 0.835 	 Training loss: 6.81
Gradient step: 8549 	 Performance: 0.855 	 Training loss: 6.70
Gradient step: 8574 	 Performance: 0.875 	 Training loss: 6.64
Gradient step: 8599 	 Performance: 0.775 	 Training loss: 6.81
Gradient step: 8624 	 Performance: 0.845 	 Training loss: 6.72
Gradient step: 8649 	 Performance: 0.805 	 Training loss: 6.83
Gradient step: 8674 	 Performance: 0.82 	 Training loss: 6.63
Gradient step: 8699 	 Performance: 0.81 	 Training loss: 6.65
Gradient step: 8724 	 Performance: 0.785 	 Training loss: 6.86
Gradient step: 8749 	 Performance: 0.82 	 Training loss: 6.58
Gradient step: 8774 	 Performance: 0.845 	 Training loss: 6.78
Gradient step: 8799 	 Performance: 0.83 	 Training loss: 6.84
Gradient step: 8824 	 Performance: 0.805 	 Training loss: 6.72
Gradient step: 8849 	 Performance: 0.84 	 Training loss: 6.93
Gradient step: 8874 	 Performance: 0.885 	 Training loss: 6.70
Gradient step: 8899 	 Performance: 0.855 	 Training loss: 6.92
Gradient step: 8924 	 Performance: 0.84 	 Training loss: 6.54
Gradient step: 8949 	 Performance: 0.825 	 Training loss: 6.63
Gradient step: 8974 	 Performance: 0.84 	 Training loss: 6.77
Gradient step: 8999 	 Performance: 0.875 	 Training loss: 6.72
Gradient step: 9024 	 Performance: 0.84 	 Training loss: 6.69
Gradient step: 9049 	 Performance: 0.83 	 Training loss: 6.60
Gradient step: 9074 	 Performance: 0.8 	 Training loss: 6.73
Gradient step: 9099 	 Performance: 0.855 	 Training loss: 6.81
Gradient step: 9124 	 Performance: 0.82 	 Training loss: 6.63
Gradient step: 9149 	 Performance: 0.84 	 Training loss: 6.82
Gradient step: 9174 	 Performance: 0.84 	 Training loss: 6.66
Gradient step: 9199 	 Performance: 0.805 	 Training loss: 6.88
Gradient step: 9224 	 Performance: 0.865 	 Training loss: 6.61
Gradient step: 9249 	 Performance: 0.825 	 Training loss: 6.80
Gradient step: 9274 	 Performance: 0.85 	 Training loss: 6.67
Gradient step: 9299 	 Performance: 0.785 	 Training loss: 6.73
Gradient step: 9324 	 Performance: 0.79 	 Training loss: 6.63
Gradient step: 9349 	 Performance: 0.815 	 Training loss: 6.70
Gradient step: 9374 	 Performance: 0.81 	 Training loss: 6.67
Gradient step: 9399 	 Performance: 0.805 	 Training loss: 6.76
Gradient step: 9424 	 Performance: 0.83 	 Training loss: 6.71
Gradient step: 9449 	 Performance: 0.83 	 Training loss: 7.06
Gradient step: 9474 	 Performance: 0.825 	 Training loss: 6.91
Gradient step: 9499 	 Performance: 0.845 	 Training loss: 6.61
Gradient step: 9524 	 Performance: 0.77 	 Training loss: 6.71
Gradient step: 9549 	 Performance: 0.82 	 Training loss: 6.84
Gradient step: 9574 	 Performance: 0.835 	 Training loss: 6.60
Gradient step: 9599 	 Performance: 0.855 	 Training loss: 6.86
Gradient step: 9624 	 Performance: 0.845 	 Training loss: 6.88
Gradient step: 9649 	 Performance: 0.815 	 Training loss: 6.69
Gradient step: 9674 	 Performance: 0.87 	 Training loss: 6.67
Gradient step: 9699 	 Performance: 0.855 	 Training loss: 6.81
Gradient step: 9724 	 Performance: 0.85 	 Training loss: 6.82
Gradient step: 9749 	 Performance: 0.835 	 Training loss: 6.71
Gradient step: 9774 	 Performance: 0.805 	 Training loss: 6.58
Gradient step: 9799 	 Performance: 0.785 	 Training loss: 6.78
Gradient step: 9824 	 Performance: 0.84 	 Training loss: 6.69
Gradient step: 9849 	 Performance: 0.845 	 Training loss: 6.89
Gradient step: 9874 	 Performance: 0.835 	 Training loss: 6.68
Gradient step: 9899 	 Performance: 0.8 	 Training loss: 6.64
Gradient step: 9924 	 Performance: 0.78 	 Training loss: 6.75
Gradient step: 9949 	 Performance: 0.83 	 Training loss: 6.82
Gradient step: 9974 	 Performance: 0.81 	 Training loss: 6.74
Gradient step: 9999 	 Performance: 0.88 	 Training loss: 6.82
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/multidmseqr/None' created successfully!
