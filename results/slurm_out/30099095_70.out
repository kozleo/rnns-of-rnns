Gradient step: 0 	 Performance: 0.1 	 Training loss: 3.00
Gradient step: 24 	 Performance: 0.09 	 Training loss: 56.99
Gradient step: 49 	 Performance: 0.07 	 Training loss: 36.75
Gradient step: 74 	 Performance: 0.105 	 Training loss: 30.93
Gradient step: 99 	 Performance: 0.155 	 Training loss: 25.90
Gradient step: 124 	 Performance: 0.185 	 Training loss: 23.05
Gradient step: 149 	 Performance: 0.285 	 Training loss: 21.24
Gradient step: 174 	 Performance: 0.31 	 Training loss: 19.60
Gradient step: 199 	 Performance: 0.42 	 Training loss: 18.69
Gradient step: 224 	 Performance: 0.47 	 Training loss: 17.63
Gradient step: 249 	 Performance: 0.5 	 Training loss: 16.82
Gradient step: 274 	 Performance: 0.485 	 Training loss: 16.14
Gradient step: 299 	 Performance: 0.545 	 Training loss: 15.30
Gradient step: 324 	 Performance: 0.53 	 Training loss: 14.68
Gradient step: 349 	 Performance: 0.525 	 Training loss: 14.09
Gradient step: 374 	 Performance: 0.54 	 Training loss: 13.56
Gradient step: 399 	 Performance: 0.635 	 Training loss: 13.18
Gradient step: 424 	 Performance: 0.6 	 Training loss: 12.57
Gradient step: 449 	 Performance: 0.705 	 Training loss: 12.00
Gradient step: 474 	 Performance: 0.67 	 Training loss: 12.07
Gradient step: 499 	 Performance: 0.645 	 Training loss: 11.25
Gradient step: 524 	 Performance: 0.71 	 Training loss: 10.89
Gradient step: 549 	 Performance: 0.73 	 Training loss: 10.19
Gradient step: 574 	 Performance: 0.83 	 Training loss: 10.24
Gradient step: 599 	 Performance: 0.715 	 Training loss: 9.49
Gradient step: 624 	 Performance: 0.645 	 Training loss: 9.33
Gradient step: 649 	 Performance: 0.645 	 Training loss: 8.99
Gradient step: 674 	 Performance: 0.76 	 Training loss: 8.51
Gradient step: 699 	 Performance: 0.76 	 Training loss: 8.53
Gradient step: 724 	 Performance: 0.7 	 Training loss: 8.52
Gradient step: 749 	 Performance: 0.785 	 Training loss: 8.28
Gradient step: 774 	 Performance: 0.775 	 Training loss: 7.96
Gradient step: 799 	 Performance: 0.745 	 Training loss: 8.53
Gradient step: 824 	 Performance: 0.835 	 Training loss: 7.71
Gradient step: 849 	 Performance: 0.825 	 Training loss: 7.25
Gradient step: 874 	 Performance: 0.81 	 Training loss: 7.53
Gradient step: 899 	 Performance: 0.825 	 Training loss: 7.08
Gradient step: 924 	 Performance: 0.85 	 Training loss: 7.55
Gradient step: 949 	 Performance: 0.845 	 Training loss: 6.87
Gradient step: 974 	 Performance: 0.78 	 Training loss: 6.86
Gradient step: 999 	 Performance: 0.825 	 Training loss: 6.72
Gradient step: 1024 	 Performance: 0.79 	 Training loss: 7.04
Gradient step: 1049 	 Performance: 0.81 	 Training loss: 6.96
Gradient step: 1074 	 Performance: 0.835 	 Training loss: 6.44
Gradient step: 1099 	 Performance: 0.865 	 Training loss: 6.46
Gradient step: 1124 	 Performance: 0.8 	 Training loss: 6.47
Gradient step: 1149 	 Performance: 0.85 	 Training loss: 6.32
Gradient step: 1174 	 Performance: 0.83 	 Training loss: 6.30
Gradient step: 1199 	 Performance: 0.865 	 Training loss: 5.92
Gradient step: 1224 	 Performance: 0.765 	 Training loss: 6.23
Gradient step: 1249 	 Performance: 0.84 	 Training loss: 6.42
Gradient step: 1274 	 Performance: 0.84 	 Training loss: 6.02
Gradient step: 1299 	 Performance: 0.925 	 Training loss: 5.99
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/None/ctxdm1seql/None' created successfully!
Gradient step: 0 	 Performance: 0.075 	 Training loss: 3.02
Gradient step: 24 	 Performance: 0.07 	 Training loss: 48.41
Gradient step: 49 	 Performance: 0.09 	 Training loss: 35.53
Gradient step: 74 	 Performance: 0.155 	 Training loss: 28.66
Gradient step: 99 	 Performance: 0.145 	 Training loss: 22.92
Gradient step: 124 	 Performance: 0.275 	 Training loss: 19.00
Gradient step: 149 	 Performance: 0.38 	 Training loss: 16.78
Gradient step: 174 	 Performance: 0.385 	 Training loss: 15.53
Gradient step: 199 	 Performance: 0.52 	 Training loss: 14.46
Gradient step: 224 	 Performance: 0.515 	 Training loss: 13.72
Gradient step: 249 	 Performance: 0.515 	 Training loss: 12.85
Gradient step: 274 	 Performance: 0.675 	 Training loss: 12.41
Gradient step: 299 	 Performance: 0.58 	 Training loss: 12.04
Gradient step: 324 	 Performance: 0.56 	 Training loss: 11.80
Gradient step: 349 	 Performance: 0.605 	 Training loss: 11.57
Gradient step: 374 	 Performance: 0.595 	 Training loss: 10.77
Gradient step: 399 	 Performance: 0.7 	 Training loss: 10.56
Gradient step: 424 	 Performance: 0.73 	 Training loss: 10.33
Gradient step: 449 	 Performance: 0.68 	 Training loss: 9.95
Gradient step: 474 	 Performance: 0.73 	 Training loss: 9.64
Gradient step: 499 	 Performance: 0.77 	 Training loss: 9.92
Gradient step: 524 	 Performance: 0.7 	 Training loss: 9.62
Gradient step: 549 	 Performance: 0.735 	 Training loss: 9.36
Gradient step: 574 	 Performance: 0.715 	 Training loss: 9.04
Gradient step: 599 	 Performance: 0.68 	 Training loss: 9.00
Gradient step: 624 	 Performance: 0.725 	 Training loss: 9.21
Gradient step: 649 	 Performance: 0.745 	 Training loss: 8.97
Gradient step: 674 	 Performance: 0.79 	 Training loss: 8.69
Gradient step: 699 	 Performance: 0.76 	 Training loss: 8.55
Gradient step: 724 	 Performance: 0.73 	 Training loss: 8.57
Gradient step: 749 	 Performance: 0.775 	 Training loss: 8.49
Gradient step: 774 	 Performance: 0.715 	 Training loss: 8.22
Gradient step: 799 	 Performance: 0.815 	 Training loss: 8.20
Gradient step: 824 	 Performance: 0.825 	 Training loss: 8.16
Gradient step: 849 	 Performance: 0.8 	 Training loss: 7.93
Gradient step: 874 	 Performance: 0.835 	 Training loss: 7.92
Gradient step: 899 	 Performance: 0.765 	 Training loss: 7.59
Gradient step: 924 	 Performance: 0.755 	 Training loss: 8.01
Gradient step: 949 	 Performance: 0.83 	 Training loss: 8.05
Gradient step: 974 	 Performance: 0.77 	 Training loss: 7.60
Gradient step: 999 	 Performance: 0.755 	 Training loss: 7.68
Gradient step: 1024 	 Performance: 0.765 	 Training loss: 7.84
Gradient step: 1049 	 Performance: 0.74 	 Training loss: 7.52
Gradient step: 1074 	 Performance: 0.815 	 Training loss: 7.48
Gradient step: 1099 	 Performance: 0.775 	 Training loss: 7.58
Gradient step: 1124 	 Performance: 0.89 	 Training loss: 7.27
Gradient step: 1149 	 Performance: 0.835 	 Training loss: 6.84
Gradient step: 1174 	 Performance: 0.82 	 Training loss: 7.45
Gradient step: 1199 	 Performance: 0.85 	 Training loss: 7.42
Gradient step: 1224 	 Performance: 0.82 	 Training loss: 7.22
Gradient step: 1249 	 Performance: 0.8 	 Training loss: 7.40
Gradient step: 1274 	 Performance: 0.76 	 Training loss: 6.89
Gradient step: 1299 	 Performance: 0.785 	 Training loss: 7.03
Gradient step: 1324 	 Performance: 0.865 	 Training loss: 7.40
Gradient step: 1349 	 Performance: 0.77 	 Training loss: 6.84
Gradient step: 1374 	 Performance: 0.84 	 Training loss: 6.83
Gradient step: 1399 	 Performance: 0.835 	 Training loss: 6.88
Gradient step: 1424 	 Performance: 0.895 	 Training loss: 6.14
Gradient step: 1449 	 Performance: 0.87 	 Training loss: 6.09
Gradient step: 1474 	 Performance: 0.88 	 Training loss: 5.67
Gradient step: 1499 	 Performance: 0.82 	 Training loss: 5.83
Gradient step: 1524 	 Performance: 0.855 	 Training loss: 5.74
Gradient step: 1549 	 Performance: 0.85 	 Training loss: 5.85
Gradient step: 1574 	 Performance: 0.875 	 Training loss: 5.70
Gradient step: 1599 	 Performance: 0.86 	 Training loss: 5.78
Gradient step: 1624 	 Performance: 0.875 	 Training loss: 5.68
Gradient step: 1649 	 Performance: 0.885 	 Training loss: 5.75
Gradient step: 1674 	 Performance: 0.87 	 Training loss: 5.45
Gradient step: 1699 	 Performance: 0.86 	 Training loss: 5.92
Gradient step: 1724 	 Performance: 0.825 	 Training loss: 5.70
Gradient step: 1749 	 Performance: 0.88 	 Training loss: 5.52
Gradient step: 1774 	 Performance: 0.84 	 Training loss: 5.46
Gradient step: 1799 	 Performance: 0.83 	 Training loss: 5.72
Gradient step: 1824 	 Performance: 0.845 	 Training loss: 5.66
Gradient step: 1849 	 Performance: 0.87 	 Training loss: 5.49
Gradient step: 1874 	 Performance: 0.885 	 Training loss: 5.69
Gradient step: 1899 	 Performance: 0.89 	 Training loss: 5.53
Gradient step: 1924 	 Performance: 0.86 	 Training loss: 5.65
Gradient step: 1949 	 Performance: 0.875 	 Training loss: 5.51
Gradient step: 1974 	 Performance: 0.865 	 Training loss: 5.58
Gradient step: 1999 	 Performance: 0.84 	 Training loss: 5.50
Gradient step: 2024 	 Performance: 0.855 	 Training loss: 5.49
Gradient step: 2049 	 Performance: 0.86 	 Training loss: 5.55
Gradient step: 2074 	 Performance: 0.895 	 Training loss: 5.61
Gradient step: 2099 	 Performance: 0.825 	 Training loss: 5.56
Gradient step: 2124 	 Performance: 0.855 	 Training loss: 5.65
Gradient step: 2149 	 Performance: 0.86 	 Training loss: 5.53
Gradient step: 2174 	 Performance: 0.86 	 Training loss: 5.62
Gradient step: 2199 	 Performance: 0.86 	 Training loss: 5.67
Gradient step: 2224 	 Performance: 0.875 	 Training loss: 5.47
Gradient step: 2249 	 Performance: 0.905 	 Training loss: 5.53
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/ctxdm1seql/conformal' created successfully!
Gradient step: 0 	 Performance: 0.035 	 Training loss: 3.00
Gradient step: 24 	 Performance: 0.07 	 Training loss: 56.75
Gradient step: 49 	 Performance: 0.07 	 Training loss: 36.59
Gradient step: 74 	 Performance: 0.15 	 Training loss: 30.16
Gradient step: 99 	 Performance: 0.16 	 Training loss: 25.23
Gradient step: 124 	 Performance: 0.27 	 Training loss: 22.77
Gradient step: 149 	 Performance: 0.31 	 Training loss: 21.03
Gradient step: 174 	 Performance: 0.39 	 Training loss: 19.61
Gradient step: 199 	 Performance: 0.46 	 Training loss: 18.64
Gradient step: 224 	 Performance: 0.51 	 Training loss: 17.42
Gradient step: 249 	 Performance: 0.475 	 Training loss: 16.65
Gradient step: 274 	 Performance: 0.49 	 Training loss: 16.06
Gradient step: 299 	 Performance: 0.585 	 Training loss: 15.35
Gradient step: 324 	 Performance: 0.515 	 Training loss: 14.94
Gradient step: 349 	 Performance: 0.585 	 Training loss: 14.42
Gradient step: 374 	 Performance: 0.61 	 Training loss: 14.06
Gradient step: 399 	 Performance: 0.585 	 Training loss: 13.60
Gradient step: 424 	 Performance: 0.52 	 Training loss: 13.49
Gradient step: 449 	 Performance: 0.62 	 Training loss: 13.00
Gradient step: 474 	 Performance: 0.56 	 Training loss: 12.62
Gradient step: 499 	 Performance: 0.565 	 Training loss: 12.23
Gradient step: 524 	 Performance: 0.69 	 Training loss: 11.50
Gradient step: 549 	 Performance: 0.705 	 Training loss: 11.39
Gradient step: 574 	 Performance: 0.645 	 Training loss: 11.12
Gradient step: 599 	 Performance: 0.58 	 Training loss: 10.90
Gradient step: 624 	 Performance: 0.645 	 Training loss: 10.43
Gradient step: 649 	 Performance: 0.675 	 Training loss: 10.09
Gradient step: 674 	 Performance: 0.695 	 Training loss: 9.75
Gradient step: 699 	 Performance: 0.7 	 Training loss: 10.05
Gradient step: 724 	 Performance: 0.68 	 Training loss: 9.33
Gradient step: 749 	 Performance: 0.715 	 Training loss: 8.98
Gradient step: 774 	 Performance: 0.81 	 Training loss: 8.96
Gradient step: 799 	 Performance: 0.645 	 Training loss: 8.38
Gradient step: 824 	 Performance: 0.825 	 Training loss: 8.28
Gradient step: 849 	 Performance: 0.75 	 Training loss: 8.28
Gradient step: 874 	 Performance: 0.815 	 Training loss: 8.02
Gradient step: 899 	 Performance: 0.78 	 Training loss: 7.84
Gradient step: 924 	 Performance: 0.765 	 Training loss: 7.61
Gradient step: 949 	 Performance: 0.75 	 Training loss: 7.63
Gradient step: 974 	 Performance: 0.815 	 Training loss: 7.47
Gradient step: 999 	 Performance: 0.81 	 Training loss: 7.07
Gradient step: 1024 	 Performance: 0.815 	 Training loss: 6.90
Gradient step: 1049 	 Performance: 0.82 	 Training loss: 6.92
Gradient step: 1074 	 Performance: 0.865 	 Training loss: 6.69
Gradient step: 1099 	 Performance: 0.83 	 Training loss: 6.79
Gradient step: 1124 	 Performance: 0.815 	 Training loss: 6.53
Gradient step: 1149 	 Performance: 0.85 	 Training loss: 6.84
Gradient step: 1174 	 Performance: 0.825 	 Training loss: 6.70
Gradient step: 1199 	 Performance: 0.79 	 Training loss: 6.65
Gradient step: 1224 	 Performance: 0.845 	 Training loss: 6.68
Gradient step: 1249 	 Performance: 0.82 	 Training loss: 6.73
Gradient step: 1274 	 Performance: 0.775 	 Training loss: 6.74
Gradient step: 1299 	 Performance: 0.84 	 Training loss: 6.76
Gradient step: 1324 	 Performance: 0.805 	 Training loss: 6.05
Gradient step: 1349 	 Performance: 0.815 	 Training loss: 5.99
Gradient step: 1374 	 Performance: 0.87 	 Training loss: 5.61
Gradient step: 1399 	 Performance: 0.89 	 Training loss: 5.39
Gradient step: 1424 	 Performance: 0.855 	 Training loss: 5.20
Gradient step: 1449 	 Performance: 0.825 	 Training loss: 5.14
Gradient step: 1474 	 Performance: 0.885 	 Training loss: 5.01
Gradient step: 1499 	 Performance: 0.905 	 Training loss: 5.17
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/ctxdm1seql/None' created successfully!
Gradient step: 0 	 Performance: 0.075 	 Training loss: 3.01
Gradient step: 24 	 Performance: 0.05 	 Training loss: 53.16
Gradient step: 49 	 Performance: 0.04 	 Training loss: 36.74
Gradient step: 74 	 Performance: 0.1 	 Training loss: 31.40
Gradient step: 99 	 Performance: 0.23 	 Training loss: 25.97
Gradient step: 124 	 Performance: 0.22 	 Training loss: 22.12
Gradient step: 149 	 Performance: 0.23 	 Training loss: 19.51
Gradient step: 174 	 Performance: 0.335 	 Training loss: 17.53
Gradient step: 199 	 Performance: 0.435 	 Training loss: 16.33
Gradient step: 224 	 Performance: 0.4 	 Training loss: 15.24
Gradient step: 249 	 Performance: 0.415 	 Training loss: 14.56
Gradient step: 274 	 Performance: 0.54 	 Training loss: 13.71
Gradient step: 299 	 Performance: 0.535 	 Training loss: 13.07
Gradient step: 324 	 Performance: 0.645 	 Training loss: 12.97
Gradient step: 349 	 Performance: 0.6 	 Training loss: 12.33
Gradient step: 374 	 Performance: 0.65 	 Training loss: 11.83
Gradient step: 399 	 Performance: 0.51 	 Training loss: 11.50
Gradient step: 424 	 Performance: 0.64 	 Training loss: 11.44
Gradient step: 449 	 Performance: 0.62 	 Training loss: 11.17
Gradient step: 474 	 Performance: 0.675 	 Training loss: 10.75
Gradient step: 499 	 Performance: 0.72 	 Training loss: 10.22
Gradient step: 524 	 Performance: 0.66 	 Training loss: 10.40
Gradient step: 549 	 Performance: 0.75 	 Training loss: 9.99
Gradient step: 574 	 Performance: 0.765 	 Training loss: 10.13
Gradient step: 599 	 Performance: 0.735 	 Training loss: 9.85
Gradient step: 624 	 Performance: 0.64 	 Training loss: 9.81
Gradient step: 649 	 Performance: 0.75 	 Training loss: 9.25
Gradient step: 674 	 Performance: 0.635 	 Training loss: 9.34
Gradient step: 699 	 Performance: 0.74 	 Training loss: 9.08
Gradient step: 724 	 Performance: 0.74 	 Training loss: 9.07
Gradient step: 749 	 Performance: 0.845 	 Training loss: 8.55
Gradient step: 774 	 Performance: 0.765 	 Training loss: 8.72
Gradient step: 799 	 Performance: 0.725 	 Training loss: 8.69
Gradient step: 824 	 Performance: 0.685 	 Training loss: 8.70
Gradient step: 849 	 Performance: 0.79 	 Training loss: 8.89
Gradient step: 874 	 Performance: 0.79 	 Training loss: 8.34
Gradient step: 899 	 Performance: 0.805 	 Training loss: 8.27
Gradient step: 924 	 Performance: 0.7 	 Training loss: 8.42
Gradient step: 949 	 Performance: 0.78 	 Training loss: 8.24
Gradient step: 974 	 Performance: 0.785 	 Training loss: 8.32
Gradient step: 999 	 Performance: 0.805 	 Training loss: 8.17
Gradient step: 1024 	 Performance: 0.79 	 Training loss: 7.89
Gradient step: 1049 	 Performance: 0.8 	 Training loss: 7.52
Gradient step: 1074 	 Performance: 0.78 	 Training loss: 7.07
Gradient step: 1099 	 Performance: 0.815 	 Training loss: 6.98
Gradient step: 1124 	 Performance: 0.74 	 Training loss: 7.02
Gradient step: 1149 	 Performance: 0.815 	 Training loss: 7.16
Gradient step: 1174 	 Performance: 0.805 	 Training loss: 6.93
Gradient step: 1199 	 Performance: 0.835 	 Training loss: 6.93
Gradient step: 1224 	 Performance: 0.775 	 Training loss: 6.89
Gradient step: 1249 	 Performance: 0.755 	 Training loss: 6.88
Gradient step: 1274 	 Performance: 0.855 	 Training loss: 6.95
Gradient step: 1299 	 Performance: 0.755 	 Training loss: 6.91
Gradient step: 1324 	 Performance: 0.86 	 Training loss: 6.93
Gradient step: 1349 	 Performance: 0.8 	 Training loss: 7.02
Gradient step: 1374 	 Performance: 0.825 	 Training loss: 7.03
Gradient step: 1399 	 Performance: 0.855 	 Training loss: 6.78
Gradient step: 1424 	 Performance: 0.835 	 Training loss: 6.82
Gradient step: 1449 	 Performance: 0.825 	 Training loss: 6.87
Gradient step: 1474 	 Performance: 0.795 	 Training loss: 6.69
Gradient step: 1499 	 Performance: 0.795 	 Training loss: 6.97
Gradient step: 1524 	 Performance: 0.82 	 Training loss: 6.86
Gradient step: 1549 	 Performance: 0.775 	 Training loss: 6.79
Gradient step: 1574 	 Performance: 0.84 	 Training loss: 6.83
Gradient step: 1599 	 Performance: 0.815 	 Training loss: 6.86
Gradient step: 1624 	 Performance: 0.825 	 Training loss: 6.60
Gradient step: 1649 	 Performance: 0.805 	 Training loss: 6.76
Gradient step: 1674 	 Performance: 0.81 	 Training loss: 6.85
Gradient step: 1699 	 Performance: 0.825 	 Training loss: 6.79
Gradient step: 1724 	 Performance: 0.81 	 Training loss: 6.83
Gradient step: 1749 	 Performance: 0.83 	 Training loss: 6.72
Gradient step: 1774 	 Performance: 0.845 	 Training loss: 6.86
Gradient step: 1799 	 Performance: 0.785 	 Training loss: 6.80
Gradient step: 1824 	 Performance: 0.86 	 Training loss: 6.77
Gradient step: 1849 	 Performance: 0.805 	 Training loss: 6.81
Gradient step: 1874 	 Performance: 0.845 	 Training loss: 6.62
Gradient step: 1899 	 Performance: 0.85 	 Training loss: 6.67
Gradient step: 1924 	 Performance: 0.8 	 Training loss: 6.70
Gradient step: 1949 	 Performance: 0.795 	 Training loss: 6.70
Gradient step: 1974 	 Performance: 0.86 	 Training loss: 6.70
Gradient step: 1999 	 Performance: 0.825 	 Training loss: 6.67
Gradient step: 2024 	 Performance: 0.865 	 Training loss: 6.89
Gradient step: 2049 	 Performance: 0.81 	 Training loss: 6.58
Gradient step: 2074 	 Performance: 0.84 	 Training loss: 6.80
Gradient step: 2099 	 Performance: 0.815 	 Training loss: 6.56
Gradient step: 2124 	 Performance: 0.85 	 Training loss: 6.82
Gradient step: 2149 	 Performance: 0.845 	 Training loss: 6.68
Gradient step: 2174 	 Performance: 0.79 	 Training loss: 6.80
Gradient step: 2199 	 Performance: 0.825 	 Training loss: 6.54
Gradient step: 2224 	 Performance: 0.8 	 Training loss: 6.62
Gradient step: 2249 	 Performance: 0.825 	 Training loss: 6.51
Gradient step: 2274 	 Performance: 0.82 	 Training loss: 6.72
Gradient step: 2299 	 Performance: 0.8 	 Training loss: 6.61
Gradient step: 2324 	 Performance: 0.84 	 Training loss: 6.75
Gradient step: 2349 	 Performance: 0.845 	 Training loss: 6.66
Gradient step: 2374 	 Performance: 0.82 	 Training loss: 6.60
Gradient step: 2399 	 Performance: 0.83 	 Training loss: 6.52
Gradient step: 2424 	 Performance: 0.865 	 Training loss: 6.72
Gradient step: 2449 	 Performance: 0.82 	 Training loss: 6.71
Gradient step: 2474 	 Performance: 0.825 	 Training loss: 6.64
Gradient step: 2499 	 Performance: 0.815 	 Training loss: 6.69
Gradient step: 2524 	 Performance: 0.835 	 Training loss: 6.69
Gradient step: 2549 	 Performance: 0.785 	 Training loss: 6.75
Gradient step: 2574 	 Performance: 0.775 	 Training loss: 6.60
Gradient step: 2599 	 Performance: 0.82 	 Training loss: 6.69
Gradient step: 2624 	 Performance: 0.84 	 Training loss: 6.54
Gradient step: 2649 	 Performance: 0.805 	 Training loss: 6.79
Gradient step: 2674 	 Performance: 0.79 	 Training loss: 6.48
Gradient step: 2699 	 Performance: 0.84 	 Training loss: 6.76
Gradient step: 2724 	 Performance: 0.855 	 Training loss: 6.68
Gradient step: 2749 	 Performance: 0.85 	 Training loss: 6.72
Gradient step: 2774 	 Performance: 0.84 	 Training loss: 6.67
Gradient step: 2799 	 Performance: 0.84 	 Training loss: 6.67
Gradient step: 2824 	 Performance: 0.81 	 Training loss: 6.67
Gradient step: 2849 	 Performance: 0.815 	 Training loss: 6.74
Gradient step: 2874 	 Performance: 0.82 	 Training loss: 6.56
Gradient step: 2899 	 Performance: 0.79 	 Training loss: 6.77
Gradient step: 2924 	 Performance: 0.835 	 Training loss: 6.61
Gradient step: 2949 	 Performance: 0.845 	 Training loss: 6.69
Gradient step: 2974 	 Performance: 0.825 	 Training loss: 6.62
Gradient step: 2999 	 Performance: 0.78 	 Training loss: 6.54
Gradient step: 3024 	 Performance: 0.785 	 Training loss: 6.55
Gradient step: 3049 	 Performance: 0.77 	 Training loss: 6.63
Gradient step: 3074 	 Performance: 0.775 	 Training loss: 6.43
Gradient step: 3099 	 Performance: 0.825 	 Training loss: 6.56
Gradient step: 3124 	 Performance: 0.8 	 Training loss: 6.76
Gradient step: 3149 	 Performance: 0.805 	 Training loss: 6.71
Gradient step: 3174 	 Performance: 0.86 	 Training loss: 6.77
Gradient step: 3199 	 Performance: 0.805 	 Training loss: 6.63
Gradient step: 3224 	 Performance: 0.78 	 Training loss: 6.66
Gradient step: 3249 	 Performance: 0.85 	 Training loss: 6.60
Gradient step: 3274 	 Performance: 0.835 	 Training loss: 6.52
Gradient step: 3299 	 Performance: 0.83 	 Training loss: 6.63
Gradient step: 3324 	 Performance: 0.815 	 Training loss: 6.70
Gradient step: 3349 	 Performance: 0.82 	 Training loss: 6.76
Gradient step: 3374 	 Performance: 0.765 	 Training loss: 6.57
Gradient step: 3399 	 Performance: 0.91 	 Training loss: 6.67
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/ctxdm1seql/conformal' created successfully!
Gradient step: 0 	 Performance: 0.065 	 Training loss: 3.01
Gradient step: 24 	 Performance: 0.075 	 Training loss: 56.62
Gradient step: 49 	 Performance: 0.075 	 Training loss: 36.76
Gradient step: 74 	 Performance: 0.13 	 Training loss: 30.49
Gradient step: 99 	 Performance: 0.23 	 Training loss: 25.51
Gradient step: 124 	 Performance: 0.265 	 Training loss: 22.69
Gradient step: 149 	 Performance: 0.225 	 Training loss: 21.17
Gradient step: 174 	 Performance: 0.37 	 Training loss: 19.47
Gradient step: 199 	 Performance: 0.45 	 Training loss: 18.60
Gradient step: 224 	 Performance: 0.465 	 Training loss: 17.42
Gradient step: 249 	 Performance: 0.465 	 Training loss: 16.80
Gradient step: 274 	 Performance: 0.495 	 Training loss: 15.76
Gradient step: 299 	 Performance: 0.505 	 Training loss: 15.38
Gradient step: 324 	 Performance: 0.57 	 Training loss: 14.41
Gradient step: 349 	 Performance: 0.52 	 Training loss: 13.71
Gradient step: 374 	 Performance: 0.575 	 Training loss: 13.06
Gradient step: 399 	 Performance: 0.625 	 Training loss: 12.69
Gradient step: 424 	 Performance: 0.605 	 Training loss: 12.19
Gradient step: 449 	 Performance: 0.69 	 Training loss: 11.75
Gradient step: 474 	 Performance: 0.73 	 Training loss: 11.15
Gradient step: 499 	 Performance: 0.66 	 Training loss: 10.83
Gradient step: 524 	 Performance: 0.685 	 Training loss: 10.35
Gradient step: 549 	 Performance: 0.76 	 Training loss: 10.18
Gradient step: 574 	 Performance: 0.75 	 Training loss: 9.67
Gradient step: 599 	 Performance: 0.76 	 Training loss: 9.41
Gradient step: 624 	 Performance: 0.735 	 Training loss: 9.08
Gradient step: 649 	 Performance: 0.76 	 Training loss: 8.77
Gradient step: 674 	 Performance: 0.78 	 Training loss: 8.38
Gradient step: 699 	 Performance: 0.765 	 Training loss: 8.39
Gradient step: 724 	 Performance: 0.82 	 Training loss: 8.20
Gradient step: 749 	 Performance: 0.825 	 Training loss: 7.76
Gradient step: 774 	 Performance: 0.72 	 Training loss: 7.90
Gradient step: 799 	 Performance: 0.775 	 Training loss: 7.64
Gradient step: 824 	 Performance: 0.79 	 Training loss: 7.63
Gradient step: 849 	 Performance: 0.76 	 Training loss: 7.05
Gradient step: 874 	 Performance: 0.78 	 Training loss: 7.34
Gradient step: 899 	 Performance: 0.795 	 Training loss: 7.34
Gradient step: 924 	 Performance: 0.83 	 Training loss: 6.94
Gradient step: 949 	 Performance: 0.785 	 Training loss: 6.89
Gradient step: 974 	 Performance: 0.79 	 Training loss: 7.04
Gradient step: 999 	 Performance: 0.85 	 Training loss: 6.76
Gradient step: 1024 	 Performance: 0.775 	 Training loss: 6.72
Gradient step: 1049 	 Performance: 0.805 	 Training loss: 7.07
Gradient step: 1074 	 Performance: 0.795 	 Training loss: 6.62
Gradient step: 1099 	 Performance: 0.8 	 Training loss: 6.36
Gradient step: 1124 	 Performance: 0.885 	 Training loss: 5.97
Gradient step: 1149 	 Performance: 0.805 	 Training loss: 6.33
Gradient step: 1174 	 Performance: 0.86 	 Training loss: 6.24
Gradient step: 1199 	 Performance: 0.81 	 Training loss: 6.53
Gradient step: 1224 	 Performance: 0.755 	 Training loss: 6.45
Gradient step: 1249 	 Performance: 0.83 	 Training loss: 6.04
Gradient step: 1274 	 Performance: 0.82 	 Training loss: 5.87
Gradient step: 1299 	 Performance: 0.885 	 Training loss: 6.12
Gradient step: 1324 	 Performance: 0.78 	 Training loss: 5.61
Gradient step: 1349 	 Performance: 0.76 	 Training loss: 5.95
Gradient step: 1374 	 Performance: 0.91 	 Training loss: 5.53
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/ctxdm1seql/None' created successfully!
