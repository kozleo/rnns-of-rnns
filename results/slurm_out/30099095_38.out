Gradient step: 0 	 Performance: 0.475 	 Training loss: 2.97
Gradient step: 24 	 Performance: 0.565 	 Training loss: 39.97
Gradient step: 49 	 Performance: 0.52 	 Training loss: 16.94
Gradient step: 74 	 Performance: 0.48 	 Training loss: 15.61
Gradient step: 99 	 Performance: 0.54 	 Training loss: 15.03
Gradient step: 124 	 Performance: 0.59 	 Training loss: 13.74
Gradient step: 149 	 Performance: 0.515 	 Training loss: 12.66
Gradient step: 174 	 Performance: 0.57 	 Training loss: 11.88
Gradient step: 199 	 Performance: 0.55 	 Training loss: 10.94
Gradient step: 224 	 Performance: 0.555 	 Training loss: 10.53
Gradient step: 249 	 Performance: 0.535 	 Training loss: 10.04
Gradient step: 274 	 Performance: 0.525 	 Training loss: 9.68
Gradient step: 299 	 Performance: 0.585 	 Training loss: 9.31
Gradient step: 324 	 Performance: 0.595 	 Training loss: 9.01
Gradient step: 349 	 Performance: 0.52 	 Training loss: 8.59
Gradient step: 374 	 Performance: 0.48 	 Training loss: 8.38
Gradient step: 399 	 Performance: 0.59 	 Training loss: 7.86
Gradient step: 424 	 Performance: 0.66 	 Training loss: 7.72
Gradient step: 449 	 Performance: 0.625 	 Training loss: 7.41
Gradient step: 474 	 Performance: 0.625 	 Training loss: 6.96
Gradient step: 499 	 Performance: 0.685 	 Training loss: 6.50
Gradient step: 524 	 Performance: 0.63 	 Training loss: 6.22
Gradient step: 549 	 Performance: 0.645 	 Training loss: 5.92
Gradient step: 574 	 Performance: 0.735 	 Training loss: 5.64
Gradient step: 599 	 Performance: 0.715 	 Training loss: 5.39
Gradient step: 624 	 Performance: 0.78 	 Training loss: 5.06
Gradient step: 649 	 Performance: 0.74 	 Training loss: 4.74
Gradient step: 674 	 Performance: 0.75 	 Training loss: 4.42
Gradient step: 699 	 Performance: 0.725 	 Training loss: 4.20
Gradient step: 724 	 Performance: 0.74 	 Training loss: 4.11
Gradient step: 749 	 Performance: 0.75 	 Training loss: 4.11
Gradient step: 774 	 Performance: 0.705 	 Training loss: 3.90
Gradient step: 799 	 Performance: 0.785 	 Training loss: 3.90
Gradient step: 824 	 Performance: 0.74 	 Training loss: 3.71
Gradient step: 849 	 Performance: 0.82 	 Training loss: 3.68
Gradient step: 874 	 Performance: 0.79 	 Training loss: 3.31
Gradient step: 899 	 Performance: 0.76 	 Training loss: 3.79
Gradient step: 924 	 Performance: 0.855 	 Training loss: 2.90
Gradient step: 949 	 Performance: 0.785 	 Training loss: 2.88
Gradient step: 974 	 Performance: 0.82 	 Training loss: 2.93
Gradient step: 999 	 Performance: 0.8 	 Training loss: 2.71
Gradient step: 1024 	 Performance: 0.92 	 Training loss: 2.71
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/None/dmcintr/None' already exists!
Gradient step: 0 	 Performance: 0.465 	 Training loss: 3.08
Gradient step: 24 	 Performance: 0.555 	 Training loss: 18.53
Gradient step: 49 	 Performance: 0.465 	 Training loss: 15.56
Gradient step: 74 	 Performance: 0.57 	 Training loss: 14.57
Gradient step: 99 	 Performance: 0.5 	 Training loss: 13.61
Gradient step: 124 	 Performance: 0.555 	 Training loss: 11.66
Gradient step: 149 	 Performance: 0.575 	 Training loss: 10.36
Gradient step: 174 	 Performance: 0.58 	 Training loss: 9.74
Gradient step: 199 	 Performance: 0.535 	 Training loss: 9.30
Gradient step: 224 	 Performance: 0.615 	 Training loss: 8.98
Gradient step: 249 	 Performance: 0.58 	 Training loss: 8.23
Gradient step: 274 	 Performance: 0.665 	 Training loss: 7.58
Gradient step: 299 	 Performance: 0.52 	 Training loss: 7.38
Gradient step: 324 	 Performance: 0.705 	 Training loss: 6.78
Gradient step: 349 	 Performance: 0.72 	 Training loss: 6.20
Gradient step: 374 	 Performance: 0.64 	 Training loss: 6.44
Gradient step: 399 	 Performance: 0.595 	 Training loss: 5.92
Gradient step: 424 	 Performance: 0.625 	 Training loss: 5.63
Gradient step: 449 	 Performance: 0.72 	 Training loss: 5.55
Gradient step: 474 	 Performance: 0.645 	 Training loss: 5.66
Gradient step: 499 	 Performance: 0.67 	 Training loss: 4.94
Gradient step: 524 	 Performance: 0.835 	 Training loss: 4.67
Gradient step: 549 	 Performance: 0.695 	 Training loss: 4.80
Gradient step: 574 	 Performance: 0.76 	 Training loss: 4.74
Gradient step: 599 	 Performance: 0.7 	 Training loss: 4.49
Gradient step: 624 	 Performance: 0.73 	 Training loss: 4.40
Gradient step: 649 	 Performance: 0.735 	 Training loss: 4.40
Gradient step: 674 	 Performance: 0.8 	 Training loss: 4.19
Gradient step: 699 	 Performance: 0.815 	 Training loss: 3.93
Gradient step: 724 	 Performance: 0.71 	 Training loss: 4.07
Gradient step: 749 	 Performance: 0.795 	 Training loss: 4.18
Gradient step: 774 	 Performance: 0.86 	 Training loss: 3.68
Gradient step: 799 	 Performance: 0.7 	 Training loss: 3.67
Gradient step: 824 	 Performance: 0.8 	 Training loss: 4.76
Gradient step: 849 	 Performance: 0.86 	 Training loss: 3.85
Gradient step: 874 	 Performance: 0.73 	 Training loss: 3.57
Gradient step: 899 	 Performance: 0.81 	 Training loss: 3.73
Gradient step: 924 	 Performance: 0.885 	 Training loss: 3.52
Gradient step: 949 	 Performance: 0.8 	 Training loss: 3.45
Gradient step: 974 	 Performance: 0.81 	 Training loss: 3.73
Gradient step: 999 	 Performance: 0.82 	 Training loss: 3.47
Gradient step: 1024 	 Performance: 0.78 	 Training loss: 3.62
Gradient step: 1049 	 Performance: 0.835 	 Training loss: 3.24
Gradient step: 1074 	 Performance: 0.745 	 Training loss: 3.09
Gradient step: 1099 	 Performance: 0.8 	 Training loss: 3.17
Gradient step: 1124 	 Performance: 0.825 	 Training loss: 3.05
Gradient step: 1149 	 Performance: 0.85 	 Training loss: 2.94
Gradient step: 1174 	 Performance: 0.82 	 Training loss: 3.05
Gradient step: 1199 	 Performance: 0.835 	 Training loss: 3.05
Gradient step: 1224 	 Performance: 0.91 	 Training loss: 2.59
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/dmcintr/conformal' already exists!
Gradient step: 0 	 Performance: 0.48 	 Training loss: 2.97
Gradient step: 24 	 Performance: 0.585 	 Training loss: 25.90
Gradient step: 49 	 Performance: 0.49 	 Training loss: 16.15
Gradient step: 74 	 Performance: 0.59 	 Training loss: 15.31
Gradient step: 99 	 Performance: 0.535 	 Training loss: 13.94
Gradient step: 124 	 Performance: 0.565 	 Training loss: 12.30
Gradient step: 149 	 Performance: 0.555 	 Training loss: 10.94
Gradient step: 174 	 Performance: 0.55 	 Training loss: 10.26
Gradient step: 199 	 Performance: 0.605 	 Training loss: 9.99
Gradient step: 224 	 Performance: 0.54 	 Training loss: 9.62
Gradient step: 249 	 Performance: 0.555 	 Training loss: 9.01
Gradient step: 274 	 Performance: 0.55 	 Training loss: 8.86
Gradient step: 299 	 Performance: 0.595 	 Training loss: 8.11
Gradient step: 324 	 Performance: 0.67 	 Training loss: 7.41
Gradient step: 349 	 Performance: 0.675 	 Training loss: 7.05
Gradient step: 374 	 Performance: 0.655 	 Training loss: 6.53
Gradient step: 399 	 Performance: 0.775 	 Training loss: 6.07
Gradient step: 424 	 Performance: 0.71 	 Training loss: 5.43
Gradient step: 449 	 Performance: 0.75 	 Training loss: 5.38
Gradient step: 474 	 Performance: 0.75 	 Training loss: 4.83
Gradient step: 499 	 Performance: 0.735 	 Training loss: 4.37
Gradient step: 524 	 Performance: 0.755 	 Training loss: 4.41
Gradient step: 549 	 Performance: 0.735 	 Training loss: 4.13
Gradient step: 574 	 Performance: 0.775 	 Training loss: 3.95
Gradient step: 599 	 Performance: 0.835 	 Training loss: 3.66
Gradient step: 624 	 Performance: 0.82 	 Training loss: 3.30
Gradient step: 649 	 Performance: 0.82 	 Training loss: 3.51
Gradient step: 674 	 Performance: 0.79 	 Training loss: 3.70
Gradient step: 699 	 Performance: 0.85 	 Training loss: 3.06
Gradient step: 724 	 Performance: 0.815 	 Training loss: 2.96
Gradient step: 749 	 Performance: 0.785 	 Training loss: 3.08
Gradient step: 774 	 Performance: 0.85 	 Training loss: 2.64
Gradient step: 799 	 Performance: 0.75 	 Training loss: 2.75
Gradient step: 824 	 Performance: 0.865 	 Training loss: 2.57
Gradient step: 849 	 Performance: 0.84 	 Training loss: 2.54
Gradient step: 874 	 Performance: 0.81 	 Training loss: 2.54
Gradient step: 899 	 Performance: 0.89 	 Training loss: 2.16
Gradient step: 924 	 Performance: 0.84 	 Training loss: 2.79
Gradient step: 949 	 Performance: 0.83 	 Training loss: 2.67
Gradient step: 974 	 Performance: 0.805 	 Training loss: 2.44
Gradient step: 999 	 Performance: 0.865 	 Training loss: 2.47
Gradient step: 1024 	 Performance: 0.935 	 Training loss: 1.98
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/dmcintr/None' already exists!
Gradient step: 0 	 Performance: 0.515 	 Training loss: 3.08
Gradient step: 24 	 Performance: 0.53 	 Training loss: 19.38
Gradient step: 49 	 Performance: 0.53 	 Training loss: 15.63
Gradient step: 74 	 Performance: 0.56 	 Training loss: 14.96
Gradient step: 99 	 Performance: 0.46 	 Training loss: 13.91
Gradient step: 124 	 Performance: 0.555 	 Training loss: 12.43
Gradient step: 149 	 Performance: 0.575 	 Training loss: 11.19
Gradient step: 174 	 Performance: 0.535 	 Training loss: 10.71
Gradient step: 199 	 Performance: 0.535 	 Training loss: 9.90
Gradient step: 224 	 Performance: 0.61 	 Training loss: 9.50
Gradient step: 249 	 Performance: 0.6 	 Training loss: 9.19
Gradient step: 274 	 Performance: 0.6 	 Training loss: 8.86
Gradient step: 299 	 Performance: 0.605 	 Training loss: 8.54
Gradient step: 324 	 Performance: 0.595 	 Training loss: 8.22
Gradient step: 349 	 Performance: 0.615 	 Training loss: 7.92
Gradient step: 374 	 Performance: 0.585 	 Training loss: 7.19
Gradient step: 399 	 Performance: 0.65 	 Training loss: 6.80
Gradient step: 424 	 Performance: 0.645 	 Training loss: 6.61
Gradient step: 449 	 Performance: 0.665 	 Training loss: 6.40
Gradient step: 474 	 Performance: 0.72 	 Training loss: 5.93
Gradient step: 499 	 Performance: 0.725 	 Training loss: 5.66
Gradient step: 524 	 Performance: 0.7 	 Training loss: 5.67
Gradient step: 549 	 Performance: 0.745 	 Training loss: 5.25
Gradient step: 574 	 Performance: 0.68 	 Training loss: 4.93
Gradient step: 599 	 Performance: 0.805 	 Training loss: 5.14
Gradient step: 624 	 Performance: 0.695 	 Training loss: 4.89
Gradient step: 649 	 Performance: 0.745 	 Training loss: 4.97
Gradient step: 674 	 Performance: 0.69 	 Training loss: 4.83
Gradient step: 699 	 Performance: 0.76 	 Training loss: 4.60
Gradient step: 724 	 Performance: 0.715 	 Training loss: 4.40
Gradient step: 749 	 Performance: 0.795 	 Training loss: 4.33
Gradient step: 774 	 Performance: 0.755 	 Training loss: 4.17
Gradient step: 799 	 Performance: 0.76 	 Training loss: 4.31
Gradient step: 824 	 Performance: 0.79 	 Training loss: 3.95
Gradient step: 849 	 Performance: 0.75 	 Training loss: 4.01
Gradient step: 874 	 Performance: 0.81 	 Training loss: 3.91
Gradient step: 899 	 Performance: 0.81 	 Training loss: 4.16
Gradient step: 924 	 Performance: 0.805 	 Training loss: 3.82
Gradient step: 949 	 Performance: 0.805 	 Training loss: 3.70
Gradient step: 974 	 Performance: 0.75 	 Training loss: 3.68
Gradient step: 999 	 Performance: 0.805 	 Training loss: 3.46
Gradient step: 1024 	 Performance: 0.83 	 Training loss: 3.60
Gradient step: 1049 	 Performance: 0.89 	 Training loss: 3.29
Gradient step: 1074 	 Performance: 0.855 	 Training loss: 3.25
Gradient step: 1099 	 Performance: 0.885 	 Training loss: 3.25
Gradient step: 1124 	 Performance: 0.785 	 Training loss: 3.32
Gradient step: 1149 	 Performance: 0.735 	 Training loss: 3.43
Gradient step: 1174 	 Performance: 0.825 	 Training loss: 3.34
Gradient step: 1199 	 Performance: 0.84 	 Training loss: 3.14
Gradient step: 1224 	 Performance: 0.88 	 Training loss: 3.21
Gradient step: 1249 	 Performance: 0.845 	 Training loss: 3.35
Gradient step: 1274 	 Performance: 0.775 	 Training loss: 3.21
Gradient step: 1299 	 Performance: 0.855 	 Training loss: 3.18
Gradient step: 1324 	 Performance: 0.84 	 Training loss: 3.30
Gradient step: 1349 	 Performance: 0.915 	 Training loss: 2.60
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/dmcintr/conformal' created successfully!
Gradient step: 0 	 Performance: 0.52 	 Training loss: 2.97
Gradient step: 24 	 Performance: 0.57 	 Training loss: 21.69
Gradient step: 49 	 Performance: 0.48 	 Training loss: 15.40
Gradient step: 74 	 Performance: 0.51 	 Training loss: 14.48
Gradient step: 99 	 Performance: 0.615 	 Training loss: 12.00
Gradient step: 124 	 Performance: 0.535 	 Training loss: 10.88
Gradient step: 149 	 Performance: 0.555 	 Training loss: 9.95
Gradient step: 174 	 Performance: 0.525 	 Training loss: 9.58
Gradient step: 199 	 Performance: 0.56 	 Training loss: 8.97
Gradient step: 224 	 Performance: 0.595 	 Training loss: 8.33
Gradient step: 249 	 Performance: 0.585 	 Training loss: 7.68
Gradient step: 274 	 Performance: 0.615 	 Training loss: 7.16
Gradient step: 299 	 Performance: 0.58 	 Training loss: 6.92
Gradient step: 324 	 Performance: 0.655 	 Training loss: 6.15
Gradient step: 349 	 Performance: 0.66 	 Training loss: 5.49
Gradient step: 374 	 Performance: 0.745 	 Training loss: 4.82
Gradient step: 399 	 Performance: 0.67 	 Training loss: 4.77
Gradient step: 424 	 Performance: 0.745 	 Training loss: 4.68
Gradient step: 449 	 Performance: 0.77 	 Training loss: 4.41
Gradient step: 474 	 Performance: 0.775 	 Training loss: 4.38
Gradient step: 499 	 Performance: 0.73 	 Training loss: 4.04
Gradient step: 524 	 Performance: 0.84 	 Training loss: 3.86
Gradient step: 549 	 Performance: 0.79 	 Training loss: 3.74
Gradient step: 574 	 Performance: 0.825 	 Training loss: 3.73
Gradient step: 599 	 Performance: 0.785 	 Training loss: 3.45
Gradient step: 624 	 Performance: 0.835 	 Training loss: 3.04
Gradient step: 649 	 Performance: 0.825 	 Training loss: 3.03
Gradient step: 674 	 Performance: 0.865 	 Training loss: 2.84
Gradient step: 699 	 Performance: 0.86 	 Training loss: 2.94
Gradient step: 724 	 Performance: 0.795 	 Training loss: 2.66
Gradient step: 749 	 Performance: 0.745 	 Training loss: 2.71
Gradient step: 774 	 Performance: 0.835 	 Training loss: 3.61
Gradient step: 799 	 Performance: 0.84 	 Training loss: 2.67
Gradient step: 824 	 Performance: 0.875 	 Training loss: 2.52
Gradient step: 849 	 Performance: 0.77 	 Training loss: 2.74
Gradient step: 874 	 Performance: 0.95 	 Training loss: 2.68
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/dmcintr/None' created successfully!
