Gradient step: 0 	 Performance: 0.085 	 Training loss: 3.50
Gradient step: 24 	 Performance: 0.065 	 Training loss: 59.26
Gradient step: 49 	 Performance: 0.07 	 Training loss: 37.39
Gradient step: 74 	 Performance: 0.07 	 Training loss: 32.81
Gradient step: 99 	 Performance: 0.135 	 Training loss: 27.17
Gradient step: 124 	 Performance: 0.235 	 Training loss: 23.46
Gradient step: 149 	 Performance: 0.255 	 Training loss: 21.41
Gradient step: 174 	 Performance: 0.415 	 Training loss: 20.24
Gradient step: 199 	 Performance: 0.385 	 Training loss: 18.77
Gradient step: 224 	 Performance: 0.425 	 Training loss: 17.84
Gradient step: 249 	 Performance: 0.51 	 Training loss: 17.11
Gradient step: 274 	 Performance: 0.45 	 Training loss: 16.27
Gradient step: 299 	 Performance: 0.53 	 Training loss: 15.63
Gradient step: 324 	 Performance: 0.51 	 Training loss: 14.50
Gradient step: 349 	 Performance: 0.605 	 Training loss: 13.84
Gradient step: 374 	 Performance: 0.65 	 Training loss: 13.38
Gradient step: 399 	 Performance: 0.64 	 Training loss: 12.53
Gradient step: 424 	 Performance: 0.615 	 Training loss: 12.02
Gradient step: 449 	 Performance: 0.56 	 Training loss: 11.52
Gradient step: 474 	 Performance: 0.64 	 Training loss: 11.06
Gradient step: 499 	 Performance: 0.72 	 Training loss: 10.53
Gradient step: 524 	 Performance: 0.695 	 Training loss: 10.01
Gradient step: 549 	 Performance: 0.73 	 Training loss: 9.55
Gradient step: 574 	 Performance: 0.695 	 Training loss: 9.35
Gradient step: 599 	 Performance: 0.735 	 Training loss: 9.18
Gradient step: 624 	 Performance: 0.82 	 Training loss: 9.19
Gradient step: 649 	 Performance: 0.74 	 Training loss: 8.91
Gradient step: 674 	 Performance: 0.765 	 Training loss: 8.85
Gradient step: 699 	 Performance: 0.765 	 Training loss: 8.30
Gradient step: 724 	 Performance: 0.81 	 Training loss: 8.35
Gradient step: 749 	 Performance: 0.83 	 Training loss: 7.90
Gradient step: 774 	 Performance: 0.81 	 Training loss: 7.67
Gradient step: 799 	 Performance: 0.785 	 Training loss: 7.75
Gradient step: 824 	 Performance: 0.84 	 Training loss: 7.40
Gradient step: 849 	 Performance: 0.825 	 Training loss: 7.65
Gradient step: 874 	 Performance: 0.835 	 Training loss: 7.57
Gradient step: 899 	 Performance: 0.715 	 Training loss: 7.27
Gradient step: 924 	 Performance: 0.875 	 Training loss: 7.45
Gradient step: 949 	 Performance: 0.795 	 Training loss: 7.02
Gradient step: 974 	 Performance: 0.845 	 Training loss: 6.93
Gradient step: 999 	 Performance: 0.85 	 Training loss: 6.92
Gradient step: 1024 	 Performance: 0.88 	 Training loss: 6.96
Gradient step: 1049 	 Performance: 0.84 	 Training loss: 6.89
Gradient step: 1074 	 Performance: 0.875 	 Training loss: 6.43
Gradient step: 1099 	 Performance: 0.885 	 Training loss: 6.43
Gradient step: 1124 	 Performance: 0.7 	 Training loss: 6.52
Gradient step: 1149 	 Performance: 0.705 	 Training loss: 6.16
Gradient step: 1174 	 Performance: 0.85 	 Training loss: 6.66
Gradient step: 1199 	 Performance: 0.835 	 Training loss: 6.51
Gradient step: 1224 	 Performance: 0.8 	 Training loss: 6.29
Gradient step: 1249 	 Performance: 0.805 	 Training loss: 6.04
Gradient step: 1274 	 Performance: 0.895 	 Training loss: 6.26
Gradient step: 1299 	 Performance: 0.875 	 Training loss: 5.85
Gradient step: 1324 	 Performance: 0.84 	 Training loss: 5.92
Gradient step: 1349 	 Performance: 0.815 	 Training loss: 6.00
Gradient step: 1374 	 Performance: 0.855 	 Training loss: 5.92
Gradient step: 1399 	 Performance: 0.79 	 Training loss: 6.02
Gradient step: 1424 	 Performance: 0.845 	 Training loss: 6.31
Gradient step: 1449 	 Performance: 0.84 	 Training loss: 5.72
Gradient step: 1474 	 Performance: 0.85 	 Training loss: 6.09
Gradient step: 1499 	 Performance: 0.88 	 Training loss: 5.78
Gradient step: 1524 	 Performance: 0.89 	 Training loss: 5.72
Gradient step: 1549 	 Performance: 0.86 	 Training loss: 5.46
Gradient step: 1574 	 Performance: 0.89 	 Training loss: 4.96
Gradient step: 1599 	 Performance: 0.87 	 Training loss: 4.90
Gradient step: 1624 	 Performance: 0.865 	 Training loss: 4.66
Gradient step: 1649 	 Performance: 0.875 	 Training loss: 4.77
Gradient step: 1674 	 Performance: 0.895 	 Training loss: 4.70
Gradient step: 1699 	 Performance: 0.91 	 Training loss: 4.63
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/None/ctxdm2seql/None' created successfully!
Gradient step: 0 	 Performance: 0.075 	 Training loss: 2.96
Gradient step: 24 	 Performance: 0.055 	 Training loss: 43.38
Gradient step: 49 	 Performance: 0.07 	 Training loss: 31.80
Gradient step: 74 	 Performance: 0.335 	 Training loss: 22.94
Gradient step: 99 	 Performance: 0.425 	 Training loss: 17.60
Gradient step: 124 	 Performance: 0.495 	 Training loss: 15.53
Gradient step: 149 	 Performance: 0.57 	 Training loss: 14.26
Gradient step: 174 	 Performance: 0.56 	 Training loss: 13.34
Gradient step: 199 	 Performance: 0.535 	 Training loss: 12.35
Gradient step: 224 	 Performance: 0.63 	 Training loss: 12.02
Gradient step: 249 	 Performance: 0.69 	 Training loss: 11.49
Gradient step: 274 	 Performance: 0.56 	 Training loss: 11.08
Gradient step: 299 	 Performance: 0.665 	 Training loss: 11.03
Gradient step: 324 	 Performance: 0.695 	 Training loss: 10.40
Gradient step: 349 	 Performance: 0.685 	 Training loss: 9.97
Gradient step: 374 	 Performance: 0.745 	 Training loss: 9.78
Gradient step: 399 	 Performance: 0.69 	 Training loss: 9.81
Gradient step: 424 	 Performance: 0.665 	 Training loss: 9.42
Gradient step: 449 	 Performance: 0.69 	 Training loss: 9.04
Gradient step: 474 	 Performance: 0.775 	 Training loss: 9.59
Gradient step: 499 	 Performance: 0.705 	 Training loss: 9.23
Gradient step: 524 	 Performance: 0.66 	 Training loss: 8.88
Gradient step: 549 	 Performance: 0.73 	 Training loss: 8.94
Gradient step: 574 	 Performance: 0.77 	 Training loss: 8.29
Gradient step: 599 	 Performance: 0.745 	 Training loss: 8.42
Gradient step: 624 	 Performance: 0.725 	 Training loss: 8.38
Gradient step: 649 	 Performance: 0.755 	 Training loss: 8.07
Gradient step: 674 	 Performance: 0.755 	 Training loss: 8.23
Gradient step: 699 	 Performance: 0.805 	 Training loss: 8.19
Gradient step: 724 	 Performance: 0.82 	 Training loss: 7.98
Gradient step: 749 	 Performance: 0.72 	 Training loss: 7.73
Gradient step: 774 	 Performance: 0.79 	 Training loss: 7.86
Gradient step: 799 	 Performance: 0.735 	 Training loss: 8.29
Gradient step: 824 	 Performance: 0.79 	 Training loss: 7.49
Gradient step: 849 	 Performance: 0.82 	 Training loss: 8.20
Gradient step: 874 	 Performance: 0.745 	 Training loss: 7.86
Gradient step: 899 	 Performance: 0.8 	 Training loss: 7.43
Gradient step: 924 	 Performance: 0.81 	 Training loss: 7.17
Gradient step: 949 	 Performance: 0.79 	 Training loss: 7.34
Gradient step: 974 	 Performance: 0.78 	 Training loss: 7.38
Gradient step: 999 	 Performance: 0.795 	 Training loss: 7.17
Gradient step: 1024 	 Performance: 0.85 	 Training loss: 6.54
Gradient step: 1049 	 Performance: 0.85 	 Training loss: 6.38
Gradient step: 1074 	 Performance: 0.84 	 Training loss: 6.24
Gradient step: 1099 	 Performance: 0.84 	 Training loss: 6.24
Gradient step: 1124 	 Performance: 0.835 	 Training loss: 6.09
Gradient step: 1149 	 Performance: 0.9 	 Training loss: 6.11
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/ctxdm2seql/conformal' created successfully!
Gradient step: 0 	 Performance: 0.055 	 Training loss: 3.51
Gradient step: 24 	 Performance: 0.065 	 Training loss: 57.81
Gradient step: 49 	 Performance: 0.025 	 Training loss: 36.61
Gradient step: 74 	 Performance: 0.135 	 Training loss: 31.65
Gradient step: 99 	 Performance: 0.155 	 Training loss: 25.82
Gradient step: 124 	 Performance: 0.245 	 Training loss: 22.94
Gradient step: 149 	 Performance: 0.31 	 Training loss: 21.00
Gradient step: 174 	 Performance: 0.34 	 Training loss: 19.76
Gradient step: 199 	 Performance: 0.42 	 Training loss: 19.35
Gradient step: 224 	 Performance: 0.545 	 Training loss: 17.97
Gradient step: 249 	 Performance: 0.48 	 Training loss: 17.13
Gradient step: 274 	 Performance: 0.525 	 Training loss: 16.33
Gradient step: 299 	 Performance: 0.41 	 Training loss: 15.69
Gradient step: 324 	 Performance: 0.57 	 Training loss: 14.95
Gradient step: 349 	 Performance: 0.55 	 Training loss: 14.08
Gradient step: 374 	 Performance: 0.57 	 Training loss: 14.18
Gradient step: 399 	 Performance: 0.605 	 Training loss: 13.39
Gradient step: 424 	 Performance: 0.63 	 Training loss: 12.32
Gradient step: 449 	 Performance: 0.645 	 Training loss: 12.02
Gradient step: 474 	 Performance: 0.645 	 Training loss: 11.39
Gradient step: 499 	 Performance: 0.62 	 Training loss: 11.02
Gradient step: 524 	 Performance: 0.645 	 Training loss: 11.17
Gradient step: 549 	 Performance: 0.7 	 Training loss: 10.19
Gradient step: 574 	 Performance: 0.695 	 Training loss: 9.82
Gradient step: 599 	 Performance: 0.72 	 Training loss: 9.28
Gradient step: 624 	 Performance: 0.75 	 Training loss: 9.43
Gradient step: 649 	 Performance: 0.675 	 Training loss: 9.31
Gradient step: 674 	 Performance: 0.765 	 Training loss: 9.10
Gradient step: 699 	 Performance: 0.755 	 Training loss: 8.59
Gradient step: 724 	 Performance: 0.765 	 Training loss: 8.24
Gradient step: 749 	 Performance: 0.785 	 Training loss: 8.19
Gradient step: 774 	 Performance: 0.8 	 Training loss: 8.01
Gradient step: 799 	 Performance: 0.8 	 Training loss: 7.81
Gradient step: 824 	 Performance: 0.795 	 Training loss: 7.65
Gradient step: 849 	 Performance: 0.785 	 Training loss: 7.30
Gradient step: 874 	 Performance: 0.785 	 Training loss: 7.52
Gradient step: 899 	 Performance: 0.85 	 Training loss: 7.11
Gradient step: 924 	 Performance: 0.81 	 Training loss: 7.14
Gradient step: 949 	 Performance: 0.83 	 Training loss: 6.99
Gradient step: 974 	 Performance: 0.81 	 Training loss: 7.26
Gradient step: 999 	 Performance: 0.795 	 Training loss: 6.99
Gradient step: 1024 	 Performance: 0.845 	 Training loss: 6.63
Gradient step: 1049 	 Performance: 0.86 	 Training loss: 6.74
Gradient step: 1074 	 Performance: 0.8 	 Training loss: 6.37
Gradient step: 1099 	 Performance: 0.815 	 Training loss: 6.32
Gradient step: 1124 	 Performance: 0.845 	 Training loss: 6.32
Gradient step: 1149 	 Performance: 0.81 	 Training loss: 6.44
Gradient step: 1174 	 Performance: 0.835 	 Training loss: 6.24
Gradient step: 1199 	 Performance: 0.755 	 Training loss: 6.40
Gradient step: 1224 	 Performance: 0.86 	 Training loss: 5.92
Gradient step: 1249 	 Performance: 0.87 	 Training loss: 5.96
Gradient step: 1274 	 Performance: 0.905 	 Training loss: 6.04
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/ctxdm2seql/None' created successfully!
Gradient step: 0 	 Performance: 0.055 	 Training loss: 2.96
Gradient step: 24 	 Performance: 0.075 	 Training loss: 42.96
Gradient step: 49 	 Performance: 0.195 	 Training loss: 30.86
Gradient step: 74 	 Performance: 0.375 	 Training loss: 21.18
Gradient step: 99 	 Performance: 0.46 	 Training loss: 16.48
Gradient step: 124 	 Performance: 0.505 	 Training loss: 14.80
Gradient step: 149 	 Performance: 0.605 	 Training loss: 13.80
Gradient step: 174 	 Performance: 0.645 	 Training loss: 13.03
Gradient step: 199 	 Performance: 0.65 	 Training loss: 12.17
Gradient step: 224 	 Performance: 0.625 	 Training loss: 11.93
Gradient step: 249 	 Performance: 0.62 	 Training loss: 11.01
Gradient step: 274 	 Performance: 0.6 	 Training loss: 10.81
Gradient step: 299 	 Performance: 0.65 	 Training loss: 11.03
Gradient step: 324 	 Performance: 0.71 	 Training loss: 10.38
Gradient step: 349 	 Performance: 0.665 	 Training loss: 9.71
Gradient step: 374 	 Performance: 0.75 	 Training loss: 9.78
Gradient step: 399 	 Performance: 0.715 	 Training loss: 9.53
Gradient step: 424 	 Performance: 0.72 	 Training loss: 9.27
Gradient step: 449 	 Performance: 0.765 	 Training loss: 9.10
Gradient step: 474 	 Performance: 0.705 	 Training loss: 8.75
Gradient step: 499 	 Performance: 0.76 	 Training loss: 8.88
Gradient step: 524 	 Performance: 0.765 	 Training loss: 8.80
Gradient step: 549 	 Performance: 0.81 	 Training loss: 8.78
Gradient step: 574 	 Performance: 0.785 	 Training loss: 8.57
Gradient step: 599 	 Performance: 0.745 	 Training loss: 8.30
Gradient step: 624 	 Performance: 0.8 	 Training loss: 8.18
Gradient step: 649 	 Performance: 0.735 	 Training loss: 8.41
Gradient step: 674 	 Performance: 0.78 	 Training loss: 7.97
Gradient step: 699 	 Performance: 0.76 	 Training loss: 7.89
Gradient step: 724 	 Performance: 0.775 	 Training loss: 7.66
Gradient step: 749 	 Performance: 0.79 	 Training loss: 7.52
Gradient step: 774 	 Performance: 0.73 	 Training loss: 8.03
Gradient step: 799 	 Performance: 0.825 	 Training loss: 7.72
Gradient step: 824 	 Performance: 0.81 	 Training loss: 7.35
Gradient step: 849 	 Performance: 0.805 	 Training loss: 7.54
Gradient step: 874 	 Performance: 0.85 	 Training loss: 7.41
Gradient step: 899 	 Performance: 0.89 	 Training loss: 7.42
Gradient step: 924 	 Performance: 0.835 	 Training loss: 7.17
Gradient step: 949 	 Performance: 0.75 	 Training loss: 7.04
Gradient step: 974 	 Performance: 0.79 	 Training loss: 7.80
Gradient step: 999 	 Performance: 0.85 	 Training loss: 7.30
Gradient step: 1024 	 Performance: 0.82 	 Training loss: 6.99
Gradient step: 1049 	 Performance: 0.82 	 Training loss: 7.28
Gradient step: 1074 	 Performance: 0.78 	 Training loss: 7.03
Gradient step: 1099 	 Performance: 0.855 	 Training loss: 7.20
Gradient step: 1124 	 Performance: 0.86 	 Training loss: 7.02
Gradient step: 1149 	 Performance: 0.81 	 Training loss: 7.05
Gradient step: 1174 	 Performance: 0.82 	 Training loss: 6.94
Gradient step: 1199 	 Performance: 0.885 	 Training loss: 6.27
Gradient step: 1224 	 Performance: 0.875 	 Training loss: 5.99
Gradient step: 1249 	 Performance: 0.89 	 Training loss: 6.05
Gradient step: 1274 	 Performance: 0.885 	 Training loss: 5.96
Gradient step: 1299 	 Performance: 0.88 	 Training loss: 5.77
Gradient step: 1324 	 Performance: 0.85 	 Training loss: 5.79
Gradient step: 1349 	 Performance: 0.845 	 Training loss: 5.81
Gradient step: 1374 	 Performance: 0.815 	 Training loss: 5.58
Gradient step: 1399 	 Performance: 0.87 	 Training loss: 5.87
Gradient step: 1424 	 Performance: 0.89 	 Training loss: 5.64
Gradient step: 1449 	 Performance: 0.82 	 Training loss: 5.42
Gradient step: 1474 	 Performance: 0.895 	 Training loss: 5.66
Gradient step: 1499 	 Performance: 0.865 	 Training loss: 5.71
Gradient step: 1524 	 Performance: 0.9 	 Training loss: 5.57
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/ctxdm2seql/conformal' created successfully!
Gradient step: 0 	 Performance: 0.03 	 Training loss: 3.50
Gradient step: 24 	 Performance: 0.065 	 Training loss: 59.33
Gradient step: 49 	 Performance: 0.09 	 Training loss: 37.09
Gradient step: 74 	 Performance: 0.12 	 Training loss: 32.42
Gradient step: 99 	 Performance: 0.215 	 Training loss: 26.74
Gradient step: 124 	 Performance: 0.325 	 Training loss: 23.04
Gradient step: 149 	 Performance: 0.33 	 Training loss: 21.28
Gradient step: 174 	 Performance: 0.28 	 Training loss: 19.98
Gradient step: 199 	 Performance: 0.34 	 Training loss: 18.87
Gradient step: 224 	 Performance: 0.46 	 Training loss: 17.86
Gradient step: 249 	 Performance: 0.46 	 Training loss: 17.35
Gradient step: 274 	 Performance: 0.565 	 Training loss: 16.44
Gradient step: 299 	 Performance: 0.52 	 Training loss: 15.33
Gradient step: 324 	 Performance: 0.51 	 Training loss: 14.86
Gradient step: 349 	 Performance: 0.485 	 Training loss: 14.22
Gradient step: 374 	 Performance: 0.535 	 Training loss: 13.43
Gradient step: 399 	 Performance: 0.515 	 Training loss: 13.32
Gradient step: 424 	 Performance: 0.57 	 Training loss: 12.63
Gradient step: 449 	 Performance: 0.69 	 Training loss: 11.80
Gradient step: 474 	 Performance: 0.665 	 Training loss: 11.54
Gradient step: 499 	 Performance: 0.575 	 Training loss: 11.08
Gradient step: 524 	 Performance: 0.725 	 Training loss: 10.62
Gradient step: 549 	 Performance: 0.745 	 Training loss: 10.14
Gradient step: 574 	 Performance: 0.705 	 Training loss: 9.82
Gradient step: 599 	 Performance: 0.765 	 Training loss: 9.35
Gradient step: 624 	 Performance: 0.725 	 Training loss: 9.35
Gradient step: 649 	 Performance: 0.67 	 Training loss: 9.30
Gradient step: 674 	 Performance: 0.715 	 Training loss: 9.27
Gradient step: 699 	 Performance: 0.715 	 Training loss: 8.49
Gradient step: 724 	 Performance: 0.8 	 Training loss: 8.51
Gradient step: 749 	 Performance: 0.69 	 Training loss: 7.99
Gradient step: 774 	 Performance: 0.76 	 Training loss: 7.62
Gradient step: 799 	 Performance: 0.79 	 Training loss: 7.84
Gradient step: 824 	 Performance: 0.825 	 Training loss: 7.48
Gradient step: 849 	 Performance: 0.8 	 Training loss: 7.46
Gradient step: 874 	 Performance: 0.825 	 Training loss: 8.18
Gradient step: 899 	 Performance: 0.835 	 Training loss: 7.15
Gradient step: 924 	 Performance: 0.8 	 Training loss: 7.46
Gradient step: 949 	 Performance: 0.785 	 Training loss: 7.30
Gradient step: 974 	 Performance: 0.8 	 Training loss: 6.89
Gradient step: 999 	 Performance: 0.845 	 Training loss: 7.42
Gradient step: 1024 	 Performance: 0.8 	 Training loss: 6.97
Gradient step: 1049 	 Performance: 0.78 	 Training loss: 6.71
Gradient step: 1074 	 Performance: 0.865 	 Training loss: 6.59
Gradient step: 1099 	 Performance: 0.8 	 Training loss: 6.45
Gradient step: 1124 	 Performance: 0.84 	 Training loss: 6.67
Gradient step: 1149 	 Performance: 0.865 	 Training loss: 6.28
Gradient step: 1174 	 Performance: 0.775 	 Training loss: 5.86
Gradient step: 1199 	 Performance: 0.825 	 Training loss: 5.97
Gradient step: 1224 	 Performance: 0.855 	 Training loss: 5.90
Gradient step: 1249 	 Performance: 0.855 	 Training loss: 6.07
Gradient step: 1274 	 Performance: 0.825 	 Training loss: 6.34
Gradient step: 1299 	 Performance: 0.84 	 Training loss: 6.18
Gradient step: 1324 	 Performance: 0.89 	 Training loss: 6.32
Gradient step: 1349 	 Performance: 0.825 	 Training loss: 5.90
Gradient step: 1374 	 Performance: 0.85 	 Training loss: 6.29
Gradient step: 1399 	 Performance: 0.79 	 Training loss: 6.25
Gradient step: 1424 	 Performance: 0.87 	 Training loss: 6.03
Gradient step: 1449 	 Performance: 0.855 	 Training loss: 5.97
Gradient step: 1474 	 Performance: 0.86 	 Training loss: 5.61
Gradient step: 1499 	 Performance: 0.885 	 Training loss: 6.16
Gradient step: 1524 	 Performance: 0.83 	 Training loss: 5.76
Gradient step: 1549 	 Performance: 0.865 	 Training loss: 5.70
Gradient step: 1574 	 Performance: 0.83 	 Training loss: 5.48
Gradient step: 1599 	 Performance: 0.835 	 Training loss: 5.58
Gradient step: 1624 	 Performance: 0.915 	 Training loss: 5.11
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/ctxdm2seql/None' created successfully!
