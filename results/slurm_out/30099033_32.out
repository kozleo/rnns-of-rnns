Gradient step: 0 	 Performance: 0.09 	 Training loss: 3.28
Gradient step: 24 	 Performance: 0.075 	 Training loss: 19.29
Gradient step: 49 	 Performance: 0.065 	 Training loss: 14.81
Gradient step: 74 	 Performance: 0.055 	 Training loss: 13.81
Gradient step: 99 	 Performance: 0.06 	 Training loss: 12.74
Gradient step: 124 	 Performance: 0.06 	 Training loss: 11.58
Gradient step: 149 	 Performance: 0.055 	 Training loss: 10.35
Gradient step: 174 	 Performance: 0.055 	 Training loss: 8.81
Gradient step: 199 	 Performance: 0.085 	 Training loss: 7.33
Gradient step: 224 	 Performance: 0.175 	 Training loss: 6.58
Gradient step: 249 	 Performance: 0.275 	 Training loss: 6.04
Gradient step: 274 	 Performance: 0.28 	 Training loss: 5.69
Gradient step: 299 	 Performance: 0.415 	 Training loss: 5.43
Gradient step: 324 	 Performance: 0.535 	 Training loss: 5.22
Gradient step: 349 	 Performance: 0.455 	 Training loss: 4.99
Gradient step: 374 	 Performance: 0.575 	 Training loss: 4.77
Gradient step: 399 	 Performance: 0.555 	 Training loss: 4.53
Gradient step: 424 	 Performance: 0.575 	 Training loss: 4.33
Gradient step: 449 	 Performance: 0.675 	 Training loss: 4.22
Gradient step: 474 	 Performance: 0.635 	 Training loss: 4.02
Gradient step: 499 	 Performance: 0.745 	 Training loss: 3.80
Gradient step: 524 	 Performance: 0.64 	 Training loss: 3.61
Gradient step: 549 	 Performance: 0.64 	 Training loss: 3.52
Gradient step: 574 	 Performance: 0.71 	 Training loss: 3.36
Gradient step: 599 	 Performance: 0.745 	 Training loss: 3.19
Gradient step: 624 	 Performance: 0.715 	 Training loss: 3.06
Gradient step: 649 	 Performance: 0.795 	 Training loss: 3.00
Gradient step: 674 	 Performance: 0.775 	 Training loss: 2.85
Gradient step: 699 	 Performance: 0.72 	 Training loss: 2.73
Gradient step: 724 	 Performance: 0.765 	 Training loss: 2.63
Gradient step: 749 	 Performance: 0.675 	 Training loss: 2.50
Gradient step: 774 	 Performance: 0.735 	 Training loss: 2.42
Gradient step: 799 	 Performance: 0.835 	 Training loss: 2.39
Gradient step: 824 	 Performance: 0.77 	 Training loss: 2.27
Gradient step: 849 	 Performance: 0.8 	 Training loss: 2.20
Gradient step: 874 	 Performance: 0.78 	 Training loss: 2.14
Gradient step: 899 	 Performance: 0.69 	 Training loss: 2.04
Gradient step: 924 	 Performance: 0.75 	 Training loss: 2.00
Gradient step: 949 	 Performance: 0.78 	 Training loss: 1.91
Gradient step: 974 	 Performance: 0.825 	 Training loss: 1.86
Gradient step: 999 	 Performance: 0.83 	 Training loss: 1.81
Gradient step: 1024 	 Performance: 0.825 	 Training loss: 1.75
Gradient step: 1049 	 Performance: 0.865 	 Training loss: 1.83
Gradient step: 1074 	 Performance: 0.795 	 Training loss: 1.68
Gradient step: 1099 	 Performance: 0.825 	 Training loss: 1.65
Gradient step: 1124 	 Performance: 0.845 	 Training loss: 1.62
Gradient step: 1149 	 Performance: 0.76 	 Training loss: 1.62
Gradient step: 1174 	 Performance: 0.835 	 Training loss: 1.54
Gradient step: 1199 	 Performance: 0.845 	 Training loss: 1.52
Gradient step: 1224 	 Performance: 0.855 	 Training loss: 1.50
Gradient step: 1249 	 Performance: 0.785 	 Training loss: 1.50
Gradient step: 1274 	 Performance: 0.84 	 Training loss: 1.44
Gradient step: 1299 	 Performance: 0.805 	 Training loss: 1.39
Gradient step: 1324 	 Performance: 0.835 	 Training loss: 1.37
Gradient step: 1349 	 Performance: 0.815 	 Training loss: 1.32
Gradient step: 1374 	 Performance: 0.805 	 Training loss: 1.31
Gradient step: 1399 	 Performance: 0.795 	 Training loss: 1.32
Gradient step: 1424 	 Performance: 0.845 	 Training loss: 1.26
Gradient step: 1449 	 Performance: 0.83 	 Training loss: 1.30
Gradient step: 1474 	 Performance: 0.81 	 Training loss: 1.26
Gradient step: 1499 	 Performance: 0.83 	 Training loss: 1.25
Gradient step: 1524 	 Performance: 0.795 	 Training loss: 1.28
Gradient step: 1549 	 Performance: 0.835 	 Training loss: 1.28
Gradient step: 1574 	 Performance: 0.845 	 Training loss: 1.28
Gradient step: 1599 	 Performance: 0.805 	 Training loss: 1.25
Gradient step: 1624 	 Performance: 0.805 	 Training loss: 1.26
Gradient step: 1649 	 Performance: 0.82 	 Training loss: 1.25
Gradient step: 1674 	 Performance: 0.8 	 Training loss: 1.25
Gradient step: 1699 	 Performance: 0.815 	 Training loss: 1.27
Gradient step: 1724 	 Performance: 0.89 	 Training loss: 1.24
Gradient step: 1749 	 Performance: 0.78 	 Training loss: 1.25
Gradient step: 1774 	 Performance: 0.795 	 Training loss: 1.26
Gradient step: 1799 	 Performance: 0.765 	 Training loss: 1.25
Gradient step: 1824 	 Performance: 0.815 	 Training loss: 1.23
Gradient step: 1849 	 Performance: 0.815 	 Training loss: 1.22
Gradient step: 1874 	 Performance: 0.795 	 Training loss: 1.28
Gradient step: 1899 	 Performance: 0.875 	 Training loss: 1.23
Gradient step: 1924 	 Performance: 0.76 	 Training loss: 1.24
Gradient step: 1949 	 Performance: 0.815 	 Training loss: 1.26
Gradient step: 1974 	 Performance: 0.855 	 Training loss: 1.25
Gradient step: 1999 	 Performance: 0.8 	 Training loss: 1.26
Gradient step: 2024 	 Performance: 0.83 	 Training loss: 1.26
Gradient step: 2049 	 Performance: 0.845 	 Training loss: 1.27
Gradient step: 2074 	 Performance: 0.815 	 Training loss: 1.26
Gradient step: 2099 	 Performance: 0.82 	 Training loss: 1.26
Gradient step: 2124 	 Performance: 0.81 	 Training loss: 1.24
Gradient step: 2149 	 Performance: 0.82 	 Training loss: 1.25
Gradient step: 2174 	 Performance: 0.815 	 Training loss: 1.27
Gradient step: 2199 	 Performance: 0.84 	 Training loss: 1.24
Gradient step: 2224 	 Performance: 0.845 	 Training loss: 1.25
Gradient step: 2249 	 Performance: 0.835 	 Training loss: 1.26
Gradient step: 2274 	 Performance: 0.855 	 Training loss: 1.25
Gradient step: 2299 	 Performance: 0.87 	 Training loss: 1.24
Gradient step: 2324 	 Performance: 0.835 	 Training loss: 1.25
Gradient step: 2349 	 Performance: 0.855 	 Training loss: 1.24
Gradient step: 2374 	 Performance: 0.8 	 Training loss: 1.24
Gradient step: 2399 	 Performance: 0.815 	 Training loss: 1.24
Gradient step: 2424 	 Performance: 0.83 	 Training loss: 1.24
Gradient step: 2449 	 Performance: 0.81 	 Training loss: 1.24
Gradient step: 2474 	 Performance: 0.84 	 Training loss: 1.27
Gradient step: 2499 	 Performance: 0.835 	 Training loss: 1.25
Gradient step: 2524 	 Performance: 0.83 	 Training loss: 1.25
Gradient step: 2549 	 Performance: 0.855 	 Training loss: 1.28
Gradient step: 2574 	 Performance: 0.845 	 Training loss: 1.26
Gradient step: 2599 	 Performance: 0.88 	 Training loss: 1.23
Gradient step: 2624 	 Performance: 0.81 	 Training loss: 1.25
Gradient step: 2649 	 Performance: 0.855 	 Training loss: 1.25
Gradient step: 2674 	 Performance: 0.825 	 Training loss: 1.24
Gradient step: 2699 	 Performance: 0.81 	 Training loss: 1.29
Gradient step: 2724 	 Performance: 0.855 	 Training loss: 1.25
Gradient step: 2749 	 Performance: 0.82 	 Training loss: 1.25
Gradient step: 2774 	 Performance: 0.83 	 Training loss: 1.25
Gradient step: 2799 	 Performance: 0.83 	 Training loss: 1.26
Gradient step: 2824 	 Performance: 0.82 	 Training loss: 1.24
Gradient step: 2849 	 Performance: 0.865 	 Training loss: 1.25
Gradient step: 2874 	 Performance: 0.855 	 Training loss: 1.26
Gradient step: 2899 	 Performance: 0.855 	 Training loss: 1.25
Gradient step: 2924 	 Performance: 0.865 	 Training loss: 1.24
Gradient step: 2949 	 Performance: 0.825 	 Training loss: 1.27
Gradient step: 2974 	 Performance: 0.865 	 Training loss: 1.27
Gradient step: 2999 	 Performance: 0.815 	 Training loss: 1.27
Gradient step: 3024 	 Performance: 0.79 	 Training loss: 1.24
Gradient step: 3049 	 Performance: 0.825 	 Training loss: 1.24
Gradient step: 3074 	 Performance: 0.825 	 Training loss: 1.27
Gradient step: 3099 	 Performance: 0.84 	 Training loss: 1.25
Gradient step: 3124 	 Performance: 0.855 	 Training loss: 1.29
Gradient step: 3149 	 Performance: 0.815 	 Training loss: 1.26
Gradient step: 3174 	 Performance: 0.82 	 Training loss: 1.25
Gradient step: 3199 	 Performance: 0.795 	 Training loss: 1.26
Gradient step: 3224 	 Performance: 0.82 	 Training loss: 1.25
Gradient step: 3249 	 Performance: 0.82 	 Training loss: 1.27
Gradient step: 3274 	 Performance: 0.8 	 Training loss: 1.26
Gradient step: 3299 	 Performance: 0.87 	 Training loss: 1.29
Gradient step: 3324 	 Performance: 0.895 	 Training loss: 1.26
Gradient step: 3349 	 Performance: 0.83 	 Training loss: 1.25
Gradient step: 3374 	 Performance: 0.81 	 Training loss: 1.26
Gradient step: 3399 	 Performance: 0.815 	 Training loss: 1.24
Gradient step: 3424 	 Performance: 0.805 	 Training loss: 1.24
Gradient step: 3449 	 Performance: 0.785 	 Training loss: 1.25
Gradient step: 3474 	 Performance: 0.815 	 Training loss: 1.24
Gradient step: 3499 	 Performance: 0.835 	 Training loss: 1.26
Gradient step: 3524 	 Performance: 0.815 	 Training loss: 1.25
Gradient step: 3549 	 Performance: 0.85 	 Training loss: 1.23
Gradient step: 3574 	 Performance: 0.85 	 Training loss: 1.26
Gradient step: 3599 	 Performance: 0.83 	 Training loss: 1.24
Gradient step: 3624 	 Performance: 0.83 	 Training loss: 1.23
Gradient step: 3649 	 Performance: 0.83 	 Training loss: 1.25
Gradient step: 3674 	 Performance: 0.835 	 Training loss: 1.23
Gradient step: 3699 	 Performance: 0.825 	 Training loss: 1.26
Gradient step: 3724 	 Performance: 0.84 	 Training loss: 1.26
Gradient step: 3749 	 Performance: 0.9 	 Training loss: 1.24
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/None/multidlydmintr/None' created successfully!
Gradient step: 0 	 Performance: 0.06 	 Training loss: 3.03
Gradient step: 24 	 Performance: 0.055 	 Training loss: 19.60
Gradient step: 49 	 Performance: 0.045 	 Training loss: 14.66
Gradient step: 74 	 Performance: 0.06 	 Training loss: 13.33
Gradient step: 99 	 Performance: 0.05 	 Training loss: 11.25
Gradient step: 124 	 Performance: 0.04 	 Training loss: 8.87
Gradient step: 149 	 Performance: 0.12 	 Training loss: 7.56
Gradient step: 174 	 Performance: 0.15 	 Training loss: 6.78
Gradient step: 199 	 Performance: 0.22 	 Training loss: 6.02
Gradient step: 224 	 Performance: 0.425 	 Training loss: 5.17
Gradient step: 249 	 Performance: 0.5 	 Training loss: 4.59
Gradient step: 274 	 Performance: 0.52 	 Training loss: 4.20
Gradient step: 299 	 Performance: 0.61 	 Training loss: 3.88
Gradient step: 324 	 Performance: 0.64 	 Training loss: 3.67
Gradient step: 349 	 Performance: 0.605 	 Training loss: 3.33
Gradient step: 374 	 Performance: 0.66 	 Training loss: 3.14
Gradient step: 399 	 Performance: 0.73 	 Training loss: 2.96
Gradient step: 424 	 Performance: 0.645 	 Training loss: 2.74
Gradient step: 449 	 Performance: 0.66 	 Training loss: 2.60
Gradient step: 474 	 Performance: 0.725 	 Training loss: 2.39
Gradient step: 499 	 Performance: 0.76 	 Training loss: 2.32
Gradient step: 524 	 Performance: 0.77 	 Training loss: 2.23
Gradient step: 549 	 Performance: 0.765 	 Training loss: 2.11
Gradient step: 574 	 Performance: 0.795 	 Training loss: 2.02
Gradient step: 599 	 Performance: 0.825 	 Training loss: 1.93
Gradient step: 624 	 Performance: 0.775 	 Training loss: 1.90
Gradient step: 649 	 Performance: 0.79 	 Training loss: 1.79
Gradient step: 674 	 Performance: 0.82 	 Training loss: 1.71
Gradient step: 699 	 Performance: 0.78 	 Training loss: 1.71
Gradient step: 724 	 Performance: 0.755 	 Training loss: 1.63
Gradient step: 749 	 Performance: 0.79 	 Training loss: 1.58
Gradient step: 774 	 Performance: 0.82 	 Training loss: 1.57
Gradient step: 799 	 Performance: 0.82 	 Training loss: 1.49
Gradient step: 824 	 Performance: 0.82 	 Training loss: 1.48
Gradient step: 849 	 Performance: 0.86 	 Training loss: 1.45
Gradient step: 874 	 Performance: 0.8 	 Training loss: 1.40
Gradient step: 899 	 Performance: 0.755 	 Training loss: 1.38
Gradient step: 924 	 Performance: 0.78 	 Training loss: 1.35
Gradient step: 949 	 Performance: 0.795 	 Training loss: 1.32
Gradient step: 974 	 Performance: 0.85 	 Training loss: 1.31
Gradient step: 999 	 Performance: 0.82 	 Training loss: 1.29
Gradient step: 1024 	 Performance: 0.77 	 Training loss: 1.29
Gradient step: 1049 	 Performance: 0.875 	 Training loss: 1.21
Gradient step: 1074 	 Performance: 0.85 	 Training loss: 1.25
Gradient step: 1099 	 Performance: 0.825 	 Training loss: 1.26
Gradient step: 1124 	 Performance: 0.85 	 Training loss: 1.22
Gradient step: 1149 	 Performance: 0.825 	 Training loss: 1.14
Gradient step: 1174 	 Performance: 0.855 	 Training loss: 1.16
Gradient step: 1199 	 Performance: 0.82 	 Training loss: 1.15
Gradient step: 1224 	 Performance: 0.835 	 Training loss: 1.11
Gradient step: 1249 	 Performance: 0.83 	 Training loss: 1.12
Gradient step: 1274 	 Performance: 0.865 	 Training loss: 1.09
Gradient step: 1299 	 Performance: 0.84 	 Training loss: 1.09
Gradient step: 1324 	 Performance: 0.825 	 Training loss: 1.11
Gradient step: 1349 	 Performance: 0.82 	 Training loss: 1.03
Gradient step: 1374 	 Performance: 0.845 	 Training loss: 1.03
Gradient step: 1399 	 Performance: 0.85 	 Training loss: 1.01
Gradient step: 1424 	 Performance: 0.875 	 Training loss: 1.00
Gradient step: 1449 	 Performance: 0.855 	 Training loss: 1.00
Gradient step: 1474 	 Performance: 0.825 	 Training loss: 1.01
Gradient step: 1499 	 Performance: 0.855 	 Training loss: 1.01
Gradient step: 1524 	 Performance: 0.89 	 Training loss: 1.01
Gradient step: 1549 	 Performance: 0.84 	 Training loss: 0.99
Gradient step: 1574 	 Performance: 0.855 	 Training loss: 0.98
Gradient step: 1599 	 Performance: 0.865 	 Training loss: 1.00
Gradient step: 1624 	 Performance: 0.84 	 Training loss: 0.99
Gradient step: 1649 	 Performance: 0.84 	 Training loss: 0.99
Gradient step: 1674 	 Performance: 0.845 	 Training loss: 0.97
Gradient step: 1699 	 Performance: 0.805 	 Training loss: 0.99
Gradient step: 1724 	 Performance: 0.875 	 Training loss: 1.02
Gradient step: 1749 	 Performance: 0.89 	 Training loss: 0.99
Gradient step: 1774 	 Performance: 0.885 	 Training loss: 1.00
Gradient step: 1799 	 Performance: 0.83 	 Training loss: 1.00
Gradient step: 1824 	 Performance: 0.845 	 Training loss: 0.98
Gradient step: 1849 	 Performance: 0.875 	 Training loss: 1.00
Gradient step: 1874 	 Performance: 0.845 	 Training loss: 0.98
Gradient step: 1899 	 Performance: 0.88 	 Training loss: 0.96
Gradient step: 1924 	 Performance: 0.885 	 Training loss: 0.96
Gradient step: 1949 	 Performance: 0.86 	 Training loss: 0.97
Gradient step: 1974 	 Performance: 0.84 	 Training loss: 0.96
Gradient step: 1999 	 Performance: 0.83 	 Training loss: 0.98
Gradient step: 2024 	 Performance: 0.82 	 Training loss: 0.99
Gradient step: 2049 	 Performance: 0.88 	 Training loss: 0.99
Gradient step: 2074 	 Performance: 0.87 	 Training loss: 0.98
Gradient step: 2099 	 Performance: 0.86 	 Training loss: 0.99
Gradient step: 2124 	 Performance: 0.855 	 Training loss: 0.96
Gradient step: 2149 	 Performance: 0.86 	 Training loss: 0.99
Gradient step: 2174 	 Performance: 0.87 	 Training loss: 0.97
Gradient step: 2199 	 Performance: 0.845 	 Training loss: 0.96
Gradient step: 2224 	 Performance: 0.83 	 Training loss: 0.95
Gradient step: 2249 	 Performance: 0.88 	 Training loss: 0.97
Gradient step: 2274 	 Performance: 0.82 	 Training loss: 0.98
Gradient step: 2299 	 Performance: 0.84 	 Training loss: 0.98
Gradient step: 2324 	 Performance: 0.88 	 Training loss: 0.96
Gradient step: 2349 	 Performance: 0.895 	 Training loss: 0.99
Gradient step: 2374 	 Performance: 0.875 	 Training loss: 0.98
Gradient step: 2399 	 Performance: 0.825 	 Training loss: 1.02
Gradient step: 2424 	 Performance: 0.87 	 Training loss: 0.97
Gradient step: 2449 	 Performance: 0.815 	 Training loss: 0.99
Gradient step: 2474 	 Performance: 0.825 	 Training loss: 0.99
Gradient step: 2499 	 Performance: 0.845 	 Training loss: 1.00
Gradient step: 2524 	 Performance: 0.89 	 Training loss: 0.98
Gradient step: 2549 	 Performance: 0.79 	 Training loss: 1.00
Gradient step: 2574 	 Performance: 0.865 	 Training loss: 0.97
Gradient step: 2599 	 Performance: 0.835 	 Training loss: 0.98
Gradient step: 2624 	 Performance: 0.835 	 Training loss: 1.00
Gradient step: 2649 	 Performance: 0.855 	 Training loss: 0.99
Gradient step: 2674 	 Performance: 0.825 	 Training loss: 0.95
Gradient step: 2699 	 Performance: 0.845 	 Training loss: 0.99
Gradient step: 2724 	 Performance: 0.83 	 Training loss: 0.98
Gradient step: 2749 	 Performance: 0.815 	 Training loss: 0.97
Gradient step: 2774 	 Performance: 0.885 	 Training loss: 1.00
Gradient step: 2799 	 Performance: 0.865 	 Training loss: 0.98
Gradient step: 2824 	 Performance: 0.84 	 Training loss: 0.99
Gradient step: 2849 	 Performance: 0.865 	 Training loss: 0.99
Gradient step: 2874 	 Performance: 0.82 	 Training loss: 0.99
Gradient step: 2899 	 Performance: 0.875 	 Training loss: 0.98
Gradient step: 2924 	 Performance: 0.85 	 Training loss: 0.99
Gradient step: 2949 	 Performance: 0.85 	 Training loss: 0.98
Gradient step: 2974 	 Performance: 0.88 	 Training loss: 1.00
Gradient step: 2999 	 Performance: 0.805 	 Training loss: 0.99
Gradient step: 3024 	 Performance: 0.805 	 Training loss: 0.99
Gradient step: 3049 	 Performance: 0.805 	 Training loss: 0.98
Gradient step: 3074 	 Performance: 0.88 	 Training loss: 0.99
Gradient step: 3099 	 Performance: 0.855 	 Training loss: 1.00
Gradient step: 3124 	 Performance: 0.86 	 Training loss: 0.98
Gradient step: 3149 	 Performance: 0.87 	 Training loss: 0.97
Gradient step: 3174 	 Performance: 0.87 	 Training loss: 0.94
Gradient step: 3199 	 Performance: 0.82 	 Training loss: 0.99
Gradient step: 3224 	 Performance: 0.87 	 Training loss: 1.00
Gradient step: 3249 	 Performance: 0.865 	 Training loss: 0.97
Gradient step: 3274 	 Performance: 0.87 	 Training loss: 0.98
Gradient step: 3299 	 Performance: 0.84 	 Training loss: 0.99
Gradient step: 3324 	 Performance: 0.855 	 Training loss: 0.98
Gradient step: 3349 	 Performance: 0.845 	 Training loss: 0.97
Gradient step: 3374 	 Performance: 0.835 	 Training loss: 0.96
Gradient step: 3399 	 Performance: 0.85 	 Training loss: 1.00
Gradient step: 3424 	 Performance: 0.94 	 Training loss: 0.97
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/multidlydmintr/conformal' created successfully!
Gradient step: 0 	 Performance: 0.06 	 Training loss: 3.29
Gradient step: 24 	 Performance: 0.045 	 Training loss: 19.29
Gradient step: 49 	 Performance: 0.04 	 Training loss: 14.79
Gradient step: 74 	 Performance: 0.045 	 Training loss: 13.87
Gradient step: 99 	 Performance: 0.07 	 Training loss: 12.76
Gradient step: 124 	 Performance: 0.055 	 Training loss: 11.56
Gradient step: 149 	 Performance: 0.065 	 Training loss: 10.27
Gradient step: 174 	 Performance: 0.055 	 Training loss: 8.80
Gradient step: 199 	 Performance: 0.115 	 Training loss: 7.40
Gradient step: 224 	 Performance: 0.15 	 Training loss: 6.57
Gradient step: 249 	 Performance: 0.26 	 Training loss: 6.05
Gradient step: 274 	 Performance: 0.28 	 Training loss: 5.68
Gradient step: 299 	 Performance: 0.275 	 Training loss: 5.43
Gradient step: 324 	 Performance: 0.385 	 Training loss: 5.17
Gradient step: 349 	 Performance: 0.46 	 Training loss: 4.88
Gradient step: 374 	 Performance: 0.575 	 Training loss: 4.67
Gradient step: 399 	 Performance: 0.565 	 Training loss: 4.54
Gradient step: 424 	 Performance: 0.6 	 Training loss: 4.24
Gradient step: 449 	 Performance: 0.64 	 Training loss: 4.10
Gradient step: 474 	 Performance: 0.59 	 Training loss: 3.90
Gradient step: 499 	 Performance: 0.605 	 Training loss: 3.74
Gradient step: 524 	 Performance: 0.7 	 Training loss: 3.55
Gradient step: 549 	 Performance: 0.675 	 Training loss: 3.39
Gradient step: 574 	 Performance: 0.72 	 Training loss: 3.23
Gradient step: 599 	 Performance: 0.735 	 Training loss: 3.12
Gradient step: 624 	 Performance: 0.655 	 Training loss: 2.97
Gradient step: 649 	 Performance: 0.805 	 Training loss: 2.88
Gradient step: 674 	 Performance: 0.735 	 Training loss: 2.75
Gradient step: 699 	 Performance: 0.76 	 Training loss: 2.65
Gradient step: 724 	 Performance: 0.72 	 Training loss: 2.53
Gradient step: 749 	 Performance: 0.765 	 Training loss: 2.45
Gradient step: 774 	 Performance: 0.735 	 Training loss: 2.32
Gradient step: 799 	 Performance: 0.72 	 Training loss: 2.27
Gradient step: 824 	 Performance: 0.82 	 Training loss: 2.20
Gradient step: 849 	 Performance: 0.795 	 Training loss: 2.07
Gradient step: 874 	 Performance: 0.795 	 Training loss: 2.07
Gradient step: 899 	 Performance: 0.805 	 Training loss: 2.05
Gradient step: 924 	 Performance: 0.795 	 Training loss: 1.97
Gradient step: 949 	 Performance: 0.795 	 Training loss: 1.96
Gradient step: 974 	 Performance: 0.79 	 Training loss: 1.85
Gradient step: 999 	 Performance: 0.825 	 Training loss: 1.76
Gradient step: 1024 	 Performance: 0.805 	 Training loss: 1.70
Gradient step: 1049 	 Performance: 0.785 	 Training loss: 1.65
Gradient step: 1074 	 Performance: 0.8 	 Training loss: 1.65
Gradient step: 1099 	 Performance: 0.815 	 Training loss: 1.63
Gradient step: 1124 	 Performance: 0.805 	 Training loss: 1.59
Gradient step: 1149 	 Performance: 0.835 	 Training loss: 1.55
Gradient step: 1174 	 Performance: 0.83 	 Training loss: 1.54
Gradient step: 1199 	 Performance: 0.83 	 Training loss: 1.55
Gradient step: 1224 	 Performance: 0.83 	 Training loss: 1.44
Gradient step: 1249 	 Performance: 0.825 	 Training loss: 1.45
Gradient step: 1274 	 Performance: 0.865 	 Training loss: 1.45
Gradient step: 1299 	 Performance: 0.78 	 Training loss: 1.35
Gradient step: 1324 	 Performance: 0.805 	 Training loss: 1.37
Gradient step: 1349 	 Performance: 0.83 	 Training loss: 1.34
Gradient step: 1374 	 Performance: 0.82 	 Training loss: 1.29
Gradient step: 1399 	 Performance: 0.845 	 Training loss: 1.28
Gradient step: 1424 	 Performance: 0.805 	 Training loss: 1.33
Gradient step: 1449 	 Performance: 0.86 	 Training loss: 1.28
Gradient step: 1474 	 Performance: 0.845 	 Training loss: 1.23
Gradient step: 1499 	 Performance: 0.89 	 Training loss: 1.21
Gradient step: 1524 	 Performance: 0.73 	 Training loss: 1.19
Gradient step: 1549 	 Performance: 0.81 	 Training loss: 1.24
Gradient step: 1574 	 Performance: 0.815 	 Training loss: 1.18
Gradient step: 1599 	 Performance: 0.815 	 Training loss: 1.20
Gradient step: 1624 	 Performance: 0.865 	 Training loss: 1.17
Gradient step: 1649 	 Performance: 0.84 	 Training loss: 1.13
Gradient step: 1674 	 Performance: 0.8 	 Training loss: 1.11
Gradient step: 1699 	 Performance: 0.805 	 Training loss: 1.23
Gradient step: 1724 	 Performance: 0.82 	 Training loss: 1.13
Gradient step: 1749 	 Performance: 0.815 	 Training loss: 1.08
Gradient step: 1774 	 Performance: 0.8 	 Training loss: 1.06
Gradient step: 1799 	 Performance: 0.84 	 Training loss: 1.06
Gradient step: 1824 	 Performance: 0.9 	 Training loss: 1.01
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/multidlydmintr/None' created successfully!
slurmstepd: error: *** JOB 30099046 ON node077 CANCELLED AT 2023-04-01T22:39:18 ***
