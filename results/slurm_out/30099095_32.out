Gradient step: 0 	 Performance: 0.07 	 Training loss: 3.55
Gradient step: 24 	 Performance: 0.055 	 Training loss: 16.52
Gradient step: 49 	 Performance: 0.08 	 Training loss: 8.99
Gradient step: 74 	 Performance: 0.27 	 Training loss: 5.93
Gradient step: 99 	 Performance: 0.585 	 Training loss: 4.61
Gradient step: 124 	 Performance: 0.56 	 Training loss: 4.14
Gradient step: 149 	 Performance: 0.72 	 Training loss: 3.01
Gradient step: 174 	 Performance: 0.745 	 Training loss: 2.27
Gradient step: 199 	 Performance: 0.765 	 Training loss: 1.91
Gradient step: 224 	 Performance: 0.805 	 Training loss: 1.72
Gradient step: 249 	 Performance: 0.785 	 Training loss: 1.51
Gradient step: 274 	 Performance: 0.81 	 Training loss: 1.42
Gradient step: 299 	 Performance: 0.88 	 Training loss: 1.38
Gradient step: 324 	 Performance: 0.78 	 Training loss: 1.26
Gradient step: 349 	 Performance: 0.825 	 Training loss: 1.18
Gradient step: 374 	 Performance: 0.82 	 Training loss: 1.15
Gradient step: 399 	 Performance: 0.82 	 Training loss: 1.06
Gradient step: 424 	 Performance: 0.86 	 Training loss: 1.06
Gradient step: 449 	 Performance: 0.855 	 Training loss: 1.03
Gradient step: 474 	 Performance: 0.87 	 Training loss: 1.00
Gradient step: 499 	 Performance: 0.88 	 Training loss: 1.00
Gradient step: 524 	 Performance: 0.91 	 Training loss: 0.95
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/None/multidlydmintr/None' already exists!
Gradient step: 0 	 Performance: 0.065 	 Training loss: 2.59
Gradient step: 24 	 Performance: 0.07 	 Training loss: 18.52
Gradient step: 49 	 Performance: 0.045 	 Training loss: 10.34
Gradient step: 74 	 Performance: 0.21 	 Training loss: 7.23
Gradient step: 99 	 Performance: 0.57 	 Training loss: 5.21
Gradient step: 124 	 Performance: 0.555 	 Training loss: 3.87
Gradient step: 149 	 Performance: 0.755 	 Training loss: 3.12
Gradient step: 174 	 Performance: 0.695 	 Training loss: 2.64
Gradient step: 199 	 Performance: 0.68 	 Training loss: 2.37
Gradient step: 224 	 Performance: 0.695 	 Training loss: 2.16
Gradient step: 249 	 Performance: 0.775 	 Training loss: 1.94
Gradient step: 274 	 Performance: 0.81 	 Training loss: 1.77
Gradient step: 299 	 Performance: 0.82 	 Training loss: 1.63
Gradient step: 324 	 Performance: 0.815 	 Training loss: 1.58
Gradient step: 349 	 Performance: 0.83 	 Training loss: 1.55
Gradient step: 374 	 Performance: 0.795 	 Training loss: 1.36
Gradient step: 399 	 Performance: 0.835 	 Training loss: 1.43
Gradient step: 424 	 Performance: 0.8 	 Training loss: 1.28
Gradient step: 449 	 Performance: 0.855 	 Training loss: 1.24
Gradient step: 474 	 Performance: 0.82 	 Training loss: 1.17
Gradient step: 499 	 Performance: 0.795 	 Training loss: 1.17
Gradient step: 524 	 Performance: 0.84 	 Training loss: 1.03
Gradient step: 549 	 Performance: 0.84 	 Training loss: 1.06
Gradient step: 574 	 Performance: 0.81 	 Training loss: 1.03
Gradient step: 599 	 Performance: 0.77 	 Training loss: 1.05
Gradient step: 624 	 Performance: 0.835 	 Training loss: 1.08
Gradient step: 649 	 Performance: 0.805 	 Training loss: 0.97
Gradient step: 674 	 Performance: 0.79 	 Training loss: 1.00
Gradient step: 699 	 Performance: 0.88 	 Training loss: 1.07
Gradient step: 724 	 Performance: 0.88 	 Training loss: 0.94
Gradient step: 749 	 Performance: 0.875 	 Training loss: 0.91
Gradient step: 774 	 Performance: 0.86 	 Training loss: 0.90
Gradient step: 799 	 Performance: 0.83 	 Training loss: 0.91
Gradient step: 824 	 Performance: 0.865 	 Training loss: 0.88
Gradient step: 849 	 Performance: 0.885 	 Training loss: 0.86
Gradient step: 874 	 Performance: 0.87 	 Training loss: 0.84
Gradient step: 899 	 Performance: 0.825 	 Training loss: 0.87
Gradient step: 924 	 Performance: 0.86 	 Training loss: 0.84
Gradient step: 949 	 Performance: 0.88 	 Training loss: 0.87
Gradient step: 974 	 Performance: 0.86 	 Training loss: 0.81
Gradient step: 999 	 Performance: 0.855 	 Training loss: 0.81
Gradient step: 1024 	 Performance: 0.885 	 Training loss: 0.82
Gradient step: 1049 	 Performance: 0.815 	 Training loss: 0.85
Gradient step: 1074 	 Performance: 0.86 	 Training loss: 0.87
Gradient step: 1099 	 Performance: 0.83 	 Training loss: 0.81
Gradient step: 1124 	 Performance: 0.87 	 Training loss: 0.88
Gradient step: 1149 	 Performance: 0.9 	 Training loss: 0.75
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/multidlydmintr/conformal' already exists!
Gradient step: 0 	 Performance: 0.055 	 Training loss: 3.57
Gradient step: 24 	 Performance: 0.09 	 Training loss: 16.69
Gradient step: 49 	 Performance: 0.075 	 Training loss: 9.46
Gradient step: 74 	 Performance: 0.285 	 Training loss: 5.88
Gradient step: 99 	 Performance: 0.48 	 Training loss: 4.59
Gradient step: 124 	 Performance: 0.67 	 Training loss: 3.63
Gradient step: 149 	 Performance: 0.77 	 Training loss: 2.70
Gradient step: 174 	 Performance: 0.795 	 Training loss: 2.13
Gradient step: 199 	 Performance: 0.755 	 Training loss: 1.85
Gradient step: 224 	 Performance: 0.8 	 Training loss: 1.68
Gradient step: 249 	 Performance: 0.77 	 Training loss: 1.50
Gradient step: 274 	 Performance: 0.825 	 Training loss: 1.38
Gradient step: 299 	 Performance: 0.8 	 Training loss: 1.29
Gradient step: 324 	 Performance: 0.845 	 Training loss: 1.23
Gradient step: 349 	 Performance: 0.85 	 Training loss: 1.29
Gradient step: 374 	 Performance: 0.805 	 Training loss: 1.15
Gradient step: 399 	 Performance: 0.855 	 Training loss: 1.08
Gradient step: 424 	 Performance: 0.825 	 Training loss: 1.00
Gradient step: 449 	 Performance: 0.845 	 Training loss: 0.99
Gradient step: 474 	 Performance: 0.835 	 Training loss: 0.99
Gradient step: 499 	 Performance: 0.915 	 Training loss: 1.00
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/multidlydmintr/None' already exists!
Gradient step: 0 	 Performance: 0.045 	 Training loss: 2.57
Gradient step: 24 	 Performance: 0.055 	 Training loss: 17.58
Gradient step: 49 	 Performance: 0.05 	 Training loss: 9.95
Gradient step: 74 	 Performance: 0.22 	 Training loss: 6.82
Gradient step: 99 	 Performance: 0.51 	 Training loss: 4.88
Gradient step: 124 	 Performance: 0.65 	 Training loss: 3.62
Gradient step: 149 	 Performance: 0.645 	 Training loss: 3.07
Gradient step: 174 	 Performance: 0.695 	 Training loss: 2.64
Gradient step: 199 	 Performance: 0.755 	 Training loss: 2.29
Gradient step: 224 	 Performance: 0.75 	 Training loss: 2.09
Gradient step: 249 	 Performance: 0.805 	 Training loss: 1.96
Gradient step: 274 	 Performance: 0.805 	 Training loss: 1.83
Gradient step: 299 	 Performance: 0.81 	 Training loss: 1.65
Gradient step: 324 	 Performance: 0.79 	 Training loss: 1.46
Gradient step: 349 	 Performance: 0.835 	 Training loss: 1.43
Gradient step: 374 	 Performance: 0.79 	 Training loss: 1.35
Gradient step: 399 	 Performance: 0.815 	 Training loss: 1.35
Gradient step: 424 	 Performance: 0.86 	 Training loss: 1.31
Gradient step: 449 	 Performance: 0.845 	 Training loss: 1.23
Gradient step: 474 	 Performance: 0.84 	 Training loss: 1.14
Gradient step: 499 	 Performance: 0.83 	 Training loss: 1.13
Gradient step: 524 	 Performance: 0.89 	 Training loss: 1.08
Gradient step: 549 	 Performance: 0.845 	 Training loss: 1.03
Gradient step: 574 	 Performance: 0.855 	 Training loss: 1.09
Gradient step: 599 	 Performance: 0.84 	 Training loss: 0.99
Gradient step: 624 	 Performance: 0.85 	 Training loss: 1.00
Gradient step: 649 	 Performance: 0.84 	 Training loss: 1.02
Gradient step: 674 	 Performance: 0.81 	 Training loss: 1.00
Gradient step: 699 	 Performance: 0.83 	 Training loss: 0.96
Gradient step: 724 	 Performance: 0.835 	 Training loss: 0.91
Gradient step: 749 	 Performance: 0.83 	 Training loss: 0.87
Gradient step: 774 	 Performance: 0.815 	 Training loss: 0.93
Gradient step: 799 	 Performance: 0.85 	 Training loss: 0.94
Gradient step: 824 	 Performance: 0.84 	 Training loss: 0.79
Gradient step: 849 	 Performance: 0.875 	 Training loss: 0.76
Gradient step: 874 	 Performance: 0.87 	 Training loss: 0.74
Gradient step: 899 	 Performance: 0.845 	 Training loss: 0.72
Gradient step: 924 	 Performance: 0.89 	 Training loss: 0.76
Gradient step: 949 	 Performance: 0.88 	 Training loss: 0.74
Gradient step: 974 	 Performance: 0.91 	 Training loss: 0.74
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/multidlydmintr/conformal' created successfully!
Gradient step: 0 	 Performance: 0.06 	 Training loss: 3.56
Gradient step: 24 	 Performance: 0.035 	 Training loss: 16.41
Gradient step: 49 	 Performance: 0.12 	 Training loss: 9.41
Gradient step: 74 	 Performance: 0.305 	 Training loss: 5.90
Gradient step: 99 	 Performance: 0.73 	 Training loss: 4.56
Gradient step: 124 	 Performance: 0.765 	 Training loss: 3.58
Gradient step: 149 	 Performance: 0.695 	 Training loss: 2.70
Gradient step: 174 	 Performance: 0.805 	 Training loss: 2.08
Gradient step: 199 	 Performance: 0.78 	 Training loss: 1.80
Gradient step: 224 	 Performance: 0.825 	 Training loss: 1.61
Gradient step: 249 	 Performance: 0.775 	 Training loss: 1.50
Gradient step: 274 	 Performance: 0.82 	 Training loss: 1.42
Gradient step: 299 	 Performance: 0.815 	 Training loss: 1.27
Gradient step: 324 	 Performance: 0.835 	 Training loss: 1.18
Gradient step: 349 	 Performance: 0.805 	 Training loss: 1.18
Gradient step: 374 	 Performance: 0.875 	 Training loss: 1.06
Gradient step: 399 	 Performance: 0.845 	 Training loss: 1.03
Gradient step: 424 	 Performance: 0.845 	 Training loss: 1.08
Gradient step: 449 	 Performance: 0.875 	 Training loss: 1.00
Gradient step: 474 	 Performance: 0.85 	 Training loss: 0.93
Gradient step: 499 	 Performance: 0.85 	 Training loss: 0.94
Gradient step: 524 	 Performance: 0.875 	 Training loss: 0.98
Gradient step: 549 	 Performance: 0.865 	 Training loss: 0.91
Gradient step: 574 	 Performance: 0.885 	 Training loss: 0.90
Gradient step: 599 	 Performance: 0.89 	 Training loss: 0.86
Gradient step: 624 	 Performance: 0.875 	 Training loss: 0.86
Gradient step: 649 	 Performance: 0.885 	 Training loss: 0.84
Gradient step: 674 	 Performance: 0.865 	 Training loss: 0.92
Gradient step: 699 	 Performance: 0.84 	 Training loss: 0.84
Gradient step: 724 	 Performance: 0.86 	 Training loss: 0.82
Gradient step: 749 	 Performance: 0.88 	 Training loss: 0.83
Gradient step: 774 	 Performance: 0.86 	 Training loss: 0.79
Gradient step: 799 	 Performance: 0.86 	 Training loss: 0.82
Gradient step: 824 	 Performance: 0.885 	 Training loss: 0.83
Gradient step: 849 	 Performance: 0.87 	 Training loss: 0.80
Gradient step: 874 	 Performance: 0.875 	 Training loss: 0.76
Gradient step: 899 	 Performance: 0.91 	 Training loss: 0.71
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/multidlydmintr/None' created successfully!
