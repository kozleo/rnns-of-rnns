Gradient step: 0 	 Performance: 0.065 	 Training loss: 3.37
Gradient step: 24 	 Performance: 0.05 	 Training loss: 36.44
Gradient step: 49 	 Performance: 0.035 	 Training loss: 32.52
Gradient step: 74 	 Performance: 0.045 	 Training loss: 28.80
Gradient step: 99 	 Performance: 0.065 	 Training loss: 23.90
Gradient step: 124 	 Performance: 0.04 	 Training loss: 21.81
Gradient step: 149 	 Performance: 0.055 	 Training loss: 21.07
Gradient step: 174 	 Performance: 0.03 	 Training loss: 20.55
Gradient step: 199 	 Performance: 0.045 	 Training loss: 20.16
Gradient step: 224 	 Performance: 0.05 	 Training loss: 19.72
Gradient step: 249 	 Performance: 0.1 	 Training loss: 19.29
Gradient step: 274 	 Performance: 0.08 	 Training loss: 18.84
Gradient step: 299 	 Performance: 0.075 	 Training loss: 18.39
Gradient step: 324 	 Performance: 0.08 	 Training loss: 17.80
Gradient step: 349 	 Performance: 0.07 	 Training loss: 17.15
Gradient step: 374 	 Performance: 0.205 	 Training loss: 16.47
Gradient step: 399 	 Performance: 0.11 	 Training loss: 15.90
Gradient step: 424 	 Performance: 0.21 	 Training loss: 15.29
Gradient step: 449 	 Performance: 0.175 	 Training loss: 14.91
Gradient step: 474 	 Performance: 0.25 	 Training loss: 14.44
Gradient step: 499 	 Performance: 0.25 	 Training loss: 14.12
Gradient step: 524 	 Performance: 0.255 	 Training loss: 13.85
Gradient step: 549 	 Performance: 0.255 	 Training loss: 13.61
Gradient step: 574 	 Performance: 0.27 	 Training loss: 13.33
Gradient step: 599 	 Performance: 0.275 	 Training loss: 13.16
Gradient step: 624 	 Performance: 0.275 	 Training loss: 12.89
Gradient step: 649 	 Performance: 0.33 	 Training loss: 12.64
Gradient step: 674 	 Performance: 0.33 	 Training loss: 12.45
Gradient step: 699 	 Performance: 0.37 	 Training loss: 12.21
Gradient step: 724 	 Performance: 0.305 	 Training loss: 11.99
Gradient step: 749 	 Performance: 0.375 	 Training loss: 11.97
Gradient step: 774 	 Performance: 0.405 	 Training loss: 11.90
Gradient step: 799 	 Performance: 0.4 	 Training loss: 11.73
Gradient step: 824 	 Performance: 0.285 	 Training loss: 11.51
Gradient step: 849 	 Performance: 0.335 	 Training loss: 11.33
Gradient step: 874 	 Performance: 0.32 	 Training loss: 11.25
Gradient step: 899 	 Performance: 0.46 	 Training loss: 10.87
Gradient step: 924 	 Performance: 0.375 	 Training loss: 10.86
Gradient step: 949 	 Performance: 0.29 	 Training loss: 10.71
Gradient step: 974 	 Performance: 0.495 	 Training loss: 10.59
Gradient step: 999 	 Performance: 0.43 	 Training loss: 10.38
Gradient step: 1024 	 Performance: 0.375 	 Training loss: 10.21
Gradient step: 1049 	 Performance: 0.455 	 Training loss: 10.28
Gradient step: 1074 	 Performance: 0.47 	 Training loss: 10.03
Gradient step: 1099 	 Performance: 0.555 	 Training loss: 9.80
Gradient step: 1124 	 Performance: 0.45 	 Training loss: 9.58
Gradient step: 1149 	 Performance: 0.54 	 Training loss: 9.48
Gradient step: 1174 	 Performance: 0.455 	 Training loss: 9.45
Gradient step: 1199 	 Performance: 0.525 	 Training loss: 9.20
Gradient step: 1224 	 Performance: 0.53 	 Training loss: 9.18
Gradient step: 1249 	 Performance: 0.61 	 Training loss: 9.01
Gradient step: 1274 	 Performance: 0.525 	 Training loss: 8.78
Gradient step: 1299 	 Performance: 0.655 	 Training loss: 8.66
Gradient step: 1324 	 Performance: 0.605 	 Training loss: 8.88
Gradient step: 1349 	 Performance: 0.545 	 Training loss: 8.52
Gradient step: 1374 	 Performance: 0.54 	 Training loss: 8.37
Gradient step: 1399 	 Performance: 0.565 	 Training loss: 8.23
Gradient step: 1424 	 Performance: 0.59 	 Training loss: 8.14
Gradient step: 1449 	 Performance: 0.63 	 Training loss: 8.04
Gradient step: 1474 	 Performance: 0.665 	 Training loss: 7.90
Gradient step: 1499 	 Performance: 0.575 	 Training loss: 7.72
Gradient step: 1524 	 Performance: 0.655 	 Training loss: 7.63
Gradient step: 1549 	 Performance: 0.71 	 Training loss: 7.49
Gradient step: 1574 	 Performance: 0.68 	 Training loss: 7.28
Gradient step: 1599 	 Performance: 0.715 	 Training loss: 7.12
Gradient step: 1624 	 Performance: 0.665 	 Training loss: 7.04
Gradient step: 1649 	 Performance: 0.775 	 Training loss: 7.08
Gradient step: 1674 	 Performance: 0.695 	 Training loss: 6.92
Gradient step: 1699 	 Performance: 0.75 	 Training loss: 6.67
Gradient step: 1724 	 Performance: 0.745 	 Training loss: 6.57
Gradient step: 1749 	 Performance: 0.8 	 Training loss: 6.40
Gradient step: 1774 	 Performance: 0.685 	 Training loss: 6.48
Gradient step: 1799 	 Performance: 0.675 	 Training loss: 6.42
Gradient step: 1824 	 Performance: 0.685 	 Training loss: 6.22
Gradient step: 1849 	 Performance: 0.74 	 Training loss: 6.11
Gradient step: 1874 	 Performance: 0.765 	 Training loss: 6.10
Gradient step: 1899 	 Performance: 0.785 	 Training loss: 5.85
Gradient step: 1924 	 Performance: 0.835 	 Training loss: 5.94
Gradient step: 1949 	 Performance: 0.73 	 Training loss: 5.70
Gradient step: 1974 	 Performance: 0.83 	 Training loss: 5.57
Gradient step: 1999 	 Performance: 0.75 	 Training loss: 5.56
Gradient step: 2024 	 Performance: 0.835 	 Training loss: 5.41
Gradient step: 2049 	 Performance: 0.82 	 Training loss: 5.36
Gradient step: 2074 	 Performance: 0.845 	 Training loss: 5.23
Gradient step: 2099 	 Performance: 0.71 	 Training loss: 5.02
Gradient step: 2124 	 Performance: 0.83 	 Training loss: 5.06
Gradient step: 2149 	 Performance: 0.85 	 Training loss: 5.10
Gradient step: 2174 	 Performance: 0.845 	 Training loss: 4.84
Gradient step: 2199 	 Performance: 0.815 	 Training loss: 4.89
Gradient step: 2224 	 Performance: 0.815 	 Training loss: 4.74
Gradient step: 2249 	 Performance: 0.835 	 Training loss: 4.61
Gradient step: 2274 	 Performance: 0.915 	 Training loss: 4.57
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/None/dlydm2seqr/None' created successfully!
Gradient step: 0 	 Performance: 0.06 	 Training loss: 2.63
Gradient step: 24 	 Performance: 0.035 	 Training loss: 35.54
Gradient step: 49 	 Performance: 0.095 	 Training loss: 28.58
Gradient step: 74 	 Performance: 0.075 	 Training loss: 23.47
Gradient step: 99 	 Performance: 0.06 	 Training loss: 21.99
Gradient step: 124 	 Performance: 0.07 	 Training loss: 21.16
Gradient step: 149 	 Performance: 0.08 	 Training loss: 20.52
Gradient step: 174 	 Performance: 0.075 	 Training loss: 20.08
Gradient step: 199 	 Performance: 0.08 	 Training loss: 19.60
Gradient step: 224 	 Performance: 0.055 	 Training loss: 19.10
Gradient step: 249 	 Performance: 0.085 	 Training loss: 18.32
Gradient step: 274 	 Performance: 0.04 	 Training loss: 17.21
Gradient step: 299 	 Performance: 0.115 	 Training loss: 15.86
Gradient step: 324 	 Performance: 0.205 	 Training loss: 14.61
Gradient step: 349 	 Performance: 0.255 	 Training loss: 13.73
Gradient step: 374 	 Performance: 0.225 	 Training loss: 13.12
Gradient step: 399 	 Performance: 0.275 	 Training loss: 12.61
Gradient step: 424 	 Performance: 0.345 	 Training loss: 12.19
Gradient step: 449 	 Performance: 0.335 	 Training loss: 11.99
Gradient step: 474 	 Performance: 0.375 	 Training loss: 11.49
Gradient step: 499 	 Performance: 0.43 	 Training loss: 11.18
Gradient step: 524 	 Performance: 0.36 	 Training loss: 10.68
Gradient step: 549 	 Performance: 0.395 	 Training loss: 10.07
Gradient step: 574 	 Performance: 0.4 	 Training loss: 9.58
Gradient step: 599 	 Performance: 0.47 	 Training loss: 8.96
Gradient step: 624 	 Performance: 0.435 	 Training loss: 8.45
Gradient step: 649 	 Performance: 0.48 	 Training loss: 8.03
Gradient step: 674 	 Performance: 0.485 	 Training loss: 7.82
Gradient step: 699 	 Performance: 0.525 	 Training loss: 7.46
Gradient step: 724 	 Performance: 0.425 	 Training loss: 7.36
Gradient step: 749 	 Performance: 0.565 	 Training loss: 7.15
Gradient step: 774 	 Performance: 0.52 	 Training loss: 6.84
Gradient step: 799 	 Performance: 0.65 	 Training loss: 6.77
Gradient step: 824 	 Performance: 0.575 	 Training loss: 6.59
Gradient step: 849 	 Performance: 0.59 	 Training loss: 6.39
Gradient step: 874 	 Performance: 0.705 	 Training loss: 6.26
Gradient step: 899 	 Performance: 0.625 	 Training loss: 6.16
Gradient step: 924 	 Performance: 0.665 	 Training loss: 6.06
Gradient step: 949 	 Performance: 0.67 	 Training loss: 5.92
Gradient step: 974 	 Performance: 0.64 	 Training loss: 5.84
Gradient step: 999 	 Performance: 0.69 	 Training loss: 5.70
Gradient step: 1024 	 Performance: 0.665 	 Training loss: 5.61
Gradient step: 1049 	 Performance: 0.635 	 Training loss: 5.54
Gradient step: 1074 	 Performance: 0.73 	 Training loss: 5.43
Gradient step: 1099 	 Performance: 0.745 	 Training loss: 5.31
Gradient step: 1124 	 Performance: 0.735 	 Training loss: 5.14
Gradient step: 1149 	 Performance: 0.695 	 Training loss: 5.06
Gradient step: 1174 	 Performance: 0.705 	 Training loss: 4.98
Gradient step: 1199 	 Performance: 0.72 	 Training loss: 4.73
Gradient step: 1224 	 Performance: 0.735 	 Training loss: 4.83
Gradient step: 1249 	 Performance: 0.745 	 Training loss: 4.77
Gradient step: 1274 	 Performance: 0.76 	 Training loss: 4.58
Gradient step: 1299 	 Performance: 0.65 	 Training loss: 4.61
Gradient step: 1324 	 Performance: 0.78 	 Training loss: 4.49
Gradient step: 1349 	 Performance: 0.75 	 Training loss: 4.40
Gradient step: 1374 	 Performance: 0.8 	 Training loss: 4.29
Gradient step: 1399 	 Performance: 0.835 	 Training loss: 4.34
Gradient step: 1424 	 Performance: 0.805 	 Training loss: 4.22
Gradient step: 1449 	 Performance: 0.775 	 Training loss: 4.13
Gradient step: 1474 	 Performance: 0.81 	 Training loss: 4.10
Gradient step: 1499 	 Performance: 0.835 	 Training loss: 3.93
Gradient step: 1524 	 Performance: 0.8 	 Training loss: 3.90
Gradient step: 1549 	 Performance: 0.84 	 Training loss: 3.87
Gradient step: 1574 	 Performance: 0.75 	 Training loss: 3.88
Gradient step: 1599 	 Performance: 0.79 	 Training loss: 3.88
Gradient step: 1624 	 Performance: 0.81 	 Training loss: 3.72
Gradient step: 1649 	 Performance: 0.825 	 Training loss: 3.70
Gradient step: 1674 	 Performance: 0.87 	 Training loss: 3.63
Gradient step: 1699 	 Performance: 0.79 	 Training loss: 3.61
Gradient step: 1724 	 Performance: 0.845 	 Training loss: 3.55
Gradient step: 1749 	 Performance: 0.835 	 Training loss: 3.36
Gradient step: 1774 	 Performance: 0.87 	 Training loss: 3.50
Gradient step: 1799 	 Performance: 0.835 	 Training loss: 3.34
Gradient step: 1824 	 Performance: 0.845 	 Training loss: 3.34
Gradient step: 1849 	 Performance: 0.83 	 Training loss: 3.29
Gradient step: 1874 	 Performance: 0.83 	 Training loss: 3.24
Gradient step: 1899 	 Performance: 0.865 	 Training loss: 3.29
Gradient step: 1924 	 Performance: 0.895 	 Training loss: 3.21
Gradient step: 1949 	 Performance: 0.89 	 Training loss: 3.16
Gradient step: 1974 	 Performance: 0.86 	 Training loss: 3.10
Gradient step: 1999 	 Performance: 0.88 	 Training loss: 3.07
Gradient step: 2024 	 Performance: 0.905 	 Training loss: 3.06
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/dlydm2seqr/conformal' created successfully!
Gradient step: 0 	 Performance: 0.07 	 Training loss: 3.36
Gradient step: 24 	 Performance: 0.045 	 Training loss: 36.41
Gradient step: 49 	 Performance: 0.07 	 Training loss: 32.48
Gradient step: 74 	 Performance: 0.07 	 Training loss: 28.71
Gradient step: 99 	 Performance: 0.06 	 Training loss: 23.67
Gradient step: 124 	 Performance: 0.1 	 Training loss: 21.78
Gradient step: 149 	 Performance: 0.045 	 Training loss: 21.06
Gradient step: 174 	 Performance: 0.06 	 Training loss: 20.53
Gradient step: 199 	 Performance: 0.05 	 Training loss: 20.13
Gradient step: 224 	 Performance: 0.05 	 Training loss: 19.72
Gradient step: 249 	 Performance: 0.075 	 Training loss: 19.29
Gradient step: 274 	 Performance: 0.07 	 Training loss: 18.85
Gradient step: 299 	 Performance: 0.05 	 Training loss: 18.27
Gradient step: 324 	 Performance: 0.085 	 Training loss: 17.48
Gradient step: 349 	 Performance: 0.085 	 Training loss: 16.76
Gradient step: 374 	 Performance: 0.145 	 Training loss: 16.07
Gradient step: 399 	 Performance: 0.175 	 Training loss: 15.45
Gradient step: 424 	 Performance: 0.2 	 Training loss: 14.95
Gradient step: 449 	 Performance: 0.165 	 Training loss: 14.50
Gradient step: 474 	 Performance: 0.225 	 Training loss: 14.16
Gradient step: 499 	 Performance: 0.245 	 Training loss: 13.85
Gradient step: 524 	 Performance: 0.265 	 Training loss: 13.70
Gradient step: 549 	 Performance: 0.21 	 Training loss: 13.35
Gradient step: 574 	 Performance: 0.295 	 Training loss: 13.24
Gradient step: 599 	 Performance: 0.25 	 Training loss: 12.94
Gradient step: 624 	 Performance: 0.28 	 Training loss: 12.75
Gradient step: 649 	 Performance: 0.29 	 Training loss: 12.60
Gradient step: 674 	 Performance: 0.295 	 Training loss: 12.37
Gradient step: 699 	 Performance: 0.25 	 Training loss: 12.31
Gradient step: 724 	 Performance: 0.35 	 Training loss: 12.02
Gradient step: 749 	 Performance: 0.31 	 Training loss: 11.88
Gradient step: 774 	 Performance: 0.29 	 Training loss: 11.86
Gradient step: 799 	 Performance: 0.325 	 Training loss: 11.55
Gradient step: 824 	 Performance: 0.35 	 Training loss: 11.29
Gradient step: 849 	 Performance: 0.4 	 Training loss: 11.23
Gradient step: 874 	 Performance: 0.34 	 Training loss: 11.16
Gradient step: 899 	 Performance: 0.44 	 Training loss: 10.96
Gradient step: 924 	 Performance: 0.3 	 Training loss: 10.85
Gradient step: 949 	 Performance: 0.365 	 Training loss: 10.62
Gradient step: 974 	 Performance: 0.325 	 Training loss: 10.52
Gradient step: 999 	 Performance: 0.39 	 Training loss: 10.41
Gradient step: 1024 	 Performance: 0.365 	 Training loss: 10.36
Gradient step: 1049 	 Performance: 0.455 	 Training loss: 10.17
Gradient step: 1074 	 Performance: 0.38 	 Training loss: 9.82
Gradient step: 1099 	 Performance: 0.385 	 Training loss: 9.94
Gradient step: 1124 	 Performance: 0.535 	 Training loss: 9.89
Gradient step: 1149 	 Performance: 0.42 	 Training loss: 9.45
Gradient step: 1174 	 Performance: 0.45 	 Training loss: 9.43
Gradient step: 1199 	 Performance: 0.505 	 Training loss: 9.20
Gradient step: 1224 	 Performance: 0.495 	 Training loss: 9.17
Gradient step: 1249 	 Performance: 0.48 	 Training loss: 9.14
Gradient step: 1274 	 Performance: 0.58 	 Training loss: 9.14
Gradient step: 1299 	 Performance: 0.53 	 Training loss: 9.04
Gradient step: 1324 	 Performance: 0.63 	 Training loss: 8.95
Gradient step: 1349 	 Performance: 0.525 	 Training loss: 8.61
Gradient step: 1374 	 Performance: 0.57 	 Training loss: 8.59
Gradient step: 1399 	 Performance: 0.505 	 Training loss: 8.30
Gradient step: 1424 	 Performance: 0.625 	 Training loss: 8.35
Gradient step: 1449 	 Performance: 0.485 	 Training loss: 8.25
Gradient step: 1474 	 Performance: 0.57 	 Training loss: 8.11
Gradient step: 1499 	 Performance: 0.53 	 Training loss: 8.05
Gradient step: 1524 	 Performance: 0.595 	 Training loss: 7.84
Gradient step: 1549 	 Performance: 0.605 	 Training loss: 7.63
Gradient step: 1574 	 Performance: 0.635 	 Training loss: 7.59
Gradient step: 1599 	 Performance: 0.675 	 Training loss: 7.52
Gradient step: 1624 	 Performance: 0.66 	 Training loss: 7.37
Gradient step: 1649 	 Performance: 0.67 	 Training loss: 7.22
Gradient step: 1674 	 Performance: 0.685 	 Training loss: 7.11
Gradient step: 1699 	 Performance: 0.7 	 Training loss: 7.20
Gradient step: 1724 	 Performance: 0.68 	 Training loss: 7.15
Gradient step: 1749 	 Performance: 0.735 	 Training loss: 6.89
Gradient step: 1774 	 Performance: 0.755 	 Training loss: 6.89
Gradient step: 1799 	 Performance: 0.75 	 Training loss: 6.72
Gradient step: 1824 	 Performance: 0.675 	 Training loss: 6.56
Gradient step: 1849 	 Performance: 0.655 	 Training loss: 6.61
Gradient step: 1874 	 Performance: 0.74 	 Training loss: 6.53
Gradient step: 1899 	 Performance: 0.705 	 Training loss: 6.45
Gradient step: 1924 	 Performance: 0.81 	 Training loss: 6.42
Gradient step: 1949 	 Performance: 0.8 	 Training loss: 6.16
Gradient step: 1974 	 Performance: 0.735 	 Training loss: 6.09
Gradient step: 1999 	 Performance: 0.83 	 Training loss: 5.99
Gradient step: 2024 	 Performance: 0.795 	 Training loss: 5.94
Gradient step: 2049 	 Performance: 0.715 	 Training loss: 5.87
Gradient step: 2074 	 Performance: 0.745 	 Training loss: 5.80
Gradient step: 2099 	 Performance: 0.75 	 Training loss: 5.86
Gradient step: 2124 	 Performance: 0.765 	 Training loss: 5.65
Gradient step: 2149 	 Performance: 0.74 	 Training loss: 5.53
Gradient step: 2174 	 Performance: 0.78 	 Training loss: 5.63
Gradient step: 2199 	 Performance: 0.765 	 Training loss: 5.46
Gradient step: 2224 	 Performance: 0.805 	 Training loss: 5.44
Gradient step: 2249 	 Performance: 0.78 	 Training loss: 5.34
Gradient step: 2274 	 Performance: 0.835 	 Training loss: 5.28
Gradient step: 2299 	 Performance: 0.84 	 Training loss: 5.05
Gradient step: 2324 	 Performance: 0.815 	 Training loss: 5.15
Gradient step: 2349 	 Performance: 0.835 	 Training loss: 5.05
Gradient step: 2374 	 Performance: 0.85 	 Training loss: 5.07
Gradient step: 2399 	 Performance: 0.86 	 Training loss: 4.93
Gradient step: 2424 	 Performance: 0.815 	 Training loss: 4.83
Gradient step: 2449 	 Performance: 0.84 	 Training loss: 4.88
Gradient step: 2474 	 Performance: 0.82 	 Training loss: 4.77
Gradient step: 2499 	 Performance: 0.87 	 Training loss: 4.77
Gradient step: 2524 	 Performance: 0.85 	 Training loss: 4.67
Gradient step: 2549 	 Performance: 0.895 	 Training loss: 4.56
Gradient step: 2574 	 Performance: 0.88 	 Training loss: 4.53
Gradient step: 2599 	 Performance: 0.875 	 Training loss: 4.56
Gradient step: 2624 	 Performance: 0.91 	 Training loss: 4.37
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/sym/dlydm2seqr/None' created successfully!
Gradient step: 0 	 Performance: 0.065 	 Training loss: 2.63
Gradient step: 24 	 Performance: 0.065 	 Training loss: 35.52
Gradient step: 49 	 Performance: 0.07 	 Training loss: 28.51
Gradient step: 74 	 Performance: 0.065 	 Training loss: 23.49
Gradient step: 99 	 Performance: 0.085 	 Training loss: 22.08
Gradient step: 124 	 Performance: 0.06 	 Training loss: 21.21
Gradient step: 149 	 Performance: 0.05 	 Training loss: 20.61
Gradient step: 174 	 Performance: 0.05 	 Training loss: 20.10
Gradient step: 199 	 Performance: 0.04 	 Training loss: 19.64
Gradient step: 224 	 Performance: 0.07 	 Training loss: 19.27
Gradient step: 249 	 Performance: 0.085 	 Training loss: 18.53
Gradient step: 274 	 Performance: 0.055 	 Training loss: 17.75
Gradient step: 299 	 Performance: 0.095 	 Training loss: 16.60
Gradient step: 324 	 Performance: 0.16 	 Training loss: 15.26
Gradient step: 349 	 Performance: 0.29 	 Training loss: 14.16
Gradient step: 374 	 Performance: 0.255 	 Training loss: 13.42
Gradient step: 399 	 Performance: 0.265 	 Training loss: 12.93
Gradient step: 424 	 Performance: 0.29 	 Training loss: 12.44
Gradient step: 449 	 Performance: 0.265 	 Training loss: 12.05
Gradient step: 474 	 Performance: 0.43 	 Training loss: 11.54
Gradient step: 499 	 Performance: 0.42 	 Training loss: 11.04
Gradient step: 524 	 Performance: 0.33 	 Training loss: 10.71
Gradient step: 549 	 Performance: 0.395 	 Training loss: 9.88
Gradient step: 574 	 Performance: 0.435 	 Training loss: 9.15
Gradient step: 599 	 Performance: 0.46 	 Training loss: 8.67
Gradient step: 624 	 Performance: 0.53 	 Training loss: 8.25
Gradient step: 649 	 Performance: 0.42 	 Training loss: 7.75
Gradient step: 674 	 Performance: 0.535 	 Training loss: 7.45
Gradient step: 699 	 Performance: 0.455 	 Training loss: 7.30
Gradient step: 724 	 Performance: 0.605 	 Training loss: 7.06
Gradient step: 749 	 Performance: 0.595 	 Training loss: 6.78
Gradient step: 774 	 Performance: 0.55 	 Training loss: 6.66
Gradient step: 799 	 Performance: 0.665 	 Training loss: 6.49
Gradient step: 824 	 Performance: 0.63 	 Training loss: 6.31
Gradient step: 849 	 Performance: 0.615 	 Training loss: 6.29
Gradient step: 874 	 Performance: 0.725 	 Training loss: 6.08
Gradient step: 899 	 Performance: 0.68 	 Training loss: 5.91
Gradient step: 924 	 Performance: 0.7 	 Training loss: 5.80
Gradient step: 949 	 Performance: 0.68 	 Training loss: 5.61
Gradient step: 974 	 Performance: 0.75 	 Training loss: 5.49
Gradient step: 999 	 Performance: 0.68 	 Training loss: 5.46
Gradient step: 1024 	 Performance: 0.7 	 Training loss: 5.32
Gradient step: 1049 	 Performance: 0.74 	 Training loss: 5.21
Gradient step: 1074 	 Performance: 0.77 	 Training loss: 5.05
Gradient step: 1099 	 Performance: 0.78 	 Training loss: 5.00
Gradient step: 1124 	 Performance: 0.745 	 Training loss: 4.91
Gradient step: 1149 	 Performance: 0.75 	 Training loss: 4.88
Gradient step: 1174 	 Performance: 0.815 	 Training loss: 4.74
Gradient step: 1199 	 Performance: 0.72 	 Training loss: 4.71
Gradient step: 1224 	 Performance: 0.72 	 Training loss: 4.71
Gradient step: 1249 	 Performance: 0.725 	 Training loss: 4.49
Gradient step: 1274 	 Performance: 0.77 	 Training loss: 4.43
Gradient step: 1299 	 Performance: 0.765 	 Training loss: 4.34
Gradient step: 1324 	 Performance: 0.77 	 Training loss: 4.30
Gradient step: 1349 	 Performance: 0.815 	 Training loss: 4.13
Gradient step: 1374 	 Performance: 0.795 	 Training loss: 4.11
Gradient step: 1399 	 Performance: 0.825 	 Training loss: 3.99
Gradient step: 1424 	 Performance: 0.75 	 Training loss: 3.95
Gradient step: 1449 	 Performance: 0.815 	 Training loss: 3.83
Gradient step: 1474 	 Performance: 0.8 	 Training loss: 3.82
Gradient step: 1499 	 Performance: 0.795 	 Training loss: 3.71
Gradient step: 1524 	 Performance: 0.755 	 Training loss: 3.83
Gradient step: 1549 	 Performance: 0.825 	 Training loss: 3.66
Gradient step: 1574 	 Performance: 0.785 	 Training loss: 3.64
Gradient step: 1599 	 Performance: 0.835 	 Training loss: 3.65
Gradient step: 1624 	 Performance: 0.84 	 Training loss: 3.52
Gradient step: 1649 	 Performance: 0.83 	 Training loss: 3.46
Gradient step: 1674 	 Performance: 0.86 	 Training loss: 3.49
Gradient step: 1699 	 Performance: 0.865 	 Training loss: 3.32
Gradient step: 1724 	 Performance: 0.82 	 Training loss: 3.41
Gradient step: 1749 	 Performance: 0.855 	 Training loss: 3.33
Gradient step: 1774 	 Performance: 0.88 	 Training loss: 3.22
Gradient step: 1799 	 Performance: 0.89 	 Training loss: 3.14
Gradient step: 1824 	 Performance: 0.895 	 Training loss: 3.15
Gradient step: 1849 	 Performance: 0.87 	 Training loss: 3.07
Gradient step: 1874 	 Performance: 0.855 	 Training loss: 3.02
Gradient step: 1899 	 Performance: 0.895 	 Training loss: 3.12
Gradient step: 1924 	 Performance: 0.91 	 Training loss: 3.06
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/dlydm2seqr/conformal' created successfully!
Gradient step: 0 	 Performance: 0.055 	 Training loss: 3.36
Gradient step: 24 	 Performance: 0.045 	 Training loss: 36.42
Gradient step: 49 	 Performance: 0.07 	 Training loss: 32.48
Gradient step: 74 	 Performance: 0.05 	 Training loss: 28.63
Gradient step: 99 	 Performance: 0.04 	 Training loss: 23.78
Gradient step: 124 	 Performance: 0.09 	 Training loss: 21.81
Gradient step: 149 	 Performance: 0.045 	 Training loss: 21.09
Gradient step: 174 	 Performance: 0.065 	 Training loss: 20.58
Gradient step: 199 	 Performance: 0.055 	 Training loss: 20.19
Gradient step: 224 	 Performance: 0.07 	 Training loss: 19.78
Gradient step: 249 	 Performance: 0.09 	 Training loss: 19.39
Gradient step: 274 	 Performance: 0.06 	 Training loss: 19.03
Gradient step: 299 	 Performance: 0.075 	 Training loss: 18.59
Gradient step: 324 	 Performance: 0.07 	 Training loss: 18.04
Gradient step: 349 	 Performance: 0.08 	 Training loss: 17.37
Gradient step: 374 	 Performance: 0.145 	 Training loss: 16.65
Gradient step: 399 	 Performance: 0.125 	 Training loss: 15.90
Gradient step: 424 	 Performance: 0.12 	 Training loss: 15.31
Gradient step: 449 	 Performance: 0.215 	 Training loss: 14.90
Gradient step: 474 	 Performance: 0.2 	 Training loss: 14.47
Gradient step: 499 	 Performance: 0.16 	 Training loss: 14.08
Gradient step: 524 	 Performance: 0.255 	 Training loss: 13.86
Gradient step: 549 	 Performance: 0.29 	 Training loss: 13.58
Gradient step: 574 	 Performance: 0.25 	 Training loss: 13.31
Gradient step: 599 	 Performance: 0.325 	 Training loss: 13.19
Gradient step: 624 	 Performance: 0.235 	 Training loss: 13.00
Gradient step: 649 	 Performance: 0.35 	 Training loss: 12.77
Gradient step: 674 	 Performance: 0.3 	 Training loss: 12.68
Gradient step: 699 	 Performance: 0.24 	 Training loss: 12.38
Gradient step: 724 	 Performance: 0.35 	 Training loss: 12.27
Gradient step: 749 	 Performance: 0.37 	 Training loss: 12.07
Gradient step: 774 	 Performance: 0.34 	 Training loss: 11.85
Gradient step: 799 	 Performance: 0.36 	 Training loss: 11.71
Gradient step: 824 	 Performance: 0.32 	 Training loss: 11.52
Gradient step: 849 	 Performance: 0.405 	 Training loss: 11.29
Gradient step: 874 	 Performance: 0.38 	 Training loss: 11.14
Gradient step: 899 	 Performance: 0.355 	 Training loss: 10.99
Gradient step: 924 	 Performance: 0.29 	 Training loss: 10.93
Gradient step: 949 	 Performance: 0.375 	 Training loss: 10.73
Gradient step: 974 	 Performance: 0.355 	 Training loss: 10.45
Gradient step: 999 	 Performance: 0.385 	 Training loss: 10.37
Gradient step: 1024 	 Performance: 0.435 	 Training loss: 10.20
Gradient step: 1049 	 Performance: 0.425 	 Training loss: 10.15
Gradient step: 1074 	 Performance: 0.385 	 Training loss: 9.94
Gradient step: 1099 	 Performance: 0.49 	 Training loss: 9.75
Gradient step: 1124 	 Performance: 0.47 	 Training loss: 9.60
Gradient step: 1149 	 Performance: 0.49 	 Training loss: 9.54
Gradient step: 1174 	 Performance: 0.53 	 Training loss: 9.33
Gradient step: 1199 	 Performance: 0.565 	 Training loss: 9.37
Gradient step: 1224 	 Performance: 0.46 	 Training loss: 9.09
Gradient step: 1249 	 Performance: 0.525 	 Training loss: 9.06
Gradient step: 1274 	 Performance: 0.47 	 Training loss: 8.89
Gradient step: 1299 	 Performance: 0.56 	 Training loss: 8.68
Gradient step: 1324 	 Performance: 0.515 	 Training loss: 8.67
Gradient step: 1349 	 Performance: 0.56 	 Training loss: 8.79
Gradient step: 1374 	 Performance: 0.555 	 Training loss: 8.52
Gradient step: 1399 	 Performance: 0.635 	 Training loss: 8.24
Gradient step: 1424 	 Performance: 0.56 	 Training loss: 8.14
Gradient step: 1449 	 Performance: 0.615 	 Training loss: 8.11
Gradient step: 1474 	 Performance: 0.63 	 Training loss: 7.98
Gradient step: 1499 	 Performance: 0.66 	 Training loss: 7.97
Gradient step: 1524 	 Performance: 0.68 	 Training loss: 7.77
Gradient step: 1549 	 Performance: 0.63 	 Training loss: 7.75
Gradient step: 1574 	 Performance: 0.585 	 Training loss: 7.58
Gradient step: 1599 	 Performance: 0.665 	 Training loss: 7.46
Gradient step: 1624 	 Performance: 0.655 	 Training loss: 7.44
Gradient step: 1649 	 Performance: 0.68 	 Training loss: 7.21
Gradient step: 1674 	 Performance: 0.6 	 Training loss: 7.19
Gradient step: 1699 	 Performance: 0.65 	 Training loss: 7.33
Gradient step: 1724 	 Performance: 0.655 	 Training loss: 7.12
Gradient step: 1749 	 Performance: 0.7 	 Training loss: 6.94
Gradient step: 1774 	 Performance: 0.67 	 Training loss: 6.73
Gradient step: 1799 	 Performance: 0.65 	 Training loss: 6.76
Gradient step: 1824 	 Performance: 0.63 	 Training loss: 6.69
Gradient step: 1849 	 Performance: 0.745 	 Training loss: 6.66
Gradient step: 1874 	 Performance: 0.715 	 Training loss: 6.35
Gradient step: 1899 	 Performance: 0.695 	 Training loss: 6.39
Gradient step: 1924 	 Performance: 0.745 	 Training loss: 6.28
Gradient step: 1949 	 Performance: 0.745 	 Training loss: 6.11
Gradient step: 1974 	 Performance: 0.78 	 Training loss: 6.02
Gradient step: 1999 	 Performance: 0.775 	 Training loss: 6.05
Gradient step: 2024 	 Performance: 0.825 	 Training loss: 5.90
Gradient step: 2049 	 Performance: 0.805 	 Training loss: 5.79
Gradient step: 2074 	 Performance: 0.84 	 Training loss: 5.82
Gradient step: 2099 	 Performance: 0.77 	 Training loss: 5.71
Gradient step: 2124 	 Performance: 0.8 	 Training loss: 5.51
Gradient step: 2149 	 Performance: 0.86 	 Training loss: 5.56
Gradient step: 2174 	 Performance: 0.74 	 Training loss: 5.49
Gradient step: 2199 	 Performance: 0.79 	 Training loss: 5.43
Gradient step: 2224 	 Performance: 0.81 	 Training loss: 5.35
Gradient step: 2249 	 Performance: 0.92 	 Training loss: 5.15
Folder '/om2/user/leokoz8/code/rnns-of-rnns/models/spectral/dlydm2seqr/None' created successfully!
